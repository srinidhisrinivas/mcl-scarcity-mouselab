{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2653ab8b",
   "metadata": {},
   "source": [
    "This notebook contains model analysis code for the model comparison results reported in the conference submission \"Learning planning strategies without feedback\" - https://osf.io/fyx4g/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41993004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from mouselab.mouselab import MouselabEnv\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy\n",
    "import math\n",
    "import sklearn.cluster\n",
    "from math import floor, ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2d095",
   "metadata": {},
   "source": [
    "# Reading Data from Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7fe000",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileprefix = \"final\" # pilot_#, final\n",
    "datafolder = '../results/anonymized_data/'\n",
    "\n",
    "all_part_files = [filename for filename in os.listdir(datafolder) if filename.startswith(fileprefix) and 'results' not in filename]\n",
    "all_part_paths = [datafolder + filename for filename in all_part_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ff701",
   "metadata": {},
   "source": [
    "# Creating Trial Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1747e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each participant has their own file, generated from the Heroku postgres output in the \n",
    "# preprocessing notebook\n",
    "\n",
    "part_stats_dicts = []\n",
    "scarcity_level = 0.25\n",
    "for file in all_part_paths:\n",
    "    with open(file, 'r') as f:\n",
    "        participant = json.load(f)\n",
    "    try:\n",
    "        beginhit = datetime.datetime.strptime(participant['Beginhit'], '%Y-%m-%d %H:%M:%S.%f')\n",
    "    except TypeError:\n",
    "        beginhit = None\n",
    "    try:\n",
    "        endhit = datetime.datetime.strptime(participant['Endhit'], '%Y-%m-%d %H:%M:%S.%f')\n",
    "    except TypeError:\n",
    "        endhit = None\n",
    "    try:    \n",
    "        beginexp = datetime.datetime.strptime(participant['Beginexp'], '%Y-%m-%d %H:%M:%S.%f')\n",
    "    except TypeError:\n",
    "        beginexp = None\n",
    "    \n",
    "    try:\n",
    "        hitLength = (endhit-beginhit).seconds\n",
    "    except TypeError:\n",
    "        hitLength = None\n",
    "    \n",
    "    try:\n",
    "        expLength = (endhit-beginexp).seconds\n",
    "    except TypeError:\n",
    "        expLength = None\n",
    "        \n",
    "    numPracTrials = 0\n",
    "    trialLengths = []\n",
    "    trialScores = []\n",
    "    expectedScores = []\n",
    "    numClicks = []\n",
    "    avgClickLevel = []\n",
    "    rewardsWithheld = []\n",
    "    demographicsAvailable = False\n",
    "    effort = \"-1\"\n",
    "    age = None\n",
    "    gender = None\n",
    "    colorblind = None\n",
    "    \n",
    "    print(participant['workerId'])\n",
    "    try:\n",
    "        firstTrialStamp = participant['data'][0]['dateTime']\n",
    "    except:\n",
    "        print(\"No data\")\n",
    "        continue\n",
    "    lastTrialStamp = participant['data'][-1]['dateTime']\n",
    "    if hitLength is None:\n",
    "        hitLength = (lastTrialStamp - firstTrialStamp) / 1000\n",
    "    if expLength is None:\n",
    "        expLength = (lastTrialStamp - firstTrialStamp) / 1000\n",
    "        \n",
    "    # Dictionary to check whether stroop tasks have been completed\n",
    "    stroop_timestamps = {\n",
    "        \"1\" : {\n",
    "            \"start\" : 0,\n",
    "            \"end\" : 0,\n",
    "            \"done\" : False\n",
    "        },\n",
    "        \"2\" : {\n",
    "            \"start\" : 0,\n",
    "            \"end\" : 0,\n",
    "            \"done\" : False\n",
    "        }\n",
    "    }\n",
    "    # Dictionary to check whether mouselab task has been completed\n",
    "    mdp_timestamps = {\n",
    "        \"start\" : 0,\n",
    "        \"end\" : 0,\n",
    "        \"done\": False\n",
    "    }\n",
    "    \n",
    "    # Use state machine to parse participant trial data, update state \n",
    "    #  based on trials seen\n",
    "    current_state = \"stroop_1\"\n",
    "    stroop1Completed = 0\n",
    "    stroop1Correct = 0\n",
    "    stroop2Completed = 0\n",
    "    stroop2Correct = 0\n",
    "    last_trial = participant['data'][0]\n",
    "    feedback = \"\"\n",
    "    comments = \"\"\n",
    "    for idx, trial in enumerate(participant['data']):\n",
    "        trial_type = trial['trialdata']['trial_type']\n",
    "        try:\n",
    "            trial_id = str(trial['trialdata']['trial_id'])\n",
    "        except KeyError:\n",
    "            trial_id = \"\"\n",
    "        \n",
    "        # Start 1st stroop task\n",
    "        if trial_id == \"stroop_1_ready_1\" and not stroop_timestamps[\"1\"][\"done\"]:\n",
    "            stroop_timestamps[\"1\"][\"start\"] = trial['dateTime']\n",
    "        \n",
    "        # Start 2nd stroop task\n",
    "        if trial_id == \"stroop_2_ready_1\" and not stroop_timestamps[\"2\"][\"done\"]:\n",
    "            stroop_timestamps[\"2\"][\"start\"] = trial['dateTime']\n",
    "        \n",
    "        # Finish 1st stroop task\n",
    "        if trial_id == \"finish_distractor_1\" and not stroop_timestamps[\"1\"][\"done\"]:\n",
    "            stroop_timestamps[\"1\"][\"end\"] = trial['dateTime']\n",
    "            stroop_timestamps[\"1\"][\"done\"] = True\n",
    "        \n",
    "        # Finish 2nd stroop task\n",
    "        if trial_id == \"finish_distractor_2\" and not stroop_timestamps[\"2\"][\"done\"]:\n",
    "            stroop_timestamps[\"2\"][\"end\"] = trial['dateTime']\n",
    "            stroop_timestamps[\"2\"][\"done\"] = True\n",
    "        \n",
    "        # Start mouselab trials\n",
    "        if trial_id == \"mouselab_instructions_1\" and not mdp_timestamps[\"done\"]:\n",
    "            mdp_timestamps[\"done\"] = True\n",
    "            mdp_timestamps[\"start\"] = trial['dateTime']\n",
    "        \n",
    "        # Finish mouselab trials\n",
    "        if trial_id.startswith(\"final_quiz\"):\n",
    "            mdp_timestamps[\"end\"] = trial['dateTime']\n",
    "            \n",
    "        # Stroop trial encountered, count whether it belongs to the first set or second set\n",
    "        if \"congruent\" in trial_id or \"incongruent\" in trial_id or \"unrelated\" in trial_id:\n",
    "            if not stroop_timestamps[\"1\"][\"done\"]:\n",
    "                stroop1Completed += 1\n",
    "                \n",
    "                if trial['trialdata'][\"response\"].lower() == trial['trialdata'][\"correct_response\"].lower():\n",
    "                    stroop1Correct += 1\n",
    "                \n",
    "            else:\n",
    "                stroop2Completed += 1\n",
    "                if trial['trialdata'][\"response\"].lower() == trial['trialdata'][\"correct_response\"].lower():\n",
    "                    stroop2Correct += 1\n",
    "        \n",
    "        # Mouselab trial encountered\n",
    "        if trial_type == 'mouselab-mdp':\n",
    "            if trial_id.startswith('practice'):\n",
    "                numPracTrials += 1\n",
    "            else:\n",
    "                trialLengths.append(trial['trialdata']['trialTime'])\n",
    "                \n",
    "                trialScores.append(trial['trialdata']['score'])\n",
    "                rewardsWithheld.append(trial['trialdata']['withholdReward'])\n",
    "                \n",
    "                # Getting expected scores\n",
    "                g_truth = [0.0] + trial['trialdata']['stateRewards'][1:]\n",
    "                mEnv = MouselabEnv.new_symmetric_registered('high_increasing', ground_truth=g_truth)\n",
    "                clicks = trial['trialdata']['queries']['click']['state']['target']\n",
    "                for click in clicks:\n",
    "                    mEnv.step(int(click))\n",
    "                \n",
    "                \n",
    "                # Saving the number of clicks and the average depth of all clicks\n",
    "                numClicks.append(len(clicks))\n",
    "                \n",
    "                # Level one nodes are 1,5,9\n",
    "                clicksL1 = len([c for c in clicks if int(c) in [1,5,9]])\n",
    "                clicksL2 = len([c for c in clicks if int(c) in [2,6,10]])\n",
    "                clicksL3 = len([c for c in clicks if int(c) in [3,4,7,8,11,12]])\n",
    "                try:\n",
    "                    avgLevel = (clicksL1 + 2*clicksL2 + 3*clicksL3)/len(clicks)\n",
    "                except:\n",
    "                    avgLevel = 0\n",
    "                    \n",
    "                avgClickLevel.append(avgLevel)\n",
    "                \n",
    "                # plan quality is expected score of the trial minus costs\n",
    "                planQuality = mEnv._term_reward() + trial['trialdata']['costs']/(1.0 if participant['condition'] == 0 else scarcity_level)\n",
    "                expectedScores.append(planQuality)\n",
    "                \n",
    "        # Save information about end questionnaire\n",
    "        if trial_type == 'survey-html-form':\n",
    "            if 'effort' in trial['trialdata']['response']:\n",
    "                effort = trial['trialdata']['response']['effort']\n",
    "                demographicsAvailable = True\n",
    "        \n",
    "        # Participant failed quiz\n",
    "        if trial_id.startswith(\"finish_fail\"):\n",
    "            demographicsAvailable = True\n",
    "        \n",
    "        # Save information about participant feedback/comments\n",
    "        if trial_type == 'survey-text':\n",
    "            feedback = trial['trialdata']['response']['Q0']\n",
    "            comments = trial['trialdata']['response']['Q2']\n",
    "        last_trial = trial\n",
    "\n",
    "    # Check whether participant dropped out by seeing whether any of the phases doesn't have an end timestamp\n",
    "    dropoutPoint = None\n",
    "    for obj, type_ in zip([mdp_timestamps, stroop_timestamps[\"1\"], stroop_timestamps[\"2\"]], [\"mdp\", \"s1\", \"s2\"]):\n",
    "        if obj[\"start\"] > 0 and obj[\"end\"] == 0:\n",
    "            dropoutPoint = type_\n",
    "            obj[\"end\"] = lastTrialStamp\n",
    "            \n",
    "    # Save participant statistics in dictionary   \n",
    "    part_stats_dict = {\n",
    "        \"workerId\": participant['workerId'],\n",
    "        \"Beginhit\": beginhit,\n",
    "        \"Endhit\": endhit,\n",
    "        \"Beginexp\": beginexp,\n",
    "        \"psiturkStatus\" : participant[\"psiturkStatus\"],\n",
    "        \"browser\" : participant[\"browser\"],\n",
    "        \"platform\" : participant[\"platform\"],\n",
    "        \"language\" : participant[\"language\"],\n",
    "        \"hitLength\": hitLength,\n",
    "        \"expLength\": expLength,\n",
    "        \"totalLengthSum\": (lastTrialStamp - firstTrialStamp) / 1000,\n",
    "        \"numQuizAttempts\": numPracTrials / 2,\n",
    "        \"trialLengths\": trialLengths,\n",
    "        \"trialScores\" : trialScores,\n",
    "        \"numTrialsCompleted\": len(trialLengths),\n",
    "        \"rewardsWithheld\": rewardsWithheld,\n",
    "        \"effort\": effort,\n",
    "        \"condition\": participant['condition'],\n",
    "        \"stroop1Length\" : (stroop_timestamps[\"1\"][\"end\"] - stroop_timestamps[\"1\"][\"start\"]) / 1000,\n",
    "        \"stroop2Length\" : (stroop_timestamps[\"2\"][\"end\"] - stroop_timestamps[\"2\"][\"start\"]) / 1000,\n",
    "        \"mouselabLength\" : (mdp_timestamps[\"end\"] - mdp_timestamps[\"start\"]) / 1000,\n",
    "        \"stroop1Completed\" : stroop1Completed,\n",
    "        \"stroop2Completed\" : stroop2Completed,\n",
    "        \"stroop1Correct\" : stroop1Correct,\n",
    "        \"stroop2Correct\" : stroop2Correct,\n",
    "        \"dropoutPoint\" : dropoutPoint,\n",
    "        \"feedback\": feedback,\n",
    "        \"comments\": comments,\n",
    "        \"expectedScores\": expectedScores,\n",
    "        \"numClicks\": numClicks,\n",
    "        \"avgClickLevel\" : avgClickLevel,\n",
    "        \"demographicsAvailable\": demographicsAvailable\n",
    "    }\n",
    "    \n",
    "    # Get final score from data file, or if not present, sum up the scores of all the rewarded trials\n",
    "    if 'questiondata' in participant and 'final_score' in participant['questiondata']:\n",
    "        part_stats_dict['finalScore'] = participant['questiondata']['final_score']\n",
    "    else:\n",
    "        part_stats_dict['finalScore'] = sum([score for (score, withheld) in zip(trialScores, rewardsWithheld) if not withheld])\n",
    "    \n",
    "    part_stats_dict[\"expLengthSum\"] = part_stats_dict[\"stroop1Length\"] + part_stats_dict[\"mouselabLength\"] + part_stats_dict[\"stroop2Length\"]\n",
    "    part_stats_dict[\"expLengthDiff\"] = part_stats_dict[\"expLengthSum\"] - part_stats_dict[\"expLength\"]\n",
    "    if len(trialLengths) > 0:\n",
    "        part_stats_dict[\"averageTrialLength\"] = sum(trialLengths) / len(trialLengths)\n",
    "        part_stats_dict[\"propWithheld\"] = sum(rewardsWithheld) / len(rewardsWithheld)\n",
    "        \n",
    "    part_stats_dicts.append(part_stats_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ba11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling all data into trial data\n",
    "\n",
    "all_trial_data = {\n",
    "    \"trialScores\": [],\n",
    "    \"expectedScores\": [],\n",
    "    \"numClicks\" : [],\n",
    "    \"avgClickLevel\" : [],\n",
    "    \"trialNumbers\": [],\n",
    "    \"scarce\": [],\n",
    "    \"numRewardedTrials\": [],\n",
    "    \"numUnrewardedTrials\": [],\n",
    "    \"workerId\": [],\n",
    "    \"rewardsWithheld\" : [],\n",
    "}\n",
    "\n",
    "count = 0;\n",
    "\n",
    "for part in part_stats_dicts:\n",
    "    if len(part['trialScores']) not in [30, 120]:\n",
    "        print(part['condition'], part['workerId'], \"MDP Data Incomplete\")\n",
    "        continue\n",
    "        \n",
    "    # Include in analysis only those participants whose data is complete\n",
    "    if not part['demographicsAvailable']:\n",
    "        print(part['condition'], part['workerId'], \"Demographics missing\")\n",
    "        continue\n",
    "    count += 1\n",
    "    all_trial_data['trialScores'] += part['trialScores']\n",
    "    all_trial_data['rewardsWithheld'] += part['rewardsWithheld']\n",
    "    all_trial_data['workerId'] += [part['workerId']] * len(part['trialScores'])\n",
    "    all_trial_data['expectedScores'] += part['expectedScores']\n",
    "    all_trial_data['numClicks'] += part['numClicks']\n",
    "    all_trial_data['avgClickLevel'] += part['avgClickLevel']\n",
    "    all_trial_data['trialNumbers'] += list(range(1,len(part['trialScores'])+1))\n",
    "    all_trial_data['scarce'] += [int(len(part['trialScores']) == 120)] * len(part['trialScores'])\n",
    "    all_trial_data['numRewardedTrials'] += [i - sum(part['rewardsWithheld'][0:i]) for i in range(len(part['trialScores']))]\n",
    "    all_trial_data['numUnrewardedTrials'] += [sum(part['rewardsWithheld'][0:i]) for i in range(len(part['trialScores']))]\n",
    "\n",
    "all_trials_df = pd.DataFrame(all_trial_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2572bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c4fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to use for the remainder of the analysis\n",
    "\n",
    "model_save_folder = \"../results/model_sim_data/\"\n",
    "control_model = \"2.0.3.0.0\"\n",
    "scarce_model = \"2.0.3.0.0\"\n",
    "num_simulations = 1\n",
    "model_file_name = f\"c{control_model}_s{scarce_model}_{num_simulations}.csv\"\n",
    "\n",
    "# Set this to true if the analysis is to be performed on data simulated by the model \n",
    "# (stored in the folder results/model_sim_data)\n",
    "use_model_data = False\n",
    "\n",
    "if use_model_data:\n",
    "    filtered_data = pd.read_csv(model_save_folder + model_file_name)\n",
    "else:\n",
    "    filtered_data = all_trials_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6df2cd",
   "metadata": {},
   "source": [
    "# Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd66de",
   "metadata": {},
   "source": [
    "## Expected Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b16941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting mean expected score vs. trial number for each condition\n",
    "\n",
    "data_to_copy = filtered_data.copy() # all_trials_df, outliers_excluded, or outliers_excluded_part\n",
    "\n",
    "num_scarce = len(data_to_copy.loc[data_to_copy.scarce == 1].groupby('workerId'))\n",
    "num_control = len(data_to_copy.loc[data_to_copy.scarce == 0].groupby('workerId'))\n",
    "\n",
    "# Taking the mean expected score over all trials \n",
    "scarce_data_all = np.array(data_to_copy.loc[data_to_copy.scarce == 1].groupby('trialNumbers').mean()[\"expectedScores\"])\n",
    "scarce_data_all_se = np.array(data_to_copy.loc[data_to_copy.scarce == 1].groupby('trialNumbers').std()[\"expectedScores\"]) / np.sqrt(num_scarce)\n",
    "control_data = np.array(data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers').mean()[\"expectedScores\"])\n",
    "control_data_se = np.array(data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers').std()[\"expectedScores\"]) / np.sqrt(num_scarce)\n",
    "scarce_data_first_30 = scarce_data_all[0:30]\n",
    "scarce_data_first_30_se = scarce_data_all_se[0:30]\n",
    "\n",
    "# Taking only the rewarded trials in the scarce condition\n",
    "rewarded_trials_df = pd.DataFrame(columns=data_to_copy.columns)\n",
    "for worker in np.unique(data_to_copy.workerId):\n",
    "    workerRows = data_to_copy.loc[(data_to_copy.scarce == 1) & (data_to_copy.workerId == worker)].reset_index(drop=True)\n",
    "    rewarded = workerRows.loc[workerRows.rewardsWithheld == False]\n",
    "    rewarded['trialNumbers'] = list(range(1,len(rewarded)+1))\n",
    "    rewarded_trials_df = pd.concat([rewarded_trials_df,rewarded], ignore_index=True)\n",
    "\n",
    "rewarded_trials_df['trialNumbers'] = rewarded_trials_df['trialNumbers'].astype(\"int64\")\n",
    "rewarded_trials_df['scarce'] = rewarded_trials_df['scarce'].astype(\"int64\")\n",
    "rewarded_trials_df['numRewardedTrials'] = rewarded_trials_df['numRewardedTrials'].astype(\"int64\")\n",
    "rewarded_trials_df['numUnrewardedTrials'] = rewarded_trials_df['numUnrewardedTrials'].astype(\"int64\")\n",
    "rewarded_trials_df['rewardsWithheld'] = rewarded_trials_df['rewardsWithheld'].astype(\"boolean\")\n",
    "scarce_data_rewarded = np.array(rewarded_trials_df.groupby('trialNumbers').mean()[\"expectedScores\"])\n",
    "scarce_data_rewarded_sd = np.array(rewarded_trials_df.groupby('trialNumbers').std()[\"expectedScores\"])\n",
    "\n",
    "plt.figure()\n",
    "#plt.title(\"Learning Curves over All Trials\")\n",
    "plt.plot(list(range(len(scarce_data_all))), scarce_data_all, label=\"Scarcity\", color='darkturquoise')\n",
    "plt.fill_between(list(range(len(scarce_data_all))), scarce_data_all + 1.96*scarce_data_all_se, scarce_data_all - 1.96*scarce_data_all_se, alpha=0.15, color='darkturquoise')\n",
    "plt.plot(list(range(len(control_data))), control_data, label=\"Control\", color='firebrick')\n",
    "plt.fill_between(list(range(len(control_data))), control_data + 1.96*control_data_se, control_data - 1.96*control_data_se, alpha=0.15, color='firebrick')\n",
    "plt.axvline(13, linestyle='--', color='darkturquoise')\n",
    "plt.axvline(9, linestyle='--', color='firebrick')\n",
    "#plt.axhline(39.97, color='k', label='Optimal', linestyle=\"--\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.ylim([10,60])\n",
    "plt.xlim([0, 130])\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Average expected trial score\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curves over Rewarded Trials\")\n",
    "plt.plot(list(range(len(scarce_data_rewarded))), scarce_data_rewarded, label=\"Scarce\")\n",
    "plt.plot(list(range(len(control_data))), control_data, label=\"Control\")\n",
    "plt.legend()\n",
    "plt.ylim([10,60])\n",
    "plt.xlim([0, 40])\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Average expected score\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curves over First 30 trials\")\n",
    "plt.plot(list(range(len(scarce_data_first_30))), scarce_data_first_30, label=\"Scarce\")\n",
    "plt.plot(list(range(len(control_data))), control_data, label=\"Control\")\n",
    "plt.legend()\n",
    "plt.ylim([10,60])\n",
    "plt.xlim([0, 40])\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Average expected score\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58507ea",
   "metadata": {},
   "source": [
    "## Clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a7968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting mean expected score vs. trial number for each condition\n",
    "\n",
    "data_to_copy = filtered_data.copy() # all_trials_df, outliers_excluded, or outliers_excluded_part\n",
    "\n",
    "# Taking the mean expected score over all trials \n",
    "scarce_level_all = np.array(data_to_copy.loc[data_to_copy.scarce == 1].groupby('trialNumbers').mean()[\"avgClickLevel\"])\n",
    "scarce_level_all_sd = np.array(data_to_copy.loc[data_to_copy.scarce == 1].groupby('trialNumbers').std()[\"avgClickLevel\"])\n",
    "control_level_all = np.array(data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers').mean()[\"avgClickLevel\"])\n",
    "control_level_sd = np.array(data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers').std()[\"avgClickLevel\"])\n",
    "\n",
    "# Taking the mean expected score over all trials \n",
    "scarce_num_all = np.array(data_to_copy.loc[data_to_copy.scarce == 1].groupby('trialNumbers').mean()[\"numClicks\"])\n",
    "scarce_num_all_sd = np.array(data_to_copy.loc[data_to_copy.scarce == 1].groupby('trialNumbers').std()[\"numClicks\"])\n",
    "control_num_all = np.array(data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers').mean()[\"numClicks\"])\n",
    "control_num_sd = np.array(data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers').std()[\"numClicks\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curves of Number of Clicks\")\n",
    "plt.plot(list(range(len(scarce_num_all))), scarce_num_all, label=\"Scarce\")\n",
    "plt.fill_between(list(range(len(scarce_num_all))), scarce_num_all + scarce_num_all_sd, scarce_num_all - scarce_num_all_sd, alpha=0.1)\n",
    "plt.plot(list(range(len(control_num_all))), control_num_all, label=\"Control\")\n",
    "plt.fill_between(list(range(len(control_num_all))), control_num_all + control_num_sd, control_num_all - control_num_sd, alpha=0.1)\n",
    "plt.axhline(3.56, color='k', label='Optimal', linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.xlim([0, 130])\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Average number of clicks\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curves of Average Click Level\")\n",
    "plt.plot(list(range(len(scarce_level_all))), scarce_level_all, label=\"Scarce\")\n",
    "plt.fill_between(list(range(len(scarce_level_all))), scarce_level_all + scarce_level_all_sd, scarce_level_all - scarce_level_all_sd, alpha=0.1)\n",
    "plt.plot(list(range(len(control_level_all))), control_level_all, label=\"Control\")\n",
    "plt.fill_between(list(range(len(control_level_all))), control_level_all + control_level_sd, control_level_all - control_level_sd, alpha=0.1)\n",
    "plt.axhline(2.968, color='k', label='Optimal', linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.xlim([0, 130])\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Average click level\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d989440f",
   "metadata": {},
   "source": [
    "## Strategy Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155216dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting mean expected score vs. trial number for each condition\n",
    "\n",
    "data_to_copy = filtered_data.copy() # all_trials_df, outliers_excluded, or outliers_excluded_part\n",
    "\n",
    "scarce_trial_group = data_to_copy.loc[data_to_copy.scarce == 1].groupby('trialNumbers')\n",
    "control_trial_group = data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers')\n",
    "\n",
    "# Taking the mean expected score over all trials \n",
    "scarce_data_all = np.array(scarce_trial_group.mean()[\"strategyScores\"])\n",
    "scarce_data_all_sd = np.array(scarce_trial_group.std()[\"strategyScores\"])\n",
    "control_data = np.array(control_trial_group.mean()[\"strategyScores\"])\n",
    "control_data_sd = np.array(control_trial_group.std()[\"strategyScores\"])\n",
    "\n",
    "# Getting the proportions of clusters\n",
    "scarce_data_adaptive = np.array(scarce_trial_group[\"cluster\"].apply(lambda x: (x==0).sum()) / scarce_trial_group[\"cluster\"].count())\n",
    "control_data_adaptive = np.array(control_trial_group[\"cluster\"].apply(lambda x: (x==0).sum()) / control_trial_group[\"cluster\"].count())\n",
    "scarce_data_modadaptive = np.array(scarce_trial_group[\"cluster\"].apply(lambda x: (x==1).sum()) / scarce_trial_group[\"cluster\"].count())\n",
    "control_data_modadaptive = np.array(control_trial_group[\"cluster\"].apply(lambda x: (x==1).sum()) / control_trial_group[\"cluster\"].count())\n",
    "scarce_data_maladaptive = np.array(scarce_trial_group[\"cluster\"].apply(lambda x: (x==2).sum()) / scarce_trial_group[\"cluster\"].count())\n",
    "control_data_maladaptive = np.array(control_trial_group[\"cluster\"].apply(lambda x: (x==2).sum()) / control_trial_group[\"cluster\"].count())\n",
    "#control_data = np.array(data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers').mean()[\"strategyScores\"])\n",
    "#control_data_sd = np.array(data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers').std()[\"strategyScores\"])\n",
    "\n",
    "# Taking only the rewarded trials in the scarce condition\n",
    "rewarded_trials_df = pd.DataFrame(columns=data_to_copy.columns)\n",
    "for worker in np.unique(data_to_copy.workerId):\n",
    "    workerRows = data_to_copy.loc[(data_to_copy.scarce == 1) & (data_to_copy.workerId == worker)].reset_index(drop=True)\n",
    "    rewarded = workerRows.loc[workerRows.rewardsWithheld == False]\n",
    "    rewarded['trialNumbers'] = list(range(1,len(rewarded)+1))\n",
    "    rewarded_trials_df = pd.concat([rewarded_trials_df,rewarded], ignore_index=True)\n",
    "\n",
    "rewarded_trials_df['trialNumbers'] = rewarded_trials_df['trialNumbers'].astype(\"int64\")\n",
    "rewarded_trials_df['scarce'] = rewarded_trials_df['scarce'].astype(\"int64\")\n",
    "rewarded_trials_df['numRewardedTrials'] = rewarded_trials_df['numRewardedTrials'].astype(\"int64\")\n",
    "rewarded_trials_df['numUnrewardedTrials'] = rewarded_trials_df['numUnrewardedTrials'].astype(\"int64\")\n",
    "rewarded_trials_df['rewardsWithheld'] = rewarded_trials_df['rewardsWithheld'].astype(\"boolean\")\n",
    "scarce_data_rewarded = np.array(rewarded_trials_df.groupby('trialNumbers').mean()[\"strategyScores\"])\n",
    "scarce_data_rewarded_sd = np.array(rewarded_trials_df.groupby('trialNumbers').std()[\"strategyScores\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Strategy score Learning Curves over All Trials\")\n",
    "plt.plot(list(range(len(scarce_data_all))), scarce_data_all, label=\"Scarce\")\n",
    "plt.fill_between(list(range(len(scarce_data_all))), scarce_data_all + scarce_data_all_sd, scarce_data_all - scarce_data_all_sd, alpha=0.1)\n",
    "plt.plot(list(range(len(control_data))), control_data, label=\"Control\")\n",
    "plt.fill_between(list(range(len(control_data))), control_data + control_data_sd, control_data - control_data_sd, alpha=0.1)\n",
    "plt.axhline(np.max(np.array(strategy_scores_control_list)[:,1], axis=0), color='k', label='Optimal', linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.xlim([0, 130])\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Average strategy score\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curves over Rewarded Trials\")\n",
    "plt.plot(list(range(len(scarce_data_rewarded))), scarce_data_rewarded, label=\"Scarce\")\n",
    "plt.plot(list(range(len(control_data))), control_data, label=\"Control\")\n",
    "plt.legend()\n",
    "plt.xlim([0, 40])\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Average strategy score\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Strategy Type Change\")\n",
    "alpha = 0.7\n",
    "plt.plot(list(range(len(scarce_data_all))), scarce_data_adaptive, label=\"Scarce Adaptive\", color='b',alpha=alpha)\n",
    "plt.plot(list(range(len(control_data))), control_data_adaptive, label=\"Control Adaptive\",color='orange',alpha=alpha)\n",
    "plt.plot(list(range(len(scarce_data_all))), scarce_data_modadaptive, label=\"Scarce Mod-Adaptive\", color='b',alpha=alpha, linestyle=\"--\")\n",
    "plt.plot(list(range(len(control_data))), control_data_modadaptive, label=\"Control Mod-Adaptive\",color='orange',alpha=alpha, linestyle=\"--\")\n",
    "plt.plot(list(range(len(scarce_data_all))), scarce_data_maladaptive, label=\"Scarce Maldaptive\", color='b',alpha=alpha, linestyle=\":\")\n",
    "plt.plot(list(range(len(control_data))), control_data_maladaptive, label=\"Control Maldaptive\",color='orange',alpha=alpha, linestyle=\":\")\n",
    "plt.legend()\n",
    "plt.ylim([0,1])\n",
    "plt.xlim([0, 130])\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Proportion of Trials\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1590e4",
   "metadata": {},
   "source": [
    "# Comparison of Fitted Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90ca05f",
   "metadata": {},
   "source": [
    "Comparing the results of model fitting to participant data\n",
    "\n",
    "Before running these scripts, run the command to start the local mongo db server. For this, mongodb has to be installed locally (https://www.prisma.io/dataguide/mongodb/setting-up-a-local-mongodb-database#setting-up-mongodb-on-macos) and the anonymized data has to be downloaded and stored in the folder \"/usr/local/var/mongodb\".\n",
    "\n",
    " mongod --dbpath /usr/local/var/mongodb --logpath /usr/local/var/log/mongodb/mongo.log --fork \n",
    "\n",
    "The local mongodb database must have a single document for each fit of the model.\n",
    "\n",
    "This is achieved by running the scripts jsonify_model_data.py and upload_json_to_mongo.py of repository mcl_toolbox on the fitted model .pkl files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcab0f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bce314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Mongo database with model information\n",
    "\n",
    "MONGO_URI_LOCAL=\"localhost:27017\"\n",
    "\n",
    "URI = MONGO_URI_LOCAL\n",
    "client = MongoClient(URI)\n",
    "db = client[\"data\"]\n",
    "collection = db[\"participants\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860ea85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_params(model_doc):\n",
    "    best_params = json.loads(model_doc[\"best_params\"])\n",
    "    model_num = model_doc[\"model\"]\n",
    "    num_priors = len([param for param in best_params.keys() if \"prior\" in param])\n",
    "    num_params = len(best_params)\n",
    "    if \"pr_weight\" in best_params.keys() and model_num[0] != \"3\":\n",
    "        num_params -= 1\n",
    "    return num_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fccb90",
   "metadata": {},
   "source": [
    "Features of the models:\n",
    "\n",
    "Pseudo-rewards (feature value 1 in paper):\n",
    "* p - pseudo-rewards are not used (0)\n",
    "* P - pseudo-rewards are used (1) \n",
    "\n",
    "Fixing of initial strategy weights based on participant's inferred strategy on first trial (feature value 2 in paper)\n",
    "* f - model is not fit with initial weights (0)\n",
    "* F - model is fit with initial weights (1)\n",
    "\n",
    "Whether the model construes no feedback as feedback of 0 (feature value 3 in paper)\n",
    "* u - do not construe absence of feedback as feedback of 0\n",
    "* U - construe absence of feedback as feedback of 0\n",
    "\n",
    "Whether the learning rule was applied after each meta-action, or only once at the end (feature value 4 in paper)\n",
    "* s - applied for each meta-action\n",
    "* S - applied once at the end with the cumulative cost of all meta-actions\n",
    "\n",
    "Ignore the object-level reward \n",
    "* i - don't ignore the object-level reward\n",
    "* I - ignore the object-level reward (models of type Prioritize Self in paper)\n",
    "\n",
    "Which signal to use as feedback\n",
    "* O - environment reward (models of type Prioritize Env in paper)\n",
    "* E - self-learning signal (models of type Prioritize Self in paper)\n",
    "* C - weighted combination of environment reward and self-learning signal (models of type Combination in paper)\n",
    "\n",
    "\n",
    "Model numbers, which consist of 4-5 digits, are translated into the combinations of features as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca9549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_name(name):\n",
    "    pr = ['L','p','P', 'M'] # LVOC, Reinforce no PR, REINFORCE yes PR, REINFORCE with learn from path\n",
    "    fit = ['f', 'F'] # don't fix init weights, fix init weights\n",
    "    action = ['s', 'S'] # don't subtract costs from reward, subtract costs from reward\n",
    "    \n",
    "    model_name_map = {\n",
    "        \"1.1\": \"uiOs\",\n",
    "        \"1.2\": \"UiOs\",\n",
    "        \"2.1\": \"uiOS\",\n",
    "        \"3.0\": \"uIE\",\n",
    "        \"3.1\": \"UiC\",\n",
    "        \"3.2\": \"uiC\",\n",
    "        \"3.3\": \"uiE\"\n",
    "    }\n",
    "    \n",
    "    name_split = name.split(\".\")\n",
    "    action_id = None\n",
    "    if len(name_split) == 5:\n",
    "        # Last digit in model name (when 5 digits) indicates whether feature value 4 is activated or not\n",
    "        action_idx = name_split.pop()\n",
    "        action_id = action[int(action_idx)]\n",
    "        \n",
    "    # First digit in model name is the model index (see next cell for model indices)\n",
    "    # This determines whether feature value 1 is activated or not\n",
    "    model_idx = name_split.pop(0)\n",
    "    model = pr[(int(model_idx)-1)]\n",
    "    \n",
    "    # Second digit in model name is whether feature value 2 is activated or not\n",
    "    fit_idx = name_split.pop(0)\n",
    "    fit_id = fit[int(fit_idx)]\n",
    "    \n",
    "    # Remaining features of the model based on the remaining two digits in the model name are translated\n",
    "    # using the \"model name map\" above\n",
    "    remaining_model = model_name_map[\".\".join(name_split)]\n",
    "    new_model = fit_id + remaining_model + (action_id if action_id else \"\") + model\n",
    "    shortened = ''.join([c for c in new_model if c.isupper()])\n",
    "    return new_model, shortened\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96440e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_evals = 6000\n",
    "\n",
    "model_idxs = [\n",
    "    \"2\",\n",
    "    \"3\"\n",
    "] # 6719, 6687\n",
    "# Models that don't use pseudo rewards and use pseudo rewards, respectively\n",
    "\n",
    "fit_wts = [\n",
    "    \"0\",\n",
    "    \"1\"\n",
    "] # fit initial weights, no/yes\n",
    "\n",
    "# See cell above for meanings of models\n",
    "models = [\n",
    "    \"1.1\",\n",
    "    \"1.2\",\n",
    "    \"2.1\",\n",
    "    \"3.0.0\",\n",
    "    #\"3.0.1\",\n",
    "    \"3.1.0\",\n",
    "    #\"3.1.1\", \n",
    "    \"3.2.0\",\n",
    "    #\"3.2.1\",\n",
    "    \"3.3.0\",\n",
    "    #\"3.3.1\"\n",
    "]\n",
    "\n",
    "all_models = []\n",
    "for model_idx in model_idxs:\n",
    "    for fit_wt in fit_wts:\n",
    "        for model in models:\n",
    "            model_identifier = model_idx + \".\" + fit_wt + \".\" + model\n",
    "            all_models.append(model_identifier)\n",
    "\n",
    "conditions = [\n",
    "    \"control\", \n",
    "    \"scarce\"\n",
    "]\n",
    "condition_trial_nums = {\n",
    "    \"scarce\" : 120,\n",
    "    \"control\" : 30\n",
    "}\n",
    "\n",
    "condition_data_sets = {\n",
    "    \"control\": all_trials_df.loc[(all_trials_df.scarce == 0)],\n",
    "    \"scarce\": all_trials_df.loc[(all_trials_df.scarce == 1)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebebac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "scarce_participants = list(all_trials_df.loc[all_trials_df.scarce == 1].groupby('workerId').mean().index)\n",
    "control_participants = list(all_trials_df.loc[all_trials_df.scarce == 0].groupby('workerId').mean().index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcde918",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_model_name_map = {}\n",
    "for model in all_models:\n",
    "    new_model, shortened = update_model_name(model)\n",
    "    print(model, new_model)\n",
    "    # Uncomment following line to print model features for MATLAB model comparison script\n",
    "    print(' '.join([\"0\" if c.islower() or c == \"O\" else (\"2\" if c == \"C\" else \"1\") for c in new_model]), \";\")\n",
    "    updated_model_name_map[model] = shortened\n",
    "    updated_model_name_map[shortened] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3ff6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to store data about model fits\n",
    "# For each condition, for each model, for each participant\n",
    "model_average_rewards = {}\n",
    "participant_BIC_scores = {}\n",
    "participant_BIC_metrics = {}\n",
    "total_BIC_metrics = {}\n",
    "condition_BIC_metrics = {}\n",
    "participant_AIC_scores = {}\n",
    "total_losses = {}\n",
    "best_eval_id = {}\n",
    "seen_pids = {}\n",
    "for condition in conditions:\n",
    "    model_average_rewards[condition] = {}\n",
    "    participant_BIC_scores[condition] = {}\n",
    "    #participant_BIC_metrics[condition] = {}\n",
    "    participant_AIC_scores[condition] = {}\n",
    "    condition_BIC_metrics[condition] = {}\n",
    "    total_losses[condition] = {}\n",
    "    best_eval_id[condition] = {}\n",
    "    seen_pids[condition] = {}\n",
    "    for model in all_models:\n",
    "        \"\"\"\n",
    "        model_average_rewards[condition][model] = {\n",
    "            \"scores\": np.array([0.] * condition_trial_nums[condition]),\n",
    "            \"expected\": np.array([0.] * condition_trial_nums[condition]),\n",
    "            \"num_parts\": 0\n",
    "        }\n",
    "        \"\"\"\n",
    "        model_average_rewards[condition][model] = {\n",
    "            \"scores\": [],\n",
    "            \"expected\": [],\n",
    "            \"num_clicks\": [],\n",
    "            \"num_parts\": 0\n",
    "        }\n",
    "        \n",
    "        participant_BIC_scores[condition][model] = {}\n",
    "        total_losses[condition][model] = {}\n",
    "        best_eval_id[condition][model] = []\n",
    "        #participant_BIC_metrics[condition][model] = {}\n",
    "        participant_AIC_scores[condition][model] = {}\n",
    "        condition_BIC_metrics[condition][model] = {}\n",
    "        condition_BIC_metrics[condition][model][\"total_loss\"] = 0\n",
    "        condition_BIC_metrics[condition][model][\"total_params\"] = 0\n",
    "        condition_BIC_metrics[condition][model][\"total_m_actions\"] = 0\n",
    "        condition_BIC_metrics[condition][model][\"num_parts\"] = 0\n",
    "        if model not in total_BIC_metrics:\n",
    "            total_BIC_metrics[model] = {}\n",
    "            total_BIC_metrics[model][\"total_loss\"] = 0\n",
    "            total_BIC_metrics[model][\"total_params\"] = 0\n",
    "            total_BIC_metrics[model][\"total_m_actions\"] = 0\n",
    "            total_BIC_metrics[model][\"num_parts\"] = 0\n",
    "        seen_pids[condition][model] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2146497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through each condition and document\n",
    "\n",
    "num_simulations = 30\n",
    "\n",
    "click_cost = {\n",
    "    \"scarce\" : 1.0,\n",
    "    \"control\": 1.0\n",
    "}\n",
    "\n",
    "\n",
    "for condition in conditions:\n",
    "    \n",
    "    for model in all_models:\n",
    "        num_parts = 0\n",
    "        num_missing_r = 0\n",
    "        # Find the documents corresponding to this condition and model\n",
    "        print({ \"condition\": condition, \"model\": model, \"num_evals\": num_evals })\n",
    "        docs = collection.find({ \"condition\": condition, \"model\": model, \"num_evals\": num_evals })\n",
    "        if model not in participant_BIC_metrics:\n",
    "            participant_BIC_metrics[model] = {}\n",
    "        print(condition, model, updated_model_name_map[model])#, len(list(docs)))\n",
    "        for doc in list(docs):\n",
    "            num_params = get_num_params(doc)\n",
    "            pid = doc[\"pid\"]\n",
    "            best_eval = json.loads(doc[\"best_eval\"])\n",
    "            data_set = condition_data_sets[condition]\n",
    "            part_data = data_set.loc[data_set.workerId == pid]\n",
    "            num_meta_actions = part_data[\"numClicks\"].sum() + len(part_data)\n",
    "        \n",
    "            if pid not in seen_pids[condition][model]:\n",
    "                seen_pids[condition][model][pid] = 1\n",
    "            else:\n",
    "                seen_pids[condition][model][pid] += 1\n",
    "            \n",
    "            # Calculate AIC/BIC score\n",
    "            BIC = 2 * best_eval[\"loss\"] + num_params * np.log(num_meta_actions)\n",
    "            AIC = 2 * best_eval[\"loss\"] + num_params * 2\n",
    "            \n",
    "            # Get the losses for the model\n",
    "            all_losses = json.loads(doc[\"all_losses\"])\n",
    "            \n",
    "            # Store the AIC and BIC of each run\n",
    "            \n",
    "            if pid in participant_BIC_scores[condition][model]:\n",
    "                participant_BIC_scores[condition][model][pid].append(BIC)\n",
    "                participant_AIC_scores[condition][model][pid].append(AIC)\n",
    "            else:\n",
    "                participant_BIC_scores[condition][model][pid] = [BIC]\n",
    "                participant_AIC_scores[condition][model][pid] = [AIC]\n",
    "            \n",
    "            \n",
    "            total_losses[condition][model][pid] = all_losses\n",
    "            best_trial = all_losses.index(best_eval[\"loss\"])\n",
    "            best_eval_id[condition][model].append(best_eval[\"tid\"])\n",
    "            \n",
    "            # Store metrics about each individual model for later computation \n",
    "            participant_BIC_metrics[model][pid] = {}\n",
    "            participant_BIC_metrics[model][pid][\"loss\"] = best_eval[\"loss\"]\n",
    "            participant_BIC_metrics[model][pid][\"params\"] = num_params\n",
    "            participant_BIC_metrics[model][pid][\"m_actions\"] = num_meta_actions\n",
    "            participant_BIC_metrics[model][pid][\"condition\"] = condition\n",
    "            \n",
    "            # Accumulate metrics about each individual model for later computation\n",
    "            # of model BIC score over all participants for a given model\n",
    "            total_BIC_metrics[model][\"total_loss\"] += best_eval[\"loss\"]\n",
    "            total_BIC_metrics[model][\"total_params\"] += num_params\n",
    "            total_BIC_metrics[model][\"total_m_actions\"] += num_meta_actions\n",
    "            total_BIC_metrics[model][\"num_parts\"] += 1\n",
    "            \n",
    "            # Accumulate metrics about each individual model for later computation\n",
    "            # of model BIC score over all participants for a given condition\n",
    "            condition_BIC_metrics[condition][model][\"total_loss\"] += best_eval[\"loss\"]\n",
    "            condition_BIC_metrics[condition][model][\"total_params\"] += num_params\n",
    "            condition_BIC_metrics[condition][model][\"total_m_actions\"] += num_meta_actions\n",
    "            condition_BIC_metrics[condition][model][\"num_parts\"] += 1\n",
    "            \n",
    "            num_parts += 1\n",
    "            # Get the simulation results for the model\n",
    "            if \"r\" not in doc:\n",
    "                #print(pid)\n",
    "                #print(doc)\n",
    "                num_missing_r += 1\n",
    "        \n",
    "            else:\n",
    "                rewards = json.loads(doc[\"r\"])\n",
    "                expected = json.loads(doc[\"mer\"])\n",
    "                clicks = json.loads(doc[\"num_clicks\"])\n",
    "                \n",
    "                for sim_num in range(len(expected)):\n",
    "                    for trial_num in range(len(expected[sim_num])):\n",
    "                        expected[sim_num][trial_num] -= clicks[sim_num][trial_num]\n",
    "                \n",
    "                expected = np.array(expected)\n",
    "                \n",
    "                # Add the average score of the simulations to the average scores for that model\n",
    "                model_average_rewards[condition][model][\"scores\"] += list(rewards) #.append(list(rewards))\n",
    "                model_average_rewards[condition][model][\"expected\"] += list(expected) #.append(list(expected))\n",
    "                model_average_rewards[condition][model][\"num_clicks\"] += list(clicks) #.append(list(clicks))\n",
    "                model_average_rewards[condition][model][\"num_parts\"] += 1\n",
    "        print(num_parts)\n",
    "        print(num_missing_r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d52f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any models missing\n",
    "\n",
    "participants = {\n",
    "    \"control\": control_participants,\n",
    "    \"scarce\": scarce_participants\n",
    "}\n",
    "expected_num_runs = 2\n",
    "total_missing = 0\n",
    "for condition in conditions:\n",
    "    print(condition + \"\\n\")\n",
    "    for model in all_models:\n",
    "        print(\"\\n\" + model)\n",
    "        for pid in participants[condition]:\n",
    "            if pid not in seen_pids[condition][model]:\n",
    "                print(pid, 0)\n",
    "                total_missing += 1\n",
    "            elif seen_pids[condition][model][pid] != expected_num_runs:\n",
    "                print(pid, seen_pids[condition][model][pid])\n",
    "                total_missing += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f077e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate single BIC score over all participants for a single model\n",
    "model_BIC = pd.DataFrame.from_dict(total_BIC_metrics, orient='index')\n",
    "\n",
    "# Calculate single BIC score per condition\n",
    "scarce_BIC = pd.DataFrame.from_dict(condition_BIC_metrics[\"scarce\"], orient='index')\n",
    "control_BIC = pd.DataFrame.from_dict(condition_BIC_metrics[\"control\"], orient='index')\n",
    "\n",
    "# Create dataframe of individual participant metrics for bootstrapping\n",
    "participant_BIC = pd.DataFrame.from_dict(participant_BIC_metrics)\n",
    "\n",
    "# Create dataframes of participant BIC/AIC scores\n",
    "scarce_part_BIC = pd.DataFrame.from_dict(participant_BIC_scores[\"scarce\"])\n",
    "control_part_BIC = pd.DataFrame.from_dict(participant_BIC_scores[\"control\"])\n",
    "#original_BIC = pd.DataFrame.from_dict(model_BIC_scores[\"original\"])\n",
    "scarce_part_AIC = pd.DataFrame.from_dict(participant_AIC_scores[\"scarce\"])\n",
    "control_part_AIC = pd.DataFrame.from_dict(participant_AIC_scores[\"control\"])\n",
    "#original_AIC = pd.DataFrame.from_dict(model_AIC_scores[\"original\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af39742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a subset from a dataframe where each cell is a list of values\n",
    "import random\n",
    "\n",
    "def sample_subset(df):\n",
    "    new_df = df.copy()\n",
    "    for col in df.columns:\n",
    "        for part in df.index:\n",
    "            vals = df[col][part]\n",
    "            try:\n",
    "                new_df[col][part] = np.random.choice(vals)\n",
    "            except:\n",
    "                new_df[col][part] = np.nan\n",
    "    \n",
    "    return new_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf1ad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample several bootstrapped BIC datasets\n",
    "\n",
    "start = 1574\n",
    "end = 10000\n",
    "\n",
    "# Preparing dataframe for MATLAB script SPM_bm.m\n",
    "for i in range(start,end+1):\n",
    "    sample_num = i+1\n",
    "    scarce_sample = sample_subset(scarce_part_BIC).dropna()\n",
    "    control_sample = sample_subset(control_part_BIC).dropna()\n",
    "    \n",
    "    csv_file_scarce = f\"../results/mcrl/bic_datasets/scarce_bic_{num_evals}_{sample_num}.csv\"\n",
    "    csv_file_control = f\"../results/mcrl/bic_datasets/control_bic_{num_evals}_{sample_num}.csv\"\n",
    "\n",
    "\n",
    "    # Save to CSV\n",
    "    scarce_sample.to_csv(csv_file_scarce)\n",
    "    control_sample.to_csv(csv_file_control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ebcd8",
   "metadata": {},
   "source": [
    "# Analysis of Model Comparison with several bootstrapped datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e4b158",
   "metadata": {},
   "source": [
    "In order to test the robustness of the results of the model comparison, the same comparison procedure (spm_BMS and spm_compare_families) was performed on several datasets of model BIC scores. Each model had a certain set of features and was fit to a particular participant's data twice. The results of these models were used to bootstrap the several datasets, and the model comparison was performed on each of the datasets.\n",
    "\n",
    "The following section contains code to parse and compile the results of the model comparison on each of the datasets, to see which results are robust and how confident we can be of the conclusions drawn from the model comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff1e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/7684333/converting-xml-to-dictionary-using-elementtree\n",
    "\n",
    "def etree_to_dict(t):\n",
    "    d = {t.tag: {} if t.attrib else None}\n",
    "    children = list(t)\n",
    "    if children:\n",
    "        dd = defaultdict(list)\n",
    "        for dc in map(etree_to_dict, children):\n",
    "            for k, v in dc.items():\n",
    "                dd[k].append(v)\n",
    "        d = {t.tag: {k: v[0] if len(v) == 1 else v\n",
    "                     for k, v in dd.items()}}\n",
    "    if t.attrib:\n",
    "        d[t.tag].update(('@' + k, v)\n",
    "                        for k, v in t.attrib.items())\n",
    "    if t.text:\n",
    "        text = t.text.strip()\n",
    "        if children or t.attrib:\n",
    "            if text:\n",
    "                d[t.tag]['#text'] = text\n",
    "        else:\n",
    "            d[t.tag] = text\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36cc220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files\n",
    "\n",
    "results_folder = \"../results/spm_results\"\n",
    "results_files = os.listdir(results_folder)\n",
    "\n",
    "metrics = [\"exp_r\", \"xp\"]\n",
    "\n",
    "# Return a list of pandas column names for a list of data categories\n",
    "# by appending the name of the metric to the categories\n",
    "def get_all_df_cols_for_names(names, init_cols=[]):\n",
    "    new_list = [*init_cols]\n",
    "    for name in names:\n",
    "        for metric in metrics:\n",
    "            new_list.append(name + \" \" + metric)\n",
    "    return new_list\n",
    "\n",
    "\n",
    "# cols = {\n",
    "#     \"bms\": get_all_df_cols_for_names(all_models, [\"condition\"]),\n",
    "#     \"comp_1\": get_all_df_cols_for_names([\"No Fix\", \"Fix\"], [\"condition\"]),\n",
    "#     \"comp_2\": get_all_df_cols_for_names([\"None\", \"Zero\"]),\n",
    "#     \"comp_3\": get_all_df_cols_for_names([\"Use OL\", \"Ignore OL\"], [\"condition\"]),\n",
    "#     \"comp_4\": get_all_df_cols_for_names([\"No PR\", \"PR\"], [\"condition\"]),\n",
    "#     \"comp_5\": get_all_df_cols_for_names([\"No ES\", \"ES\"], [\"condition\"]),\n",
    "#     \"comp_6\": get_all_df_cols_for_names([\"Only OL\", \"Only ER\", \"Both\"], [\"condition\"]),\n",
    "#     \"comp_7\": get_all_df_cols_for_names([\"Only OL\", \"Only ER\", \"ER only when no OL\",\"Some ER Always\"], [\"condition\"]),\n",
    "# }\n",
    "\n",
    "dataframe_dict_lists = {}\n",
    "\n",
    "control_count = 0\n",
    "scarce_count = 0\n",
    "max_count_per_cond = 8800\n",
    "\n",
    "# Converting result xml files into dataframes \n",
    "# One dataframe for each analysis - 1 individual model analysis and 7 family comparisons\n",
    "for file in results_files:\n",
    "    full_file_path = os.path.join(results_folder, file)\n",
    "    condition = 1 if \"scarce\" in file else 0\n",
    "    try:\n",
    "        etree = ET.parse(full_file_path)\n",
    "    except:\n",
    "        print(file)\n",
    "        continue\n",
    "        \n",
    "    if max(condition * scarce_count, (1-condition) * control_count) >= max_count_per_cond:\n",
    "        continue\n",
    "    #print(ET.dump(etree.getroot()))\n",
    "    \n",
    "    control_count += 1-condition\n",
    "    scarce_count += condition\n",
    "    xml_dict = etree_to_dict(etree.getroot())['struct']\n",
    "    #print(xml_dict)\n",
    "    for analysis, outcome in xml_dict.items():\n",
    "        if analysis not in dataframe_dict_lists:\n",
    "            dataframe_dict_lists[analysis] = []\n",
    "        analysis_dict = {\n",
    "            \"condition\": condition\n",
    "        }\n",
    "        # Individual model comparison\n",
    "        if analysis == \"bms\":\n",
    "            for idx, model in enumerate(outcome[\"models\"]):\n",
    "                model_name = model.replace(\"x\", \"\").replace(\"_\", \".\")\n",
    "                relevant_cols = get_all_df_cols_for_names([model_name])\n",
    "                for relevant_col in relevant_cols:\n",
    "                    for metric in metrics:\n",
    "                        if (\" \" + metric) in relevant_col:\n",
    "                            analysis_dict[relevant_col] = float(outcome[metric][idx])\n",
    "                \n",
    "        # Family comparisons\n",
    "        elif analysis.startswith(\"comp\"):\n",
    "            for idx, family_name in enumerate(outcome[\"names\"]):\n",
    "                relevant_cols = get_all_df_cols_for_names([family_name])\n",
    "               \n",
    "                for relevant_col in relevant_cols:\n",
    "                    for metric in metrics:\n",
    "                        if (\" \" + metric) in relevant_col:\n",
    "                            #print(metric, relevant_col, outcome[metric][idx])\n",
    "                            analysis_dict[relevant_col] = float(outcome[metric][idx])\n",
    "                \n",
    "        dataframe_dict_lists[analysis].append(analysis_dict)\n",
    "    \n",
    "\n",
    "# Create one dataframe for each analysis\n",
    "# bms - individual model analysis\n",
    "# comp1 - family comparison of fixing of strategy weights (feature value 2 = 1 vs. 0)\n",
    "# comp2 - family comparison of feature value 3 = 1 vs.0 \n",
    "# comp3 - family comparison of models that use environment reward and models that dont\n",
    "# comp4 - family comparison of models that use pseudo rewards vs. those that don't (fv 1 = 1 vs. 0)\n",
    "# comp5 - family comparison of models that use self-generated reward vs those that don't\n",
    "# comp6 - family comparison of models that exclusively use one signal (environment or self-generated rewards) or both\n",
    "# comp7 - family  comparison of how models use learning signals - only env reward, only self-generated, self-generated when env reward is absent, or always in combination\n",
    "dataframes = {key: pd.DataFrame.from_dict(value) for key, value in dataframe_dict_lists.items()}\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889cfa00",
   "metadata": {},
   "source": [
    "Descriptions of family names:\n",
    "\n",
    "* comp_1\n",
    "    * Fix - Initial weight priors are fixed by participant's initial strategy\n",
    "    * No Fix - Initial weight priors are standard normal distributions\n",
    "* comp_2\n",
    "    * None - Absent reward construed as such\n",
    "    * Zero - Absent rewrad construed as reward of zero\n",
    "* comp_3\n",
    "    * Ignore OL - Ignore the environment reward when learning\n",
    "    * Use OL - Use the environment reward to learn where present\n",
    "* comp_4\n",
    "    * No PR - No pseudo rewards used\n",
    "    * PR - Pseudo rewards used\n",
    "* comp_5\n",
    "    * No ES - self-generated expected reward not used\n",
    "    * ES - self-generated expected reward used\n",
    "* comp_6\n",
    "    * Only OL - Uses only the environment reward for learning\n",
    "    * Only ER - Uses only the self-generated expected reward for learning\n",
    "    * Both - Uses both of the above for learning\n",
    "* comp_7\n",
    "    * Only OL - Uses only the environment reward for learning\n",
    "    * Only ER - Uses only the self-generated expected reward for learning\n",
    "    * ER only when no OL - Uses the self-generated expected reward for learning when there is no env reward\n",
    "    * Some ER always - Always uses self-generated expected reward in combination with the env reward (if present) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4879873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_descriptions = {\n",
    "    \"bms\": \"Individual model analysis\",\n",
    "    \"comp_1\": \"Fixing initial weights\",\n",
    "    \"comp_2\": \"Absent reward = reward 0?\",\n",
    "    \"comp_3\": \"Use environment reward?\",\n",
    "    \"comp_4\": \"Use Pseudo Reward?\",\n",
    "    \"comp_5\": \"Use Expected Reward?\",\n",
    "    \"comp_6\": \"Exclusive use of signals\",\n",
    "    \"comp_7\": \"How Expected Reward is used\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a393e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating and plotting 95% CIs for the outcomes of all analyses\n",
    "\n",
    "fig, axes = plt.subplots(len(dataframes), len(conditions), figsize=(7 * len(conditions), 7 * len(dataframes)))\n",
    "for an_idx, (analysis, df) in enumerate(dataframes.items()):\n",
    "    control = df.loc[df.condition == 0]\n",
    "    scarce = df.loc[df.condition == 1]\n",
    "    \n",
    "    for cond_idx, condition in enumerate(conditions):\n",
    "        ax = axes[an_idx][cond_idx]\n",
    "        for metric in metrics:\n",
    "            relevant_columns = [column for column in df.columns if (\" \" + metric) in column]\n",
    "            relevant_df = df.loc[df.condition == cond_idx][relevant_columns]\n",
    "            mean = relevant_df.mean()\n",
    "            sd = relevant_df.std()\n",
    "            # confidence level\n",
    "            z = 1.96\n",
    "            \n",
    "            num_samples = len(relevant_df)\n",
    "            error_margin = z * sd / np.sqrt(num_samples)\n",
    "            if analysis != \"bms\":# and metric == \"exp_r\":\n",
    "                # Printing mean statistics and confidence intervals\n",
    "                print(condition, analysis)\n",
    "                print(\"\\t\",list(relevant_df.columns))\n",
    "                print(\"M\\t\",np.array(mean))\n",
    "                print(\"LB\\t\",np.array(mean) - np.array(error_margin))\n",
    "                print(\"UB\\t\", np.array(mean) + np.array(error_margin))\n",
    "                print(\"E\\t\", np.array(error_margin))\n",
    "                print(\"\\n\")\n",
    "            \n",
    "            # Printing aggregate statistics of proportions of participants who use\n",
    "            # expected reward on trials with feedback vs. those without\n",
    "            # with 95% CI\n",
    "            if analysis == \"comp_7\" and metric == \"exp_r\":\n",
    "                sum1_columns = [\"Only ER exp_r\", \"ER only when no OL exp_r\", \"Some ER Always exp_r\"]\n",
    "                sums1 = np.array(relevant_df[sum1_columns].fillna(0)).sum(axis=1)\n",
    "                sum1_mean = sums1.mean()\n",
    "                sum1_sd = sums1.std()\n",
    "                sum1_err = z* sum1_sd / np.sqrt(num_samples)\n",
    "                sum2_columns = [\"Only ER exp_r\", \"Some ER Always exp_r\"]\n",
    "                sums2 = np.array(relevant_df[sum2_columns].fillna(0)).sum(axis=1)\n",
    "                sum2_mean = sums2.mean()\n",
    "                sum2_sd = sums2.std()\n",
    "                sum2_err = z* sum2_sd / np.sqrt(num_samples)\n",
    "                print(condition, \"Proportion of participants using ER when no feedback\")\n",
    "                print(\"M\\t\", sum1_mean)\n",
    "                print(\"LB\\t\", sum1_mean-sum1_err)\n",
    "                print(\"UB\\t\", sum1_mean+sum1_err)\n",
    "                print(\"E\\t\", sum1_err)\n",
    "                print(condition, \"Proportion of participants using ER even with feedback\")\n",
    "                print(\"M\\t\", sum2_mean)\n",
    "                print(\"LB\\t\", sum2_mean-sum2_err)\n",
    "                print(\"UB\\t\", sum2_mean+sum2_err)\n",
    "                print(\"E\\t\", sum2_err)\n",
    "                \n",
    "            #print(mean)\n",
    "            axis_labels = [\" \".join(col.split(\" \")[:-1]) for col in relevant_columns]\n",
    "            ax.scatter(axis_labels,mean, label=metric)\n",
    "            ax.errorbar(axis_labels,mean, yerr=error_margin,fmt='o',capsize=4)\n",
    "            if analysis == \"bms\":\n",
    "                ax.set_xticklabels(axis_labels,rotation=90,fontsize=9)\n",
    "            else:\n",
    "                ax.set_xticklabels(axis_labels,rotation=45,fontsize=14)\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"{analysis_descriptions[analysis]}, {condition}, {len(relevant_df)}\", fontsize=14)\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cca73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking qualitative results - how frequently is a family the winning family?\n",
    "# A family is the winning family if the exceedance probability is above 75%\n",
    "\n",
    "xp_threshold = 0.75\n",
    "\n",
    "for analysis, df in dataframes.items():\n",
    "    if analysis == \"bms\": \n",
    "        continue\n",
    "    for idx, condition in enumerate([\"control\", \"scarce\"]):\n",
    "        relevant_df = df.loc[df.condition == idx].copy()\n",
    "        xp_cols = [col for col in relevant_df.columns if (\" xp\" in col)]\n",
    "        relevant_df = relevant_df[xp_cols]\n",
    "        relevant_df = relevant_df.where(relevant_df > xp_threshold, 0)\n",
    "        relevant_df = relevant_df.where(relevant_df == 0, 1)\n",
    "        print(\"\\n\",analysis, condition, len(relevant_df))\n",
    "        print([\" \".join(col.split(\" \")[:-1]) for col in relevant_df.columns])\n",
    "        print(relevant_df.sum().values)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908be644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prettier graph of results of last comparison for the paper\n",
    "\n",
    "df = dataframes[\"comp_7\"]\n",
    "control_df = df.loc[df.condition == 0]\n",
    "scarce_df = df.loc[df.condition == 1]\n",
    "\n",
    "relevant_columns = [col for col in scarce_df.columns if (\" exp_r\" in col)]\n",
    "\n",
    "meanc = control_df[relevant_columns].mean()\n",
    "means = scarce_df[relevant_columns].mean()\n",
    "\n",
    "sdc = control_df[relevant_columns].std()\n",
    "sds = scarce_df[relevant_columns].std()\n",
    "\n",
    "# confidence level\n",
    "z = 1.96\n",
    "\n",
    "num_samples = len(control_df)\n",
    "errc = z * sdc / np.sqrt(num_samples)\n",
    "errs = z * sds / np.sqrt(num_samples)\n",
    "\n",
    "relevant_columns = [col for col in scarce_df.columns if (\" exp_r\" in col)]\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "axis_labels = [\"V\", \"PS\", \"PE\", \"C\"]\n",
    "plt.scatter(axis_labels,meanc, label=\"Control\",color='firebrick',marker=\"^\", s=40)\n",
    "plt.scatter(axis_labels,means, label=\"Scarcity\",color='darkturquoise', s=40)\n",
    "#plt.errorbar(axis_labels,meanc, yerr=errc,capsize=4,fmt='o', color='firebrick')\n",
    "#plt.errorbar(axis_labels,means, yerr=errs,capsize=4,fmt='o', color='darkturquoise')\n",
    "plt.ylabel('Prop. of Ps Explained')\n",
    "plt.xlabel('Model Type')\n",
    "plt.ylim([0,1])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e821f68",
   "metadata": {},
   "source": [
    "# 1.10 - Bar Chart of Model BMS Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ea0c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the BMS results\n",
    "\n",
    "cbms_file = \"../results/mcrl/cBMS_results.csv\"\n",
    "sbms_file = \"../results/mcrl/sBMS_results.csv\"\n",
    "\n",
    "cbms_rows = []\n",
    "sbms_rows = []\n",
    "\n",
    "with open(cbms_file, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        cbms_rows.append(row)\n",
    "        \n",
    "with open(sbms_file, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        sbms_rows.append(row)\n",
    "        \n",
    "cbms_rows = [row[1:] for row in cbms_rows[0:3]]\n",
    "sbms_rows = [row[1:] for row in sbms_rows[0:3]]\n",
    "\n",
    "cbms_rows[1] = [float(val) for val in cbms_rows[1]]\n",
    "cbms_rows[2] = [float(val) for val in cbms_rows[2]]\n",
    "\n",
    "sbms_rows[1] = [float(val) for val in sbms_rows[1]]\n",
    "sbms_rows[2] = [float(val) for val in sbms_rows[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc58e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summing up proportions of both conditions\n",
    "\n",
    "import copy, functools\n",
    "\n",
    "summed_props = copy.deepcopy(cbms_rows)\n",
    "\n",
    "for i in range(len(summed_props[0])):\n",
    "    summed_props[1][i] += sbms_rows[1][sbms_rows[0].index(summed_props[0][i])]\n",
    "    \n",
    "# Sort models according to summed proportions\n",
    "\n",
    "model_props = [(x, y) for x,y in zip(summed_props[0], summed_props[1])] \n",
    "\n",
    "def sort_fn(a, b):\n",
    "    if b[1] > a[1]:\n",
    "        return 1\n",
    "    elif b[1] < a[1]:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "sorted_models = sorted(model_props, key=functools.cmp_to_key(sort_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_name(name):\n",
    "    # Remove x\n",
    "    name = name[1:]\n",
    "    \n",
    "    model_type = None\n",
    "    features = [0,0,0,0]\n",
    "    name_split = name.split(\"_\")\n",
    "    # PR\n",
    "    model = name_split.pop(0)\n",
    "    features[0] = int(model) - 2\n",
    "    \n",
    "    # Fix wts\n",
    "    fix_wts = name_split.pop(0)\n",
    "    features[1] = int(fix_wts)\n",
    "    \n",
    "    if len(name_split) == 3:\n",
    "        # Subtract costs\n",
    "        sub = name_split.pop()\n",
    "        features[3] = int(sub)\n",
    "        \n",
    "        # Remaining model\n",
    "        remaining_model = '_'.join(name_split)\n",
    "        if remaining_model == \"3_0\":\n",
    "            model_type = \"PS\"\n",
    "            features[2] = 0\n",
    "        elif remaining_model == \"3_1\":\n",
    "            model_type = \"C\"\n",
    "            features[2] = 1\n",
    "        elif remaining_model == \"3_2\":\n",
    "            model_type = \"C\"\n",
    "            features[2] = 0\n",
    "        elif remaining_model == \"3_3\":\n",
    "            model_type = \"PE\"\n",
    "            features[2] = 0\n",
    "    else:\n",
    "        # Remaining model\n",
    "        remaining_model = '_'.join(name_split)\n",
    "        if remaining_model == \"1_1\":\n",
    "            model_type = \"V\"\n",
    "            features[2] = 0\n",
    "            features[3] = 0\n",
    "        elif remaining_model == \"1_2\":\n",
    "            model_type = \"V\"\n",
    "            features[2] = 1\n",
    "            features[3] = 0\n",
    "        elif remaining_model == \"2_1\":\n",
    "            model_type = \"V\"\n",
    "            features[2] = 0\n",
    "            features[3] = 1\n",
    "    \n",
    "    features = [str(f) for f in features]\n",
    "    return model_type+\", \" + \"\".join(features)\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76cf97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model, prop in sorted_models:\n",
    "    print(\"{}\\t{}\".format(model, convert_model_name(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc86c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create stacked bar plot\n",
    "\n",
    "num_models_to_plot = 38# len(sorted_models)\n",
    "\n",
    "control_props = [cbms_rows[1][cbms_rows[0].index(model)] for model, prop in sorted_models[0:num_models_to_plot]]\n",
    "scarce_props = [sbms_rows[1][sbms_rows[0].index(model)] for model, prop in sorted_models[0:num_models_to_plot]]\n",
    "labels = [convert_model_name(model) for model,prop in sorted_models[0:num_models_to_plot]]\n",
    "xpos = [2.5*i for i in range(len(labels))][::-1]\n",
    "\n",
    "print(sum(control_props) + sum(scarce_props))\n",
    "\n",
    "fig = plt.figure(figsize=(6,6.5))\n",
    "width = 2.0\n",
    "xlim = 0.2\n",
    "# plt.barh(labels, control_props, width, label='Control')\n",
    "# plt.barh(labels, scarce_props, width, label='Scarce', left=control_props)\n",
    "for i in range(len(control_props)):\n",
    "    plt.barh(xpos[i], control_props[i], height=width,label='Control', color='firebrick', align='edge')\n",
    "    bar = plt.barh(xpos[i], scarce_props[i], height=width,label='Scarce', left=control_props[i], color='darkturquoise', align='edge')\n",
    "    bar = list(bar)[0]\n",
    "    w, h = bar.get_width(), bar.get_height()\n",
    "    x0, y0 = bar.xy\n",
    "    right_coords = x0 + w + 0.002, y0 + h/2\n",
    "    plt.text(*right_coords, labels[i], va='center', fontsize=8)#,bbox=dict(facecolor='none', edgecolor='red'))\n",
    "#     bar1 = plt.barh(xpos[i], control_props[i], height=width,label='Control', color='firebrick', align='edge')\n",
    "#     bar2 = plt.barh(xpos[i], scarce_props[i], height=width,label='Scarce', left=xlim-scarce_props[i], color='darkturquoise', align='edge')\n",
    "#     bar1 = list(bar1)[0]\n",
    "#     w, h = bar1.get_width(), bar1.get_height()\n",
    "#     x0, y0 = bar1.xy\n",
    "#     right_coords = x0 + w + 0.002, y0 + h/2\n",
    "#     plt.text(xlim/2, y0+h/2, labels[i], ha='center', va='center', fontsize=8)#,bbox=dict(facecolor='none', edgecolor='red'))\n",
    "plt.legend(['Control', 'Scarcity'], prop={'size': 12})\n",
    "plt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\n",
    "plt.xticks(np.linspace(0,0.2,5))\n",
    "plt.xlabel(\"Prop. of participants in condition best explained\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.xlim([0, xlim])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6bdbd1",
   "metadata": {},
   "source": [
    "# End of Analyses Reported in the Paper\n",
    "\n",
    "The model comparison takes place using the MATLAB script SPM_bm.m. The remainder of this notebook is irrelevant to the results reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9333ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting the model performance against participant performance\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "column = \"expected\" #expected, scores\n",
    "df_column = \"expectedScores\" #trialScores, expectedScores\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(all_models),ncols=len(conditions),figsize=(5 * len(conditions), 3 * len(all_models)),squeeze=False)\n",
    "ax = ax.flatten()\n",
    "\n",
    "show_std = False\n",
    "\n",
    "plot_idx = 0\n",
    "\n",
    "model_label_map = {\n",
    "    \n",
    "}\n",
    "\n",
    "for model in all_models:\n",
    "    updated_name = updated_model_name_map[model]\n",
    "    for condition in conditions:\n",
    "        model_exp_c = np.array(model_average_rewards[condition][model][column])\n",
    "        avg = model_exp_c.mean(axis=0)\n",
    "        std = model_exp_c.std(axis=0)\n",
    "        part_data = condition_data_sets[condition]\n",
    "        print(condition, len(part_data))\n",
    "        c_data = np.array(part_data.loc[(part_data.workerId.isin(seen_pids[condition][model]))].groupby('trialNumbers').mean()[df_column])\n",
    "        c_data_sd = np.array(part_data.loc[(part_data.workerId.isin(seen_pids[condition][model]))].groupby('trialNumbers').std()[df_column])\n",
    "        try:\n",
    "            ax[plot_idx].plot(range(len(avg)), avg, label=\"model\", color='blue')\n",
    "            if show_std:\n",
    "                ax[plot_idx].fill_between(\n",
    "                    list(range(len(avg))),\n",
    "                    avg + std,\n",
    "                    avg - std,\n",
    "                    alpha=0.1\n",
    "                )\n",
    "            ax[plot_idx].plot(range(len(c_data)), c_data, label=\"participants\", color='orange')\n",
    "            if show_std:\n",
    "                ax[plot_idx].fill_between(\n",
    "                    list(range(len(c_data))),\n",
    "                    c_data + c_data_sd,\n",
    "                    c_data - c_data_sd,\n",
    "                    alpha=0.1\n",
    "                )\n",
    "            ax[plot_idx].legend()\n",
    "        except:\n",
    "            pass\n",
    "        ax[plot_idx].set_title(f\"{updated_name} - {condition}\")\n",
    "    \n",
    "        plot_idx += 1\n",
    "    \n",
    "\n",
    "\n",
    "plt.subplots_adjust(\n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b72fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping between models and which family they belong\n",
    "\n",
    "model_name_update = [\"x\"+\"_\".join(m.split(\".\")) for m in all_models]\n",
    "model_family_map = {}\n",
    "\n",
    "for m, um in zip(all_models, model_name_update):\n",
    "    model_family_map[m] = convert_model_name(um).split(\",\")[0]\n",
    "    \n",
    "model_families = list(np.unique(list(model_family_map.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7afac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(model_average_rewards[\"scarce\"][\"2.0.1.1\"][\"expected\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709aad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "column = \"expected\" #expected, scores\n",
    "df_column = \"expectedScores\" #trialScores, expectedScores\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1,ncols=len(conditions),figsize=(5 * len(conditions), 3),squeeze=False)\n",
    "ax = ax.flatten()\n",
    "\n",
    "show_std = False\n",
    "\n",
    "colors = [\"orange\", \"red\", \"green\", \"gold\"]\n",
    "\n",
    "for plot_idx, condition in enumerate(conditions):\n",
    "    part_data = condition_data_sets[condition]\n",
    "    N = len(part_data) / np.max(part_data.trialNumbers)\n",
    "    print(condition, N)\n",
    "    c_data = np.array(part_data.groupby('trialNumbers').mean()[df_column])\n",
    "    c_data_se = np.array(part_data.groupby('trialNumbers').std()[df_column]) / np.sqrt(N) \n",
    "    ax[plot_idx].plot(range(len(c_data)), c_data, label=\"participants\", color='orange')\n",
    "    if show_std:\n",
    "        ax[plot_idx].fill_between(\n",
    "            list(range(len(c_data))),\n",
    "            c_data + c_data_sd,\n",
    "            c_data - c_data_sd,\n",
    "            alpha=0.1\n",
    "        )\n",
    "    for fidx, family in enumerate(model_families):\n",
    "        rel_model_list = [m for m in all_models if model_family_map[m]==family]\n",
    "        family_total_rewards = None\n",
    "        total_rew = 0\n",
    "        for m in rel_model_list:\n",
    "            print(condition, m, column)\n",
    "            mrew = model_average_rewards[condition][m][column]\n",
    "            if family_total_rewards is None:\n",
    "                family_total_rewards = np.array(mrew)\n",
    "            else:\n",
    "                family_total_rewards = np.append(family_total_rewards, np.array(mrew), axis=0)\n",
    "            \n",
    "        avg = family_total_rewards.mean(axis=0)\n",
    "        std = family_total_rewards.std(axis=0) / np.sqrt(family_total_rewards.shape[0])\n",
    "        print(family, family_total_rewards.shape[0])\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting model losses\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(all_models),ncols=len(conditions),figsize=(5 * len(conditions), 3 * len(all_models)),squeeze=False)\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Set to true to also see confidence interval on true average \n",
    "show_std = True\n",
    "\n",
    "plot_idx = 0\n",
    "\n",
    "model_label_map = {\n",
    "    \n",
    "}\n",
    "\n",
    "for model in all_models:\n",
    "    updated_name = updated_model_name_map[model]\n",
    "    for condition in conditions:\n",
    "        all_losses = total_losses[condition][model]\n",
    "        losses_list = [value for key,value in all_losses.items() if len(value) == num_evals]\n",
    "        losses = np.array(losses_list)\n",
    "        try:\n",
    "            avg = losses.mean(axis=0)\n",
    "            std = losses.std(axis=0) / np.sqrt(losses.shape[0])\n",
    "        except Exception as e:\n",
    "            avg = losses.mean(axis=0)\n",
    "            std = losses.std(axis=0) / np.sqrt(losses.shape[0])\n",
    "        try:\n",
    "            ax[plot_idx].plot(range(len(avg)), avg, label=\"model\", color='blue')\n",
    "            if show_std:\n",
    "                ax[plot_idx].fill_between(\n",
    "                    list(range(len(avg))),\n",
    "                    avg + 1.96 * std,\n",
    "                    avg - 1.96 * std,\n",
    "                    alpha=0.9\n",
    "                )\n",
    "        except:\n",
    "            pass\n",
    "        ax[plot_idx].set_title(f\"{updated_name} - {model} - {condition}\")\n",
    "    \n",
    "        plot_idx += 1\n",
    "    \n",
    "\n",
    "\n",
    "plt.subplots_adjust(\n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2bf114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Observing size of individual loss improvement\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(all_models),ncols=len(conditions),figsize=(5 * len(conditions), 3 * len(all_models)),squeeze=False)\n",
    "ax = ax.flatten()\n",
    "\n",
    "show_std = False\n",
    "\n",
    "plot_idx = 0\n",
    "\n",
    "model_label_map = {\n",
    "    \n",
    "}\n",
    "\n",
    "for model in all_models:\n",
    "    updated_name = updated_model_name_map[model]\n",
    "    for condition in conditions:\n",
    "        all_losses = total_losses[condition][model]\n",
    "        losses_list = [value for key,value in all_losses.items() if len(value) == num_evals]\n",
    "        losses = np.array(losses_list)\n",
    "        loss_improvements = []\n",
    "        for part_loss in losses:\n",
    "            part_imp = []\n",
    "            best_loss = part_loss[0]\n",
    "            for loss in part_loss[1:]:\n",
    "                part_imp.append(max(0, best_loss - loss))\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "            loss_improvements.append(part_imp)\n",
    "        loss_improvements = np.array(loss_improvements)\n",
    "        try:\n",
    "            avg = loss_improvements.mean(axis=0)\n",
    "            std = loss_improvements.std(axis=0) / np.sqrt(loss_improvements.shape[0])\n",
    "        except:\n",
    "            avg = loss_improvements.mean(axis=0)\n",
    "            std = loss_improvements.std(axis=0) / np.sqrt(loss_improvements.shape[0])\n",
    "        try:\n",
    "            ax[plot_idx].plot(range(len(avg)), avg, label=\"model\", color='blue')\n",
    "            if show_std:\n",
    "                ax[plot_idx].fill_between(\n",
    "                    list(range(len(avg))),\n",
    "                    avg + 1.96 * std,\n",
    "                    avg - 1.96 * std,\n",
    "                    alpha=0.9\n",
    "                )\n",
    "        except:\n",
    "            pass\n",
    "        ax[plot_idx].set_title(f\"{updated_name} - {model} - {condition}\")\n",
    "    \n",
    "        plot_idx += 1\n",
    "    \n",
    "\n",
    "\n",
    "plt.subplots_adjust(\n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817c55cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Observing evolution of best loss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(all_models),ncols=len(conditions),figsize=(5 * len(conditions), 3 * len(all_models)),squeeze=False)\n",
    "ax = ax.flatten()\n",
    "\n",
    "show_std = True\n",
    "\n",
    "plot_idx = 0\n",
    "\n",
    "model_label_map = {\n",
    "    \n",
    "}\n",
    "\n",
    "for model in all_models:\n",
    "    updated_name = updated_model_name_map[model]\n",
    "    for condition in conditions:\n",
    "        all_losses = total_losses[condition][model]\n",
    "        losses_list = [value for key,value in all_losses.items() if len(value) == num_evals]\n",
    "        losses = np.array(losses_list)\n",
    "        loss_improvements = []\n",
    "        for part_loss in losses:\n",
    "            part_imp = []\n",
    "            best_loss = part_loss[0]\n",
    "            for loss in part_loss[1:]:\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                part_imp.append(best_loss)\n",
    "            loss_improvements.append(part_imp)\n",
    "        loss_improvements = np.array(loss_improvements)\n",
    "        try:\n",
    "            avg = loss_improvements.mean(axis=0)[14500:]\n",
    "            std = loss_improvements.std(axis=0)[14500:] / np.sqrt(loss_improvements.shape[0])\n",
    "            range_diff = \"{0:0.3f}\".format(avg[0] - avg[-1])\n",
    "        except:\n",
    "            avg = loss_improvements.mean(axis=0)\n",
    "            std = loss_improvements.std(axis=0) / np.sqrt(loss_improvements.shape[0])\n",
    "            range_diff = \"0\"\n",
    "        \n",
    "        try:\n",
    "            ax[plot_idx].plot(range(len(avg)), avg, label=\"model\", color='blue')\n",
    "            if show_std:\n",
    "                ax[plot_idx].fill_between(\n",
    "                    list(range(len(avg))),\n",
    "                    avg + 1.96 * std,\n",
    "                    avg - 1.96 * std,\n",
    "                    alpha=0.5\n",
    "                )\n",
    "        except:\n",
    "            pass\n",
    "        ax[plot_idx].set_title(f\"{updated_name} - {model} - {condition}: {range_diff}\")\n",
    "    \n",
    "        plot_idx += 1\n",
    "    \n",
    "\n",
    "\n",
    "plt.subplots_adjust(\n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d18b31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Histogram of the trial ID where the best loss was achieved\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(all_models),ncols=len(conditions),figsize=(5 * len(conditions), 3 * len(all_models)),squeeze=False)\n",
    "ax = ax.flatten()\n",
    "\n",
    "show_std = False\n",
    "\n",
    "plot_idx = 0\n",
    "\n",
    "model_label_map = {\n",
    "    \n",
    "}\n",
    "\n",
    "for model in all_models:\n",
    "    updated_name = updated_model_name_map[model]\n",
    "    for condition in conditions:\n",
    "        best_trials = best_eval_id[condition][model]\n",
    "        ax[plot_idx].hist(best_trials)\n",
    "        avg_trial_num = float(\"{0:0.3f}\".format(np.mean(best_trials)))\n",
    "        ax[plot_idx].set_title(f\"{updated_name} - {model} - {condition}: {avg_trial_num}\")\n",
    "    \n",
    "        plot_idx += 1\n",
    "    \n",
    "\n",
    "\n",
    "plt.subplots_adjust(\n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0683f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l = np.sort(losses.mean(axis=0))[::-1]\n",
    "y = losses.mean(axis=0)\n",
    "best_loss = y[0]\n",
    "l = [best_loss]\n",
    "\n",
    "for loss in y[1:]:\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "    l.append(best_loss)\n",
    "\n",
    "plt.plot(range(len(l)), l)\n",
    "plt.xlabel(\"Evaluation #\")\n",
    "plt.ylabel(\"Best loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd29a6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comparing BIC scores of models\n",
    "\n",
    "metric = \"mean\" # average, mode\n",
    "\n",
    "if metric == \"mean\":\n",
    "    s_BIC = list(scarce_part_BIC.mean().sort_values().index)\n",
    "    s_BIC = [updated_model_name_map[m] for m in s_BIC]\n",
    "    c_BIC = list(control_part_BIC.mean().sort_values().index)\n",
    "    c_BIC = [updated_model_name_map[m] for m in c_BIC]\n",
    "    #o_BIC = list(original_BIC.mean().sort_values().index)\n",
    "    s_AIC = list(scarce_part_AIC.mean().sort_values().index)\n",
    "    s_AIC = [updated_model_name_map[m] for m in s_AIC]\n",
    "    c_AIC = list(control_part_AIC.mean().sort_values().index)\n",
    "    c_AIC = [updated_model_name_map[m] for m in c_AIC]\n",
    "    #o_AIC = list(original_AIC.mean().sort_values().index)\n",
    "    \n",
    "    print(\"Best models by average\")\n",
    "    print(\"Scarce BIC - Best Models\")\n",
    "    print(s_BIC)\n",
    "    print(\"Control BIC - Best Models\")\n",
    "    print(c_BIC)\n",
    "#     print(\"Original BIC - Best Models\")\n",
    "#     print(o_BIC)\n",
    "    print(\"Scarce AIC - Best Models\")\n",
    "    print(s_AIC)\n",
    "    print(\"Control AIC - Best Models\")\n",
    "    print(c_AIC)\n",
    "#     print(\"Original AIC - Best Models\")\n",
    "#     print(o_BIC)\n",
    "\n",
    "elif metric == \"mode\":\n",
    "    s_BIC = list(scarce_part_BIC.idxmin(axis=1).value_counts().index)\n",
    "    s_BIC = [updated_model_name_map[m] for m in s_BIC]\n",
    "    c_BIC = list(control_part_BIC.idxmin(axis=1).value_counts().index)\n",
    "    c_BIC = [updated_model_name_map[m] for m in c_BIC]\n",
    "    #o_BIC = list(original_BIC.idxmin(axis=1).value_counts().index)\n",
    "    s_AIC = list(scarce_part_AIC.idxmin(axis=1).value_counts().index)\n",
    "    s_AIC = [updated_model_name_map[m] for m in s_AIC]\n",
    "    c_AIC = list(control_part_AIC.idxmin(axis=1).value_counts().index)\n",
    "    c_AIC = [updated_model_name_map[m] for m in c_AIC]\n",
    "\n",
    "    print(\"Best models by mode\")\n",
    "    print(\"Scarce BIC - Best Models\")\n",
    "    print(s_BIC)\n",
    "    print(\"Control BIC - Best Models\")\n",
    "    print(c_BIC)\n",
    "#     print(\"Original BIC - Best Models\")\n",
    "#     print(o_BIC)\n",
    "    print(\"Scarce AIC - Best Models\")\n",
    "    print(s_AIC)\n",
    "    print(\"Control AIC - Best Models\")\n",
    "    print(c_AIC)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec6a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_series(series):\n",
    "    models = series.index\n",
    "    vals = list(series)\n",
    "    for model, val in zip(models, vals):\n",
    "        print(\"\\t{0}\\t{1:0.5f}\\t{2}\".format(updated_model_name_map[model],val,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6e306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing single BIC score for each model\n",
    "\n",
    "model_BIC[\"BIC\"] = 2 * model_BIC[\"total_loss\"] + model_BIC[\"total_params\"] * np.log(model_BIC[\"total_m_actions\"])\n",
    "model_BIC[\"AIC\"] = 2 * model_BIC[\"total_loss\"] + model_BIC[\"total_params\"] * 2\n",
    "\n",
    "print(\"BIC:\")\n",
    "print_model_series(model_BIC[\"BIC\"].sort_values(ascending=True)[0:15])\n",
    "\n",
    "print(\"\\nAIC:\")\n",
    "print_model_series(model_BIC[\"AIC\"].sort_values(ascending=True)[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e6b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing single BIC score for each model for each condition\n",
    "\n",
    "scarce_BIC[\"BIC\"] = 2 * scarce_BIC[\"total_loss\"] + scarce_BIC[\"total_params\"] * np.log(scarce_BIC[\"total_m_actions\"])\n",
    "scarce_BIC[\"AIC\"] = 2 * scarce_BIC[\"total_loss\"] + scarce_BIC[\"total_params\"] * 2\n",
    "\n",
    "print(\"Scarce BIC:\")\n",
    "print_model_series(scarce_BIC[\"BIC\"].sort_values()[0:10])\n",
    "\n",
    "print(\"\\nScarce AIC:\")\n",
    "print_model_series(scarce_BIC[\"AIC\"].sort_values()[0:10])\n",
    "\n",
    "control_BIC[\"BIC\"] = 2 * control_BIC[\"total_loss\"] + control_BIC[\"total_params\"] * np.log(control_BIC[\"total_m_actions\"])\n",
    "control_BIC[\"AIC\"] = 2 * control_BIC[\"total_loss\"] + control_BIC[\"total_params\"] * 2\n",
    "\n",
    "print(\"Control BIC:\")\n",
    "print_model_series(control_BIC[\"BIC\"].sort_values()[0:10])\n",
    "\n",
    "print(\"\\nControl AIC:\")\n",
    "print_model_series(control_BIC[\"AIC\"].sort_values()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b571fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrapping the BIC/AIC confidence intervals for all participants taken together\n",
    "import random\n",
    "\n",
    "models_to_bootstrap = [\n",
    "    '2.0.1.1',\n",
    "    '2.0.1.2',\n",
    "    '2.0.2.1',\n",
    "    '2.0.3.0.0',\n",
    "    '2.0.3.0.1',\n",
    "    '2.0.3.1.0',\n",
    "    '2.0.3.2.0',\n",
    "    '2.0.3.3.0',\n",
    "    '3.0.1.1',\n",
    "    '3.0.1.2',\n",
    "    '3.0.2.1',\n",
    "    '3.0.3.0.0',\n",
    "    '3.0.3.1.0',\n",
    "    '3.0.3.2.0',\n",
    "    '3.0.3.3.0',\n",
    "]\n",
    "    \n",
    "\n",
    "num_samples = 1000\n",
    "\n",
    "sample_size = len(participant_BIC)\n",
    "bootstrapped_BICs = {}\n",
    "bootstrapped_AICs = {}\n",
    "\n",
    "bdifference_matrix = np.zeros((len(models_to_bootstrap), len(models_to_bootstrap), num_samples))\n",
    "adifference_matrix = np.zeros((len(models_to_bootstrap), len(models_to_bootstrap), num_samples))\n",
    "\n",
    "for sample_num in range(num_samples):\n",
    "    if (sample_num+1) % 100 == 0: print(sample_num+1)\n",
    "    sample_ids = np.random.choice(range(0,sample_size), size=(sample_size,), replace=True)\n",
    "    for idx, model in enumerate(models_to_bootstrap):\n",
    "        relevant_column = participant_BIC[model]\n",
    "        if model not in bootstrapped_BICs:\n",
    "            bootstrapped_BICs[model] = []\n",
    "        if model not in bootstrapped_AICs:\n",
    "            bootstrapped_AICs[model] = []\n",
    "\n",
    "\n",
    "        samples = relevant_column[sample_ids]\n",
    "        for sample in samples:\n",
    "            # Random sample with replacement\n",
    "            \n",
    "            %timeit sample = relevant_column.sample().values[0]\n",
    "            sample_loss += sample[\"loss\"]\n",
    "            sample_params += sample[\"params\"]\n",
    "            sample_actions += sample[\"m_actions\"]\n",
    "        sample_BIC = 2 * sample_loss + sample_params * np.log(sample_actions)\n",
    "        sample_AIC = 2 * sample_loss + sample_params * 2\n",
    "        bootstrapped_BICs[model].append(sample_BIC)\n",
    "        bootstrapped_AICs[model].append(sample_AIC)\n",
    "        \n",
    "        # Subtract and add as needed to compute the difference in the scores between\n",
    "        # all the models for this sample\n",
    "        bdifference_matrix[idx,:,sample_num] += sample_BIC\n",
    "        bdifference_matrix[:,idx,sample_num] -= sample_BIC\n",
    "        adifference_matrix[idx,:,sample_num] += sample_AIC\n",
    "        adifference_matrix[:,idx,sample_num] -= sample_AIC\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4825ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 95% CIs for each model\n",
    "\n",
    "model_BIC_CIs = {}\n",
    "model_AIC_CIs = {}\n",
    "\n",
    "for model in models_to_bootstrap:\n",
    "    bmean = np.mean(bootstrapped_BICs[model])\n",
    "    bstd = np.std(bootstrapped_BICs[model])\n",
    "    amean = np.mean(bootstrapped_AICs[model])\n",
    "    astd = np.std(bootstrapped_AICs[model])\n",
    "    \n",
    "    model_BIC_CIs[model] = (bmean - 2 * bstd, bmean + 2 * bstd)\n",
    "    model_AIC_CIs[model] = (amean - 2 * astd, amean + 2 * astd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms of BICs\n",
    "models_to_plot = [\n",
    "    \"2.0.3.0.0\",\n",
    "    \"2.0.2.1\",\n",
    "    #\"2.0.3.2.0\",\n",
    "    #\"3.1.1.1\"\n",
    "]\n",
    "\n",
    "min_BIC = float(\"inf\")\n",
    "max_BIC = float(\"-inf\")\n",
    "\n",
    "min_AIC = float(\"inf\")\n",
    "max_AIC = float(\"-inf\")\n",
    "\n",
    "for model in models_to_plot:\n",
    "    model_min_BIC = min(bootstrapped_BICs[model])\n",
    "    model_min_AIC = min(bootstrapped_AICs[model])\n",
    "    model_max_BIC = max(bootstrapped_BICs[model])\n",
    "    model_max_AIC = max(bootstrapped_AICs[model])\n",
    "    \n",
    "    if model_min_BIC < min_BIC:\n",
    "        min_BIC = model_min_BIC\n",
    "    \n",
    "    if model_min_AIC < min_AIC:\n",
    "        min_AIC = model_min_AIC\n",
    "        \n",
    "    if model_max_BIC > max_BIC:\n",
    "        max_BIC = model_max_BIC\n",
    "        \n",
    "    if model_max_AIC > max_AIC:\n",
    "        max_AIC = model_max_AIC\n",
    "        \n",
    "BIC_bins = np.linspace(min_BIC - 1000, max_BIC + 1000, 100)\n",
    "AIC_bins = np.linspace(min_AIC - 1000, max_AIC + 1000, 100)\n",
    "\n",
    "colors = ['orange', 'blue', 'green', 'yellow']\n",
    "\n",
    "for idx, model in enumerate(models_to_plot):\n",
    "    plt.figure(1)\n",
    "    plt.hist(bootstrapped_BICs[model], BIC_bins, alpha=0.5, label=model, color=colors[idx])\n",
    "    plt.axvline(model_BIC_CIs[model][1], linestyle='--', color=colors[idx])\n",
    "    plt.axvline(model_BIC_CIs[model][0], linestyle='-.', color=colors[idx])\n",
    "    plt.figure(2)\n",
    "    plt.hist(bootstrapped_AICs[model], AIC_bins, alpha=0.5, label=model, color=colors[idx])\n",
    "    plt.axvline(model_AIC_CIs[model][1], linestyle='--', color=colors[idx])\n",
    "    plt.axvline(model_AIC_CIs[model][0], linestyle='-.', color=colors[idx])\n",
    "plt.figure(1)\n",
    "plt.legend()\n",
    "plt.figure(2)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a240db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bootstrapping the BIC/AIC confidence intervals for conditions separately\n",
    "import random\n",
    "\n",
    "models_to_bootstrap = [\n",
    "    '2.0.1.1',\n",
    "    '2.0.1.2',\n",
    "    '2.0.2.1',\n",
    "    '2.0.3.0.0',\n",
    "    '2.0.3.0.1',\n",
    "    '2.0.3.1.0',\n",
    "    '2.0.3.2.0',\n",
    "    '2.0.3.3.0',\n",
    "    '3.0.1.1',\n",
    "    '3.0.1.2',\n",
    "    '3.0.2.1',\n",
    "    '3.0.3.0.0',\n",
    "    '3.0.3.1.0',\n",
    "    '3.0.3.2.0',\n",
    "    '3.0.3.3.0',\n",
    "]\n",
    "    \n",
    "\n",
    "num_samples = 1000\n",
    "\n",
    "\n",
    "bootstrapped_BICs_conds = {}\n",
    "bootstrapped_AICs_conds = {}\n",
    "\n",
    "for condition in conditions:\n",
    "    condition_BICs = participant_BIC.loc[participant_BIC.index.isin(condition_data_sets[condition].workerId)]\n",
    "    bootstrapped_BICs_conds[condition] = {}\n",
    "    bootstrapped_AICs_conds[condition] = {}\n",
    "    sample_size = len(condition_BICs)\n",
    "    for model in models_to_bootstrap:\n",
    "        relevant_column = condition_BICs[model]\n",
    "        print(condition, model)\n",
    "        bootstrapped_BICs_conds[condition][model] = []\n",
    "        bootstrapped_AICs_conds[condition][model] = []\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            if (i+1) % 100 == 0: print(i+1)\n",
    "            sample_loss = 0\n",
    "            sample_params = 0\n",
    "            sample_actions = 0\n",
    "\n",
    "            samples = relevant_column.sample(sample_size, replace=True)\n",
    "            for sample in samples:\n",
    "                # Random sample with replacement\n",
    "                sample = relevant_column.sample().values[0]\n",
    "                sample_loss += sample[\"loss\"]\n",
    "                sample_params += sample[\"params\"]\n",
    "                sample_actions += sample[\"m_actions\"]\n",
    "            sample_BIC = 2 * sample_loss + sample_params * np.log(sample_actions)\n",
    "            sample_AIC = 2 * sample_loss + sample_params * 2\n",
    "            bootstrapped_BICs_conds[condition][model].append(sample_BIC)\n",
    "            bootstrapped_AICs_conds[condition][model].append(sample_AIC)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 95% CIs for each model for conditions separately\n",
    "\n",
    "model_BIC_CIs_conds = {}\n",
    "model_AIC_CIs_conds = {}\n",
    "\n",
    "for condition in conditions:\n",
    "    model_BIC_CIs_conds[condition] = {}\n",
    "    model_AIC_CIs_conds[condition] = {}\n",
    "    for model in models_to_bootstrap:\n",
    "        model_BIC_CIs_conds[condition][model] = {}\n",
    "        model_AIC_CIs_conds[condition][model] = {}\n",
    "        bmean = np.mean(bootstrapped_BICs_conds[condition][model])\n",
    "        bstd = np.std(bootstrapped_BICs_conds[condition][model])\n",
    "        amean = np.mean(bootstrapped_AICs_conds[condition][model])\n",
    "        astd = np.std(bootstrapped_AICs_conds[condition][model])\n",
    "\n",
    "        model_BIC_CIs_conds[condition][model] = (bmean - 2 * bstd, bmean + 2 * bstd)\n",
    "        model_AIC_CIs_conds[condition][model] = (amean - 2 * astd, amean + 2 * astd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c563d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms of BICs\n",
    "models_to_plot = [\n",
    "    \"2.0.3.0.0\",\n",
    "    \"3.0.1.2\",\n",
    "    \"2.0.1.1\",\n",
    "    #\"3.1.1.1\"\n",
    "]\n",
    "\n",
    "for cidx, condition in enumerate(conditions):\n",
    "    min_BIC = float(\"inf\")\n",
    "    max_BIC = float(\"-inf\")\n",
    "\n",
    "    min_AIC = float(\"inf\")\n",
    "    max_AIC = float(\"-inf\")\n",
    "\n",
    "    for model in models_to_plot:\n",
    "        model_min_BIC = min(bootstrapped_BICs_conds[condition][model])\n",
    "        model_min_AIC = min(bootstrapped_AICs_conds[condition][model])\n",
    "        model_max_BIC = max(bootstrapped_BICs_conds[condition][model])\n",
    "        model_max_AIC = max(bootstrapped_AICs_conds[condition][model])\n",
    "\n",
    "        if model_min_BIC < min_BIC:\n",
    "            min_BIC = model_min_BIC\n",
    "\n",
    "        if model_min_AIC < min_AIC:\n",
    "            min_AIC = model_min_AIC\n",
    "\n",
    "        if model_max_BIC > max_BIC:\n",
    "            max_BIC = model_max_BIC\n",
    "\n",
    "        if model_max_AIC > max_AIC:\n",
    "            max_AIC = model_max_AIC\n",
    "\n",
    "    BIC_bins = np.linspace(min_BIC - 1000, max_BIC + 1000, 100)\n",
    "    AIC_bins = np.linspace(min_AIC - 1000, max_AIC + 1000, 100)\n",
    "\n",
    "    colors = ['orange', 'blue', 'green', 'yellow']\n",
    "\n",
    "    for idx, model in enumerate(models_to_plot):\n",
    "        plt.figure((cidx)*2 + 1)\n",
    "        plt.hist(bootstrapped_BICs_conds[condition][model], BIC_bins, alpha=0.5, label=model, color=colors[idx])\n",
    "        plt.axvline(model_BIC_CIs_conds[condition][model][1], linestyle='--', color=colors[idx])\n",
    "        plt.axvline(model_BIC_CIs_conds[condition][model][0], linestyle='-.', color=colors[idx])\n",
    "        plt.title(f\"{condition}_BIC\")\n",
    "        plt.legend()\n",
    "        plt.figure((cidx)*2 + 2)\n",
    "        plt.hist(bootstrapped_AICs_conds[condition][model], AIC_bins, alpha=0.5, label=model, color=colors[idx])\n",
    "        plt.axvline(model_AIC_CIs_conds[condition][model][1], linestyle='--', color=colors[idx])\n",
    "        plt.axvline(model_AIC_CIs_conds[condition][model][0], linestyle='-.', color=colors[idx])\n",
    "        plt.title(f\"{condition}_AIC\")\n",
    "        plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check overlap between confidence intervals\n",
    "overlap_matrix = np.zeros((len(models_to_bootstrap), len(models_to_bootstrap)))\n",
    "\n",
    "for i in range(len(overlap_matrix)):\n",
    "    for j in range(i+1, len(overlap_matrix)):\n",
    "        CI_i = model_BIC_CIs[models_to_bootstrap[i]]\n",
    "        CI_j = model_BIC_CIs[models_to_bootstrap[j]]\n",
    "        #print(CI_i, CI_j)\n",
    "        if CI_i[0] <= CI_j[1] and CI_j[0] <= CI_i[1]:\n",
    "            # Overlap in confidence intervals\n",
    "            overlap_matrix[i][j] = min([CI_i[1]-CI_j[0], CI_j[1]-CI_i[0]])\n",
    "            \n",
    "overlap_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72973ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Inspecting the signal weight parameters for the models that have 3.1.x or 3.2.x\n",
    "\n",
    "signal_wts = {}\n",
    "\n",
    "for condition in conditions:\n",
    "    signal_wts[condition] = {}\n",
    "    for model in all_models:\n",
    "        if not (\".3.1.\" in model or \".3.2.\" in model):\n",
    "            continue\n",
    "        print(condition, model)\n",
    "        signal_wts[condition][model] = []\n",
    "        docs = collection.find({ \"condition\": condition, \"model\": model })\n",
    "        for doc in docs:\n",
    "            best_params = json.loads(doc[\"best_params\"])\n",
    "            signal_wts[condition][model].append(best_params[\"feedback_weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2cc84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of feedback_weight parameter\n",
    "\n",
    "condition = \"scarce\"\n",
    "model = \"2.0.3.2.0\"\n",
    "\n",
    "data = np.array(signal_wts[condition][model])\n",
    "mean = data.mean()\n",
    "sd = data.std()\n",
    "\n",
    "plt.hist(data)\n",
    "plt.axvline(mean, linestyle='--', color='k')\n",
    "plt.xlabel('Feedback weight parameter')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(\"{0} {1}, Mean: {2:0.2f}, SD: {3:0.2f}\".format(condition, updated_model_name_map[model], mean,sd))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba029d1",
   "metadata": {},
   "source": [
    "# 1.8 Creating Dataframes of Model Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a0568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframes of model simulated data for statistical analysis:\n",
    "condition = \"scarce\"\n",
    "models = {\n",
    "    \"control\": \"2.0.3.0.0\",\n",
    "    \"scarce\": \"2.0.3.0.0\"\n",
    "}\n",
    "control_model = models[\"control\"]\n",
    "scarce_model = models[\"scarce\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20270692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining paths\n",
    "\n",
    "# CM inferred strategies\n",
    "results_path = '../results'\n",
    "inferred_path = results_path + \"/cm/inferred_strategies\"\n",
    "model_sim_path = results_path + \"/model_sim_data\"\n",
    "\n",
    "scarce_inferred = inferred_path + f\"/scarcity_scarce_{scarce_model}/strategies.pkl\"\n",
    "control_inferred = inferred_path + f\"/scarcity_control_{control_model}/strategies.pkl\"\n",
    "\n",
    "# Expected scores of all strategies\n",
    "strategy_scores_scarce_path = results_path + \"/cm/strategy_scores/scarcity_scarce_clickcost_0.25_strategy_scores.pkl\"\n",
    "strategy_scores_control_path = results_path + \"/cm/strategy_scores/scarcity_control_clickcost_1.0_strategy_scores.pkl\"\n",
    "\n",
    "model_data_df_rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd7658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading inferred strategies\n",
    "\n",
    "with open(scarce_inferred, 'rb') as f:\n",
    "    scarce_strategies = pickle.load(f)\n",
    "    \n",
    "with open(control_inferred, 'rb') as f:\n",
    "    control_strategies = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b06bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading strategy scores\n",
    "\n",
    "with open(strategy_scores_scarce_path, 'rb') as file:\n",
    "    strategy_scores_scarce = pickle.load(file)\n",
    "    \n",
    "with open(strategy_scores_control_path, 'rb') as file:\n",
    "    strategy_scores_control = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26870df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the strategy score dataframes\n",
    "\n",
    "strategy_scores_scarce_list = [[k,v] for k,v in strategy_scores_scarce.items()]\n",
    "strategy_scores_control_list = [[k,v] for k,v in strategy_scores_control.items()]\n",
    "\n",
    "scarce_cluster_df = pd.DataFrame(strategy_scores_scarce_list, columns=[\"strategy\", \"score\"])\n",
    "control_cluster_df = pd.DataFrame(strategy_scores_control_list, columns=[\"strategy\", \"score\"])\n",
    "\n",
    "# Scale to 0-1:\n",
    "scarce_cluster_df['scoreScaled'] = (scarce_cluster_df['score'] - scarce_cluster_df['score'].min()) / (scarce_cluster_df['score'].max() - scarce_cluster_df['score'].min())\n",
    "control_cluster_df['scoreScaled'] = (control_cluster_df['score'] - control_cluster_df['score'].min()) / (control_cluster_df['score'].max() - control_cluster_df['score'].min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the strategy scores\n",
    "\n",
    "scarce_clusters = scipy.cluster.vq.kmeans(scarce_cluster_df['scoreScaled'], k_or_guess=[0.6,0.5,0.4])\n",
    "scarce_cluster_centers = scarce_clusters[0]\n",
    "\n",
    "control_clusters = scipy.cluster.vq.kmeans(control_cluster_df['scoreScaled'], k_or_guess=[0.6,0.5,0.4])\n",
    "control_cluster_centers = control_clusters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5581f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cluster labels: 0 - adaptive, 1 - moderately adaptive, 2 - maladaptive\n",
    "\n",
    "scarce_clusters_repeated = np.repeat(np.expand_dims(scarce_cluster_centers, axis=0), len(scarce_cluster_df), axis=0)\n",
    "scarce_cluster_distances = (scarce_clusters_repeated - np.repeat(np.expand_dims(np.array(scarce_cluster_df['scoreScaled']), axis=1),repeats=len(scarce_cluster_centers), axis=1))**2\n",
    "scarce_cluster_labels = np.argmin(scarce_cluster_distances, axis=1)\n",
    "\n",
    "control_clusters_repeated = np.repeat(np.expand_dims(control_cluster_centers, axis=0), len(control_cluster_df), axis=0)\n",
    "control_cluster_distances = (control_clusters_repeated - np.repeat(np.expand_dims(np.array(control_cluster_df['scoreScaled']), axis=1),repeats=len(control_cluster_centers), axis=1))**2\n",
    "control_cluster_labels = np.argmin(control_cluster_distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of cluster labels:\n",
    "\n",
    "scarce_cluster_dict = { strat: cluster for (strat, cluster) in zip(scarce_cluster_df.strategy, scarce_cluster_labels)}\n",
    "\n",
    "control_cluster_dict = { strat: cluster for (strat, cluster) in zip(control_cluster_df.strategy, control_cluster_labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a6ff53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Creating the trial dataframe\n",
    "\n",
    "inf_strat = {\n",
    "    \"control\": control_strategies,\n",
    "    \"scarce\": scarce_strategies\n",
    "}\n",
    "\n",
    "strat_clust = {\n",
    "    \"control\": control_cluster_dict,\n",
    "    \"scarce\": scarce_cluster_dict\n",
    "}\n",
    "\n",
    "strat_scores = {\n",
    "    \"control\": strategy_scores_control,\n",
    "    \"scarce\": strategy_scores_scarce\n",
    "}\n",
    "\n",
    "scarcity_level = {\n",
    "    \"control\": 1.0,\n",
    "    \"scarce\": 0.25\n",
    "}\n",
    "\n",
    "trials_dicts = []\n",
    "\n",
    "for condition in models.keys():\n",
    "    docs = collection.find( { \"condition\": condition, \"model\": models[condition] } )\n",
    "    for doc in docs:\n",
    "        pid = doc[\"pid\"]\n",
    "        rewards = json.loads(doc['r'])\n",
    "        expected = json.loads(doc['mer'])\n",
    "        clicks = json.loads(doc['num_clicks'])\n",
    "\n",
    "        part_data = all_trials_df.loc[all_trials_df.workerId == pid]\n",
    "        num_simulations = 1 # range(len(rewards)) # Only doing for one simulation initially\n",
    "        print(pid, len(part_data), num_simulations, models[condition])\n",
    "        for sim_num in range(num_simulations): \n",
    "            new_pid = pid + str(sim_num)\n",
    "            sim_r = rewards[sim_num]\n",
    "            sim_mer = expected[sim_num]\n",
    "            sim_clicks = clicks[sim_num]\n",
    "\n",
    "            adjusted_sim_mer = [mer - c for (mer,c) in zip(sim_mer, sim_clicks)]\n",
    "            for trial in range(len(sim_r)):\n",
    "                strategy = inf_strat[condition][pid][trial]\n",
    "                cluster = strat_clust[condition][strategy-1]\n",
    "                strat_score = strat_scores[condition][strategy-1] / scarcity_level[condition]\n",
    "                #print(trial, strategy, cluster,strat_score)\n",
    "                trials_dicts.append({\n",
    "                    \"workerId\": new_pid,\n",
    "                    \"trialNumbers\": trial + 1,\n",
    "                    \"scarce\": int(condition == \"scarce\"),\n",
    "                    \"trialScores\": sim_r[trial],\n",
    "                    \"expectedScores\": adjusted_sim_mer[trial],\n",
    "                    \"numClicks\": sim_clicks[trial],\n",
    "                    \"numRewardedTrials\": list(part_data.loc[part_data.trialNumbers == (trial + 1)].numRewardedTrials)[0],\n",
    "                    \"numUnrewardedTrials\": list(part_data.loc[part_data.trialNumbers == (trial + 1)].numUnrewardedTrials)[0],\n",
    "                    \"strategy\": strategy,\n",
    "                    \"avgClickLevel\": list(part_data.loc[part_data.trialNumbers == (trial + 1)].avgClickLevel)[0],\n",
    "                    \"strategyScores\": strat_score,\n",
    "                    \"rewardsWithheld\": list(part_data.loc[part_data.trialNumbers == (trial + 1)].rewardsWithheld)[0],\n",
    "                    \"cluster\": cluster\n",
    "                })\n",
    "\n",
    "model_df = pd.DataFrame(trials_dicts)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98229e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = '../results/model_sim_data'\n",
    "file_name = f\"/c{control_model}_s{scarce_model}_{num_simulations}.csv\"\n",
    "\n",
    "model_df.to_csv(save_folder + file_name,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf03092d",
   "metadata": {},
   "source": [
    "# 1.9 Comparing strategy evolution of models vs. participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277a70d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframes of model simulated data for statistical analysis:\n",
    "\n",
    "models = {\n",
    "    \"control\": \"2.0.3.3.0\",\n",
    "    \"scarce\": \"2.0.3.0.0\"\n",
    "}\n",
    "control_model = models[\"control\"]\n",
    "scarce_model = models[\"scarce\"]\n",
    "\n",
    "file_name = f\"../results/model_sim_data/c{control_model}_s{scarce_model}_1.csv\"\n",
    "\n",
    "model_df = pd.read_csv(file_name)\n",
    "\n",
    "model_dfs = {\n",
    "    \"control\": model_df.loc[model_df.scarce == 0],\n",
    "    \"scarce\": model_df.loc[model_df.scarce == 1]\n",
    "}\n",
    "\n",
    "part_dfs = {\n",
    "    \"control\": all_trials_df.loc[all_trials_df.scarce == 0],\n",
    "    \"scarce\": all_trials_df.loc[all_trials_df.scarce == 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefaedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = part_dfs[\"control\"]\n",
    "\n",
    "test.loc[test.strategy == 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ebc2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 5\n",
    "num_top_strats = 6\n",
    "num_end_strats = 3\n",
    "\n",
    "begin_trials_until = {\n",
    "    \"control\": 10,\n",
    "    \"scarce\": 10,\n",
    "} \n",
    "end_trials_from = {\n",
    "    \"control\": 20,\n",
    "    \"scarce\": 110,\n",
    "} \n",
    "\n",
    "colors = ['red', 'blue','green','orange','black','purple','grey','brown','cyan','gold','crimson']\n",
    "\n",
    "bar_width = 0.05\n",
    "fig, ax = plt.subplots(2,2, figsize=(20,20))\n",
    "\n",
    "for cond_id, condition in enumerate(models.keys()):\n",
    "    part_df = part_dfs[condition]\n",
    "    model_df = model_dfs[condition]\n",
    "    partition_width = condition_trial_nums[condition] / num_partitions\n",
    "#     begin_trials = df.loc[df.trialNumbers <= begin_trials_until[condition]]\n",
    "#     strategy_counts = begin_trials[\"strategy\"].value_counts()[0:num_begin_strats]\n",
    "#     top_strategies = list(strategy_counts.index)\n",
    "    \n",
    "#     relevant_strats = top_strategies\n",
    "    \n",
    "#     end_trials = df.loc[df.trialNumbers >= end_trials_from[condition]]\n",
    "#     strategy_counts = end_trials[\"strategy\"].value_counts()[0:num_end_strats]\n",
    "    part_strategy_counts = part_df[\"strategy\"].value_counts()[0:num_top_strats]\n",
    "    part_top_strategies = list(part_strategy_counts.index)\n",
    "    print(part_strategy_counts)\n",
    "    \n",
    "    model_strategy_counts = model_df[\"strategy\"].value_counts()[0:num_top_strats]\n",
    "    model_top_strategies = list(model_strategy_counts.index)\n",
    "    #print(model_strategy_counts)\n",
    "    \n",
    "    top_strategies = np.union1d(part_top_strategies, model_top_strategies)\n",
    "    \n",
    "#     print(part_top_strategies)\n",
    "#     print(model_top_strategies)\n",
    "    \n",
    "    ind = np.arange(num_partitions)\n",
    "    \n",
    "    \n",
    "    for df_id, df in enumerate([part_dfs[condition], model_dfs[condition]]):\n",
    "        current_ax = ax[cond_id, df_id]\n",
    "        for i, idx in enumerate(ind):\n",
    "            relevant_data = df.loc[(df.trialNumbers > i*partition_width) & (df.trialNumbers <= (i+1) * partition_width)]\n",
    "            for j, strat in enumerate(top_strategies):\n",
    "                #print(len(relevant_data.loc[relevant_data.strategy.isin(top_strategies)])/len(relevant_data))\n",
    "                strat_prop = len(relevant_data.loc[relevant_data.strategy == strat]) / len(relevant_data)\n",
    "                current_ax.bar(idx + j*bar_width, strat_prop, bar_width, color=colors[j], label=strat if i==0 else None)\n",
    "        current_ax.legend()\n",
    "    \n",
    "    #print(top_strategies)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c2ef0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for strat in [21, 30, 31, 27, 22, 26, 24, 14, 76, 18, 2, 39, 3]:\n",
    "    print(strat, control_cluster_dict[strat-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf180e64",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83e26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking control models that are missing\n",
    "\n",
    "control_done_parts = control_part_BIC[~control_part_BIC['2.0.3.3.0'].isna()]\n",
    "control_remaining_parts = set()\n",
    "control_remaining_models = []\n",
    "missing_models = 0\n",
    "model_missing_parts_dict = {}\n",
    "for col in control_done_parts.columns:\n",
    "    model_missing_parts = list(control_done_parts.loc[control_done_parts[col].isna()].index)\n",
    "    print(\"\\n\" + col)\n",
    "    print(\"\\n\".join(model_missing_parts))\n",
    "    if len(model_missing_parts) > 0:\n",
    "        control_remaining_models.append(col)\n",
    "        control_remaining_parts = control_remaining_parts.union(set(model_missing_parts))\n",
    "        missing_models += len(model_missing_parts)\n",
    "        model_missing_parts_dict[col] = model_missing_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9620afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving PIDS of missing models to files\n",
    "for model, parts in model_missing_parts_dict.items():\n",
    "    filename = f\"control_remaining/{model}.txt\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"\\n\".join(parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3bcace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking scarce models that are missing\n",
    "\n",
    "scarce_done_parts = scarce_part_BIC[~scarce_part_BIC['2.0.3.1.0'].isna()]\n",
    "scarce_remaining_parts = set()\n",
    "scarce_remaining_models = []\n",
    "missing_models = 0\n",
    "model_missing_parts_dict = {}\n",
    "for col in scarce_done_parts.columns:\n",
    "    model_missing_parts = list(scarce_done_parts.loc[scarce_done_parts[col].isna()].index)\n",
    "    print(\"\\n\" + col)\n",
    "    print(\"\\n\".join(model_missing_parts))\n",
    "    if len(model_missing_parts) > 0:\n",
    "        scarce_remaining_models.append(col)\n",
    "        scarce_remaining_parts = scarce_remaining_parts.union(set(model_missing_parts))\n",
    "        missing_models += len(model_missing_parts)\n",
    "        model_missing_parts_dict[col] = model_missing_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821af62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving PIDS of missing models to files\n",
    "for model, parts in model_missing_parts_dict.items():\n",
    "    filename = f\"scarce_remaining/{model}.txt\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"\\n\".join(parts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee73e3f",
   "metadata": {},
   "source": [
    "## Analyzing Inferred Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00040ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining paths\n",
    "\n",
    "# CM inferred strategies\n",
    "results_path = '../results'\n",
    "inferred_path = results_path + \"/cm/inferred_strategies\"\n",
    "\n",
    "scarce_inferred = inferred_path + \"/scarcity_scarce/strategies.pkl\"\n",
    "control_inferred = inferred_path + \"/scarcity_control/strategies.pkl\"\n",
    "\n",
    "# Output files for strategies\n",
    "scarce_file = results_path + \"/mouselab-mdp-final-scarce.csv\"\n",
    "control_file = results_path + \"/mouselab-mdp-final-control.csv\"\n",
    "\n",
    "# Expected scores of all strategies\n",
    "strategy_scores_scarce_path = results_path + \"/cm/strategy_scores/scarcity_scarce_clickcost_0.25_strategy_scores.pkl\"\n",
    "strategy_scores_control_path = results_path + \"/cm/strategy_scores/scarcity_control_clickcost_1.0_strategy_scores.pkl\"\n",
    "\n",
    "\n",
    "scarce_df_rows = []\n",
    "control_df_rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading csv files\n",
    "\n",
    "with open(scarce_file, newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for idx, row in enumerate(reader):\n",
    "        if idx == 0: continue\n",
    "        scarce_df_rows.append([row[0], int(row[1])])\n",
    "        \n",
    "with open(control_file, newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for idx, row in enumerate(reader):\n",
    "        if idx == 0: continue\n",
    "        control_df_rows.append([row[0], int(row[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d356090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading inferred strategies\n",
    "\n",
    "with open(scarce_inferred, 'rb') as f:\n",
    "    scarce_strategies = pickle.load(f)\n",
    "    \n",
    "with open(control_inferred, 'rb') as f:\n",
    "    control_strategies = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc40aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_df = all_trials_df[['workerId', 'trialNumbers', 'scarce']].copy()\n",
    "strategy_df['strategy'] = [None] * len(strategy_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for workerId, stratList in scarce_strategies.items():\n",
    "    try:\n",
    "        strategy_df.loc[strategy_df.workerId == workerId, 'strategy'] = stratList\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "for workerId, stratList in control_strategies.items():\n",
    "    try:\n",
    "        strategy_df.loc[strategy_df.workerId == workerId, 'strategy'] = stratList\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636aa3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading strategy scores\n",
    "\n",
    "with open(strategy_scores_scarce_path, 'rb') as file:\n",
    "    strategy_scores_scarce = pickle.load(file)\n",
    "    \n",
    "with open(strategy_scores_control_path, 'rb') as file:\n",
    "    strategy_scores_control = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7241715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the strategy score dataframes\n",
    "\n",
    "strategy_scores_scarce_list = [[k,v] for k,v in strategy_scores_scarce.items()]\n",
    "strategy_scores_control_list = [[k,v] for k,v in strategy_scores_control.items()]\n",
    "\n",
    "scarce_cluster_df = pd.DataFrame(strategy_scores_scarce_list, columns=[\"strategy\", \"score\"])\n",
    "control_cluster_df = pd.DataFrame(strategy_scores_control_list, columns=[\"strategy\", \"score\"])\n",
    "\n",
    "# Scale to 0-1:\n",
    "scarce_cluster_df['scoreScaled'] = (scarce_cluster_df['score'] - scarce_cluster_df['score'].min()) / (scarce_cluster_df['score'].max() - scarce_cluster_df['score'].min())\n",
    "control_cluster_df['scoreScaled'] = (control_cluster_df['score'] - control_cluster_df['score'].min()) / (control_cluster_df['score'].max() - control_cluster_df['score'].min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6899f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the strategy scores\n",
    "\n",
    "scarce_clusters = scipy.cluster.vq.kmeans(scarce_cluster_df['scoreScaled'], k_or_guess=[0.6,0.5,0.4])\n",
    "scarce_cluster_centers = scarce_clusters[0]\n",
    "\n",
    "control_clusters = scipy.cluster.vq.kmeans(control_cluster_df['scoreScaled'], k_or_guess=[0.6,0.5,0.4])\n",
    "control_cluster_centers = control_clusters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cluster labels: 0 - adaptive, 1 - moderately adaptive, 2 - maladaptive\n",
    "\n",
    "scarce_clusters_repeated = np.repeat(np.expand_dims(scarce_cluster_centers, axis=0), len(scarce_cluster_df), axis=0)\n",
    "scarce_cluster_distances = (scarce_clusters_repeated - np.repeat(np.expand_dims(np.array(scarce_cluster_df['scoreScaled']), axis=1),repeats=len(scarce_cluster_centers), axis=1))**2\n",
    "scarce_cluster_labels = np.argmin(scarce_cluster_distances, axis=1)\n",
    "\n",
    "control_clusters_repeated = np.repeat(np.expand_dims(control_cluster_centers, axis=0), len(control_cluster_df), axis=0)\n",
    "control_cluster_distances = (control_clusters_repeated - np.repeat(np.expand_dims(np.array(control_cluster_df['scoreScaled']), axis=1),repeats=len(control_cluster_centers), axis=1))**2\n",
    "control_cluster_labels = np.argmin(control_cluster_distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of cluster labels:\n",
    "\n",
    "scarce_cluster_dict = { strat: cluster for (strat, cluster) in zip(scarce_cluster_df.strategy, scarce_cluster_labels)}\n",
    "\n",
    "control_cluster_dict = { strat: cluster for (strat, cluster) in zip(control_cluster_df.strategy, control_cluster_labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394036b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add clusters and strategy scores to strategy_df \n",
    "\n",
    "strategy_df['cluster'] = [None] * len(strategy_df)\n",
    "strategy_df['strategyScores'] = [None] * len(strategy_df)\n",
    "\n",
    "for idx, row in strategy_df.iterrows():\n",
    "    if row['scarce'] == 0:\n",
    "        strategy_df.at[idx, 'cluster'] = scarce_cluster_dict[row['strategy']-1]\n",
    "        strategy_df.at[idx, 'strategyScores'] = strategy_scores_scarce[row['strategy']-1] / scarcity_level\n",
    "    else:\n",
    "        strategy_df.at[idx, 'cluster'] = control_cluster_dict[row['strategy']-1]\n",
    "        strategy_df.at[idx, 'strategyScores'] = strategy_scores_control[row['strategy']-1]\n",
    "    \n",
    "    \n",
    "strategy_df['strategyScores'] = strategy_df['strategyScores'].astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab66400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging trial data with strategy data\n",
    "\n",
    "strategy_cols = [\"workerId\", \"trialNumbers\", \"strategy\", \"cluster\", \"strategyScores\"]\n",
    "\n",
    "all_trials_df = pd.merge(all_trials_df, strategy_df[strategy_cols], on=['workerId', \"trialNumbers\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96684a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95161a4e",
   "metadata": {},
   "source": [
    "## Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7fe4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group statistics by participant\n",
    "all_part_groups = all_trials_df.groupby('workerId')\n",
    "control_part_groups = all_trials_df.loc[all_trials_df.scarce==0].groupby(\"workerId\")\n",
    "scarce_part_groups = all_trials_df.loc[all_trials_df.scarce==1].groupby(\"workerId\")\n",
    "\n",
    "# Get the averages of each statistic by participant\n",
    "control_part_averages = control_part_groups.mean()\n",
    "scarce_part_averages = scarce_part_groups.mean()\n",
    "all_part_averages = all_part_groups.mean()\n",
    "\n",
    "# Compute the proportion of trials with no clicks\n",
    "all_part_averages['pctgSomeClick'] = (all_part_groups.apply(lambda x: x[x > 0].count()) / all_part_groups.count())['numClicks']\n",
    "scarce_part_averages['pctgSomeClick'] = (scarce_part_groups.apply(lambda x: x[x > 0].count()) / scarce_part_groups.count())['numClicks']\n",
    "control_part_averages['pctgSomeClick'] = (control_part_groups.apply(lambda x: x[x > 0].count()) / control_part_groups.count())['numClicks']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d607679c",
   "metadata": {},
   "source": [
    "### Removing Based on Selected Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f70dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_exclusion_col = 'pctgSomeClick'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers \n",
    "\n",
    "# Excluding those that have 100% of their trials with no clicks\n",
    "\n",
    "\n",
    "zscore_threshold = (0.0 - all_part_averages[selected_exclusion_col].mean()) / all_part_averages[selected_exclusion_col].std()\n",
    "excluded_parts_control = control_part_averages[control_part_averages[selected_exclusion_col] == 0]\n",
    "included_parts_control = control_part_averages[control_part_averages[selected_exclusion_col] > 0]\n",
    "\n",
    "zscore_threshold = (0.0 - scarce_part_averages[selected_exclusion_col].mean()) / scarce_part_averages[selected_exclusion_col].std()\n",
    "excluded_parts_scarce = scarce_part_averages[scarce_part_averages[selected_exclusion_col] == 0]\n",
    "included_parts_scarce = scarce_part_averages[scarce_part_averages[selected_exclusion_col] > 0]\n",
    "\n",
    "num_excluded_parts = len(excluded_parts_scarce) + len(excluded_parts_control)\n",
    "\n",
    "print(\"Number of participants excluded: {0} ({1:0.2f}%)\".format(\n",
    "    num_excluded_parts,\n",
    "    100 * num_excluded_parts /len(all_trials_df.groupby(\"workerId\"))\n",
    "))\n",
    "print(\"\\tControl: {0} ({1:0.2f}%)\".format(\n",
    "    len(excluded_parts_control),\n",
    "    100 * len(excluded_parts_control) / len(control_part_averages)\n",
    "))\n",
    "print(\"\\tScarce: {0} ({1:0.2f}%)\".format(\n",
    "    len(excluded_parts_scarce),\n",
    "    100 * len(excluded_parts_scarce) / len(scarce_part_averages)\n",
    "))\n",
    "included_part_IDs = list(included_parts_scarce.index) + list(included_parts_control.index)\n",
    "outliers_excluded_part = all_trials_df.loc[all_trials_df.workerId.isin(included_part_IDs)].reset_index(drop=True)\n",
    "outliers_excluded_part['trialNumbers'] = outliers_excluded_part['trialNumbers'].astype('int64')\n",
    "outliers_excluded_part['scarce'] = outliers_excluded_part['scarce'].astype('int64')\n",
    "outliers_excluded_part['numRewardedTrials'] = outliers_excluded_part['numRewardedTrials'].astype('int64')\n",
    "outliers_excluded_part['numUnrewardedTrials'] = outliers_excluded_part['numUnrewardedTrials'].astype('int64')\n",
    "\n",
    "\n",
    "print(\"Number of total trials excluded: {0} ({1:0.2f}%)\".format(len(all_trials_df)-len(outliers_excluded_part), 100-100*len(outliers_excluded_part)/(len(all_trials_df))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
