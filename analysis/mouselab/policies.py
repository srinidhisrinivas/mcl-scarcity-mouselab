import random
from abc import abstractmethod
from collections import Counter, defaultdict, deque, namedtuple

import numpy as np
from numpy.random import default_rng
from toolz import memoize

from mouselab.agents import Component, Model
from mouselab.utils import PriorityQueue, softmax

np.set_printoptions(precision=3, linewidth=200)


class Policy(Component):
    """Chooses actions."""

    def __init__(self):
        super().__init__()

    @abstractmethod
    def act(self, state):
        """Returns an action to take in the given state."""
        pass

    def attach(self, agent):
        if not hasattr(agent, "env"):
            raise ValueError("Must attach env before attaching policy.")
        super().attach(agent)


class FunctionPolicy(Policy):
    def __init__(self, policy):
        super().__init__()
        self.policy = policy

    def act(self, state):
        return self.policy(state)


class RandomPolicy(Policy):
    """Chooses actions randomly."""

    def __init__(self, seed=None):
        super().__init__()
        self.rng = default_rng(seed)

    def act(self, state):
        probabilities = self.action_distribution(state)
        try:
            return self.rng.choice(len(probabilities), p=probabilities)
        except Exception:
            raise Exception("Problem in action selection of RandomPolicy.")

    def action_distribution(self, state):
        action_probabilities = np.zeros(self.n_action)
        possible_actions = [a for a in self.env.actions(state)]
        num_actions = len(possible_actions)
        for a in possible_actions:
            action_probabilities[a] = 1.0 / num_actions
        return action_probabilities


class SoftmaxPolicy(Policy):
    """Samples actions from a softmax over preferences."""

    def __init__(self, preference=None, temp=1e-9, noise=1e-9, seed=None):
        super().__init__()
        if preference is None:
            assert hasattr(self, "preference")
        elif isinstance(preference, dict):
            self.preference = lambda state, action: preference[(state, action)]
        else:
            self.preference = preference
        self.temp = temp
        self.noise = noise
        self.rng = default_rng(seed)

    def act(self, state):
        probs = self.action_distribution(state)
        probs += self.rng.random(len(probs)) * self.noise
        probs /= probs.sum()
        return self.rng.choice(len(probs), p=probs)

    def action_distribution(self, state):
        """
        Finds the action distribution given a softmax
        :param state:
        :return:
        """
        q, possible_actions = self.preferences(state)
        softmax_actions = softmax(q, self.temp)
        # mostly for readability, not efficiency
        softmax_dict = dict(zip(possible_actions, softmax_actions))

        action_probs = np.fromiter(
            (
                0 if action not in possible_actions else softmax_dict[action]
                for action in range(self.n_action)
            ),
            dtype=np.float64,
        )
        return action_probs

    def preferences(self, state):
        """
        Finds the Q values for possible actions in a state
        :param state: state of interest
        :return: q values, respective possible_actions
        """
        # need to first only do softmax over possible actions
        possible_actions = np.fromiter(self.env.actions(state), dtype=int)
        q = np.fromiter(
            (self.preference(state, a) for a in possible_actions), dtype=np.float64
        )
        return q, possible_actions


class OptimalQ(Policy):
    """Samples from optimal preferences in a state."""

    def __init__(self, preference=None, seed=None):
        super().__init__()
        if preference is None:
            assert hasattr(self, "preference")
        elif isinstance(preference, dict):
            self.preference = lambda state, action: preference[(state, action)]
        else:
            self.preference = preference
        self.rng = default_rng(seed)

    def act(self, state):
        probs = self.action_distribution(state)
        return self.rng.choice(len(probs), p=probs)

    def action_distribution(self, state):
        """
        Finds the actions with max Q
        :param state:
        :return:
        """
        q, possible_actions = self.preferences(state)
        max_q = np.amax(q)
        max_actions = [
            curr_action
            for curr_q, curr_action in zip(q, possible_actions)
            if np.abs(max_q - curr_q) < np.finfo(np.float).eps
        ]

        action_probs = np.fromiter(
            (
                0 if action not in max_actions else 1.0 / len(max_actions)
                for action in range(self.n_action)
            ),
            dtype=np.float64,
        )
        return action_probs

    def preferences(self, state):
        """
        Finds the Q values for possible actions in a state
        :param state: state of interest
        :return: q values, respective possible_actions
        """
        # need to first only do softmax over possible actions
        possible_actions = np.fromiter(self.env.actions(state), dtype=int)
        q = np.fromiter(
            (self.preference(state, a) for a in possible_actions), dtype=np.float64
        )
        return q, possible_actions


class RandomTreePolicy(Policy):
    """Chooses actions randomly."""

    def __init__(self):
        super().__init__()

    def act(self, state):
        actions = list(self.env.actions(self.env._state))
        return random.choice(actions)


class MaxQPolicy(Policy):
    """Chooses the action with highest Q value."""

    def __init__(self, Q, epsilon=0.5, anneal=0.95, **kwargs):
        super().__init__(**kwargs)
        self.Q = Q
        self.epsilon = epsilon
        self.anneal = anneal

    def act(self, state, anneal_step=0):
        q = self.Q.predict(state)
        epsilon = self.epsilon * self.anneal ** anneal_step
        if np.random.rand() < epsilon:
            noise = np.random.random(q.shape) * 1000
        else:
            noise = np.random.random(q.shape) * 0.001
        return np.argmax(q + noise)


class LiederPolicy(Policy):
    """The meta-policy of Lieder et al. (2017) AAAI."""

    def __init__(self, theta, bad_max=False):
        self.theta = np.array(theta)

    def act(self, state):
        def Q(a):
            if a == self.env.term_action:
                return self.env.expected_term_reward(self.env._state)
            else:
                return np.dot(self.theta, self.env.action_features(a))

        action = max(self.env.actions(state), key=Q)
        return action


class MaxQSamplePolicy(Policy):
    """Chooses the action with highest sampled Q value.

    `Q.predict` must have the kwarg `return_var`."""

    def __init__(self, Q, **kwargs):
        super().__init__(**kwargs)
        self.Q = Q
        self.save_regret = True

    def act(self, state):
        q, var = self.Q.predict(state, return_var=True)
        sigma = var ** 0.5
        q_samples = q + np.random.randn() * sigma
        a = np.argmax(q_samples)
        if self.save_regret:
            q = q.flat
            a1 = np.argmax(q)
            self.save("max", a == a1)
            self.save("regret", q[a1] - q[a])
        return a


class ActorCritic(Policy):
    """docstring for ActorCritic"""

    def __init__(self, critic, actor_lr=0.001, discount=0.99, actor_lambda=1, **kwargs):
        super().__init__()
        self.critic = critic
        self.discount = discount
        self.actor_lambda = actor_lambda
        self.actor_lr = actor_lr

        self._actor_discount = np.array(
            [(self.discount * self.actor_lambda) ** i for i in range(5000)]
        )

        self.memory = deque(maxlen=100)
        self.batch_size = 20

    def attach(self, agent):
        super().attach(agent)
        self.actor = self.build_actor()

    def build_actor(self):
        from keras.layers import Dense
        from keras.models import Sequential
        from keras.optimizers import Nadam

        actor = Sequential(
            [
                Dense(
                    24,
                    input_dim=self.state_size,
                    activation="relu",
                    kernel_initializer="he_uniform",
                ),
                # Dense(24, activation='relu',
                #       kernel_initializer='he_uniform'),
                Dense(
                    self.n_action, activation="softmax", kernel_initializer="he_uniform"
                ),
            ]
        )
        # actor.summary()
        actor.compile(loss="categorical_crossentropy", optimizer=Nadam(self.actor_lr))
        return actor

    def act(self, state):
        policy = self.actor.predict(state.reshape(1, -1)).flatten()
        return np.random.choice(self.n_action, 1, p=policy)[0]

    # update networks every episode
    def finish_episode(self, trace):
        if len(self.memory) >= self.batch_size:
            batch = np.random.choice(self.memory, self.batch_size)
            batch.append(trace)
            self.train_batch(batch)

    def train_batch(self, traces):
        def data():
            for trace in traces:
                yield trace["states"][:-1], trace["actions"], trace["rewards"]

        state, action, reward = map(np.concatenate, zip(*data()))
        n_step = len(state)
        # See Schulman et al. 2016 ICLR paper
        value = np.r_[self.critic.predict(state).flat, 0]  # final state value
        delta = reward + self.discount * value[1:] - value[:-1]  # pg. 4
        advantage = np.zeros((n_step, self.n_action))
        value_target = np.zeros((n_step, 1))

        for i in range(n_step):
            adv = np.sum(delta[i:] * self._actor_discount[: n_step - i])
            advantage[i, action[i]] = adv
            val_error = np.sum(delta[i:] * self._critic_discount[: n_step - i])
            value_target[i, 0] = value[i] + val_error
            # value_target[i, 0] = reward[i] + self.discount * value[i+1]

        self.actor.fit(state, advantage, epochs=1, verbose=0)
        self.critic.fit(state, value_target, epochs=1, verbose=0)

    def train(self, trace):
        state = np.stack(trace["states"][:-1])  # ignore final state
        action = trace["actions"]
        reward = trace["rewards"]
        n_step = len(state)

        # See Schulman et al. 2016 ICLR paper
        value = np.r_[self.critic.predict(state).flat, 0]  # final state value
        delta = reward + self.discount * value[1:] - value[:-1]  # pg. 4
        advantage = np.zeros((n_step, self.n_action))
        value_target = np.zeros((n_step, 1))

        for i in range(n_step):
            adv = np.sum(delta[i:] * self._actor_discount[: n_step - i])
            advantage[i, action[i]] = adv
            val_error = np.sum(delta[i:] * self._critic_discount[: n_step - i])
            value_target[i, 0] = value[i] + val_error
            # value_target[i, 0] = reward[i] + self.discount * value[i+1]

        self.actor.fit(state, advantage, epochs=1, verbose=0)
        self.critic.fit(state, value_target, epochs=1, verbose=0)


class GeneralizedAdvantageEstimation(Policy):
    """A variance-reducing extension of Advantage Actor Critic.

    https://arxiv.org/abs/1506.02438
    """

    def __init__(
        self,
        actor_lr=0.001,
        critic_lr=0.005,
        discount=0.99,
        actor_lambda=1,
        critic_lambda=1,
        **kwargs
    ):
        super().__init__()
        self.discount = discount
        self.actor_lambda = actor_lambda
        self.critic_lambda = critic_lambda
        self.actor_lr = actor_lr
        self.critic_lr = critic_lr

        self._actor_discount = np.array(
            [(self.discount * self.actor_lambda) ** i for i in range(5000)]
        )
        self._critic_discount = np.array(
            [(self.discount * self.critic_lambda) ** i for i in range(5000)]
        )

        self._memory = deque(maxlen=100)
        self.batch_size = 20

    def attach(self, agent):
        super().attach(agent)
        self.actor = self.build_actor()
        self.critic = self.build_critic()

    def build_actor(self):
        from keras.layers import Dense
        from keras.models import Sequential
        from keras.optimizers import Nadam

        actor = Sequential(
            [
                # Dense(24, input_dim=self.state_size, activation='relu',
                #       kernel_initializer='he_uniform'),
                # Dense(24, activation='relu',
                #       kernel_initializer='he_uniform'),
                Dense(
                    self.n_action,
                    input_dim=self.state_size,
                    activation="softmax",
                    kernel_initializer="he_uniform",
                )
            ]
        )
        # actor.summary()
        actor.compile(loss="categorical_crossentropy", optimizer=Nadam(self.actor_lr))
        return actor

    # critic: state is input and value of state is output of model
    def build_critic(self):
        from keras.layers import Dense
        from keras.models import Sequential
        from keras.optimizers import Nadam

        critic = Sequential(
            [
                # Dense(24, input_dim=self.state_size, activation='relu',
                #       kernel_initializer='he_uniform'),
                # Dense(24, activation='relu',
                #       kernel_initializer='he_uniform'),
                Dense(
                    1,
                    input_dim=self.state_size,
                    activation="linear",
                    kernel_initializer="he_uniform",
                )
            ]
        )
        # critic.summary()
        critic.compile(loss="mse", optimizer=Nadam(self.critic_lr))
        return critic

    def act(self, state):
        policy = self.actor.predict(state.reshape(1, -1)).flatten()
        return np.random.choice(self.n_action, 1, p=policy)[0]

    # update networks every episode
    def finish_episode(self, trace):
        if len(self._memory) >= self.batch_size:
            batch = np.random.choice(self._memory, self.batch_size)
            batch.append(trace)
            self.train_batch(batch)
        else:
            self.train(trace)

    def train_batch(self, traces):
        def data():
            for trace in traces:
                yield trace["states"][:-1], trace["actions"], trace["rewards"]

        state, action, reward = map(np.concatenate, zip(*data()))
        n_step = len(state)
        # See Schulman et al. 2016 ICLR paper
        # value = np.r_[self.critic.predict(state).flat, 0]  # final state value
        value = np.r_[state.sum(1), 0]
        delta = reward + self.discount * value[1:] - value[:-1]  # pg. 4
        advantage = np.zeros((n_step, self.n_action))
        value_target = np.zeros((n_step, 1))

        for i in range(n_step):
            adv = np.sum(delta[i:] * self._actor_discount[: n_step - i])
            advantage[i, action[i]] = adv
            val_error = np.sum(delta[i:] * self._critic_discount[: n_step - i])
            value_target[i, 0] = value[i] + val_error
            # value_target[i, 0] = reward[i] + self.discount * value[i+1]

        self.actor.fit(state, advantage, epochs=1, verbose=0)
        self.critic.fit(state, value_target, epochs=1, verbose=0)

    def train(self, trace):
        state = np.stack(trace["states"][:-1])  # ignore final state
        action = trace["actions"]
        reward = trace["rewards"]
        n_step = len(state)

        # See Schulman et al. 2016 ICLR paper
        value = np.r_[self.critic.predict(state).flat, 0]  # final state value
        delta = reward + self.discount * value[1:] - value[:-1]  # pg. 4
        advantage = np.zeros((n_step, self.n_action))
        value_target = np.zeros((n_step, 1))

        for i in range(n_step):
            adv = np.sum(delta[i:] * self._actor_discount[: n_step - i])
            advantage[i, action[i]] = adv
            val_error = np.sum(delta[i:] * self._critic_discount[: n_step - i])
            value_target[i, 0] = value[i] + val_error
            # value_target[i, 0] = reward[i] + self.discount * value[i+1]

        self.actor.fit(state, advantage, epochs=1, verbose=0)
        self.critic.fit(state, value_target, epochs=1, verbose=0)


class FixedPlanPolicy(Policy):
    """A policy that blindly executes a fixed sequence of actions."""

    Node = namedtuple("Node", ("state", "path", "reward", "done"))

    def __init__(self, plan, **kwargs):
        super().__init__(**kwargs)
        self._plan = plan

    def start_episode(self, state):
        super().start_episode(state)
        self.plan = iter(self._plan)
        # self.model = Model(self.env)

    def act(self, state):
        return next(self.plan)


class ValSearchPolicy(Policy):
    """Searches for the maximum reward path using a model."""

    def __init__(self, V, replan=False, epsilon=0, noise=1, anneal=1, **kwargs):
        super().__init__(**kwargs)
        self.V = V
        self.replan = replan
        self.epsilon = epsilon
        self.noise = noise
        self.anneal = anneal
        self.history = None
        self.model = None
        self.plan = None

    def start_episode(self, state):
        self.history = Counter()
        self.model = Model(self.env)
        self.plan = iter(())  # start with no plan

    def finish_episode(self, trace):
        self.ep_trace["berries"] = self.env._observe()[-1]

    def act(self, state):
        # return self.env.action_space.sample()
        self.history[state] += 1
        try:
            if self.replan:
                raise StopIteration()
            else:
                return next(self.plan)
        except StopIteration:
            self.plan = iter(self.make_plan(state))
            return next(self.plan)

    def make_plan(self, state, expansions=2000):

        Node = namedtuple("Node", ("state", "path", "reward", "done"))
        env = self.env
        V = memoize(self.V.predict)
        self.node_history = []

        def eval_node(node, noisy=False):
            if not node.path:
                return np.inf  # the empty plan has infinite cost
            obs = env._observe(node.state)
            noise = (
                np.random.rand() * (self.noise * self.anneal ** self.i_episode)
                if noisy
                else 0
            )
            value = 0 if node.done else V(obs)[0]
            boredom = -0.1 * self.history[obs]
            score = node.reward + value + noise + boredom
            return -score

        start = Node(env._state, [], 0, False)
        frontier = PriorityQueue(key=eval_node)
        frontier.push(start)
        reward_to_state = defaultdict(lambda: -np.inf)
        reward_to_state[start.state] = 0
        best_finished = start

        def expand(node):
            nonlocal best_finished
            best_finished = min((best_finished, node), key=eval_node)
            s0, p0, r0, _ = node
            for a, s1, r, done in self.model.options(s0):
                node1 = Node(s1, p0 + [a], r0 + r, done)
                if node1.reward <= reward_to_state[s1]:
                    continue  # cannot be better than an existing node
                self.node_history.append(
                    {
                        "path": node1.path,
                        "r": node1.reward,
                        "b": self.env._observe(node1.state)[-1],
                        "v": -eval_node(node1),
                    }
                )
                reward_to_state[s1] = node1.reward
                if done:
                    best_finished = min((best_finished, node1), key=eval_node)
                else:
                    frontier.push(node1)

        for i in range(expansions):
            if frontier:
                expand(frontier.pop())
            else:
                break

        if frontier:
            plan = min(best_finished, frontier.pop(), key=eval_node)
        else:
            plan = best_finished
        # choices = concat([completed, map(get(1), take(100, frontier))])
        # plan = min(choices, key=eval_node(noisy=True))
        self.log(
            i,
            len(plan.path),
            -round(eval_node(plan, noisy=False), 2),
            plan.done,
        )
        # self._trace['paths'].append(plan.path)
        return plan.path


class Astar(Policy):
    """A* search finds the shortest path to a goal."""

    def __init__(self, heuristic):
        assert 0  # this implementation is incorrect
        super().__init__()
        self.heuristic = heuristic
        self.plan = iter(())

    def start_episode(self, state):
        self.history = Counter()
        self.model = Model(self.env)

    def act(self, state):
        # return self.env.action_space.sample()
        self.history[state] += 1
        try:
            return next(self.plan)
        except StopIteration:
            self.plan = iter(self.make_plan(state))
            return next(self.plan)

    def eval_node(self, node):
        if not node.path:
            return np.inf  # the empty plan has infinite cost
        obs = self.env._observe(node.state)
        value = 0 if node.done else self.heuristic(self.env, obs)
        boredom = -0.1 * self.history[obs]
        score = node.reward + value + boredom
        return -score

    def make_plan(self, state, expansions=5000):

        Node = namedtuple("Node", ("state", "path", "reward", "done"))
        eval_node = self.eval_node
        start = Node(self.env._state, [], 0, False)
        frontier = PriorityQueue(key=eval_node)
        frontier.push(start)
        reward_to_state = defaultdict(lambda: -np.inf)
        # import IPython; IPython.embed()
        best_finished = start

        def expand(node):
            # print(
            #     node.state,
            #     node.reward,
            #     self.rts[node.state],
            #     V(env._observe(node.state)),
            # )
            # time.sleep(0.1)
            nonlocal best_finished
            # best_finished = min((best_finished, node), key=eval_node)
            s0, p0, r0, _ = node
            for a, s1, r, done in self.model.options(s0):
                node1 = Node(s1, p0 + [a], r0 + r, done)
                if node1.reward <= reward_to_state[s1]:
                    # print('abandon')
                    pass
                    continue  # cannot be better than an existing node
                # self.save('node', node)
                reward_to_state[s1] = node1.reward
                if done:
                    best_finished = min((best_finished, node1), key=eval_node)
                else:
                    frontier.push(node1)

        for i in range(expansions):
            self.save("frontier", [n[1].state for n in frontier])
            if frontier:
                expand(frontier.pop())
            else:
                break

        if frontier:
            # plan = min(best_finished, frontier.pop(), key=eval_node)
            plan = frontier.pop()
            raise RuntimeError("No plan found.")
        else:
            plan = best_finished
        # choices = concat([completed, map(get(1), take(100, frontier))])
        # plan = min(choices, key=eval_node(noisy=True))
        # self.log(
        #     i,
        #     len(plan.path),
        #     -round(eval_node(plan, noisy=False), 2),
        #     plan.done,
        # )
        # self._trace['paths'].append(plan.path)
        self.save("plan", plan)
        return plan.path
