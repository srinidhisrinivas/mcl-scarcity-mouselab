{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41993004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from mouselab.mouselab import MouselabEnv\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy\n",
    "import math\n",
    "import sklearn.cluster\n",
    "from math import floor, ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2d095",
   "metadata": {},
   "source": [
    "# Reading Data from Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9365a",
   "metadata": {},
   "source": [
    "The anonymized participant data can be found on the GitHub repository for this project in the folder 'anonymized_data'. Then, the remainder of the cells in the notebook can be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7fe000",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileprefix = \"final\" # pilot_#, final\n",
    "datafolder = '../results/anonymized_data/'\n",
    "\n",
    "all_part_files = [filename for filename in os.listdir(datafolder) if filename.startswith(fileprefix) and 'results' not in filename]\n",
    "all_part_paths = [datafolder + filename for filename in all_part_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ff701",
   "metadata": {},
   "source": [
    "# Creating Trial Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1747e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each participant has their own file, generated from the Heroku postgres output in the \n",
    "# preprocessing notebook\n",
    "\n",
    "part_stats_dicts = []\n",
    "scarcity_level = 0.25\n",
    "for file in all_part_paths:\n",
    "    with open(file, 'r') as f:\n",
    "        participant = json.load(f)\n",
    "    try:\n",
    "        beginhit = datetime.datetime.strptime(participant['Beginhit'], '%Y-%m-%d %H:%M:%S.%f')\n",
    "    except TypeError:\n",
    "        beginhit = None\n",
    "    try:\n",
    "        endhit = datetime.datetime.strptime(participant['Endhit'], '%Y-%m-%d %H:%M:%S.%f')\n",
    "    except TypeError:\n",
    "        endhit = None\n",
    "    try:    \n",
    "        beginexp = datetime.datetime.strptime(participant['Beginexp'], '%Y-%m-%d %H:%M:%S.%f')\n",
    "    except TypeError:\n",
    "        beginexp = None\n",
    "    \n",
    "    try:\n",
    "        hitLength = (endhit-beginhit).seconds\n",
    "    except TypeError:\n",
    "        hitLength = None\n",
    "    \n",
    "    try:\n",
    "        expLength = (endhit-beginexp).seconds\n",
    "    except TypeError:\n",
    "        expLength = None\n",
    "        \n",
    "    numPracTrials = 0\n",
    "    trialLengths = []\n",
    "    trialScores = []\n",
    "    expectedScores = []\n",
    "    numClicks = []\n",
    "    avgClickLevel = []\n",
    "    rewardsWithheld = []\n",
    "    demographicsAvailable = False\n",
    "    effort = \"-1\"\n",
    "    age = None\n",
    "    gender = None\n",
    "    colorblind = None\n",
    "    \n",
    "    print(participant['workerId'])\n",
    "    try:\n",
    "        firstTrialStamp = participant['data'][0]['dateTime']\n",
    "    except:\n",
    "        print(\"No data\")\n",
    "        continue\n",
    "    lastTrialStamp = participant['data'][-1]['dateTime']\n",
    "    if hitLength is None:\n",
    "        hitLength = (lastTrialStamp - firstTrialStamp) / 1000\n",
    "    if expLength is None:\n",
    "        expLength = (lastTrialStamp - firstTrialStamp) / 1000\n",
    "        \n",
    "    # Dictionary to check whether stroop tasks have been completed\n",
    "    stroop_timestamps = {\n",
    "        \"1\" : {\n",
    "            \"start\" : 0,\n",
    "            \"end\" : 0,\n",
    "            \"done\" : False\n",
    "        },\n",
    "        \"2\" : {\n",
    "            \"start\" : 0,\n",
    "            \"end\" : 0,\n",
    "            \"done\" : False\n",
    "        }\n",
    "    }\n",
    "    # Dictionary to check whether mouselab task has been completed\n",
    "    mdp_timestamps = {\n",
    "        \"start\" : 0,\n",
    "        \"end\" : 0,\n",
    "        \"done\": False\n",
    "    }\n",
    "    \n",
    "    # Use state machine to parse participant trial data, update state \n",
    "    #  based on trials seen\n",
    "    current_state = \"stroop_1\"\n",
    "    stroop1Completed = 0\n",
    "    stroop1Correct = 0\n",
    "    stroop2Completed = 0\n",
    "    stroop2Correct = 0\n",
    "    last_trial = participant['data'][0]\n",
    "    feedback = \"\"\n",
    "    comments = \"\"\n",
    "    for idx, trial in enumerate(participant['data']):\n",
    "        trial_type = trial['trialdata']['trial_type']\n",
    "        try:\n",
    "            trial_id = str(trial['trialdata']['trial_id'])\n",
    "        except KeyError:\n",
    "            trial_id = \"\"\n",
    "        \n",
    "        # Start 1st stroop task\n",
    "        if trial_id == \"stroop_1_ready_1\" and not stroop_timestamps[\"1\"][\"done\"]:\n",
    "            stroop_timestamps[\"1\"][\"start\"] = trial['dateTime']\n",
    "        \n",
    "        # Start 2nd stroop task\n",
    "        if trial_id == \"stroop_2_ready_1\" and not stroop_timestamps[\"2\"][\"done\"]:\n",
    "            stroop_timestamps[\"2\"][\"start\"] = trial['dateTime']\n",
    "        \n",
    "        # Finish 1st stroop task\n",
    "        if trial_id == \"finish_distractor_1\" and not stroop_timestamps[\"1\"][\"done\"]:\n",
    "            stroop_timestamps[\"1\"][\"end\"] = trial['dateTime']\n",
    "            stroop_timestamps[\"1\"][\"done\"] = True\n",
    "        \n",
    "        # Finish 2nd stroop task\n",
    "        if trial_id == \"finish_distractor_2\" and not stroop_timestamps[\"2\"][\"done\"]:\n",
    "            stroop_timestamps[\"2\"][\"end\"] = trial['dateTime']\n",
    "            stroop_timestamps[\"2\"][\"done\"] = True\n",
    "        \n",
    "        # Start mouselab trials\n",
    "        if trial_id == \"mouselab_instructions_1\" and not mdp_timestamps[\"done\"]:\n",
    "            mdp_timestamps[\"done\"] = True\n",
    "            mdp_timestamps[\"start\"] = trial['dateTime']\n",
    "        \n",
    "        # Finish mouselab trials\n",
    "        if trial_id.startswith(\"final_quiz\"):\n",
    "            mdp_timestamps[\"end\"] = trial['dateTime']\n",
    "            \n",
    "        # Stroop trial encountered, count whether it belongs to the first set or second set\n",
    "        if \"congruent\" in trial_id or \"incongruent\" in trial_id or \"unrelated\" in trial_id:\n",
    "            if not stroop_timestamps[\"1\"][\"done\"]:\n",
    "                stroop1Completed += 1\n",
    "                \n",
    "                if trial['trialdata'][\"response\"].lower() == trial['trialdata'][\"correct_response\"].lower():\n",
    "                    stroop1Correct += 1\n",
    "                \n",
    "            else:\n",
    "                stroop2Completed += 1\n",
    "                if trial['trialdata'][\"response\"].lower() == trial['trialdata'][\"correct_response\"].lower():\n",
    "                    stroop2Correct += 1\n",
    "        \n",
    "        # Mouselab trial encountered\n",
    "        if trial_type == 'mouselab-mdp':\n",
    "            if trial_id.startswith('practice'):\n",
    "                numPracTrials += 1\n",
    "            else:\n",
    "                trialLengths.append(trial['trialdata']['trialTime'])\n",
    "                \n",
    "                trialScores.append(trial['trialdata']['score'])\n",
    "                rewardsWithheld.append(trial['trialdata']['withholdReward'])\n",
    "                \n",
    "                # Getting expected scores\n",
    "                g_truth = [0.0] + trial['trialdata']['stateRewards'][1:]\n",
    "                mEnv = MouselabEnv.new_symmetric_registered('high_increasing', ground_truth=g_truth)\n",
    "                clicks = trial['trialdata']['queries']['click']['state']['target']\n",
    "                for click in clicks:\n",
    "                    mEnv.step(int(click))\n",
    "                \n",
    "                \n",
    "                # Saving the number of clicks and the average depth of all clicks\n",
    "                numClicks.append(len(clicks))\n",
    "                \n",
    "                # Level one nodes are 1,5,9\n",
    "                clicksL1 = len([c for c in clicks if int(c) in [1,5,9]])\n",
    "                clicksL2 = len([c for c in clicks if int(c) in [2,6,10]])\n",
    "                clicksL3 = len([c for c in clicks if int(c) in [3,4,7,8,11,12]])\n",
    "                try:\n",
    "                    avgLevel = (clicksL1 + 2*clicksL2 + 3*clicksL3)/len(clicks)\n",
    "                except:\n",
    "                    avgLevel = 0\n",
    "                    \n",
    "                avgClickLevel.append(avgLevel)\n",
    "                \n",
    "                # plan quality is expected score of the trial minus costs\n",
    "                planQuality = mEnv._term_reward() + trial['trialdata']['costs']/(1.0 if participant['condition'] == 0 else scarcity_level)\n",
    "                expectedScores.append(planQuality)\n",
    "                \n",
    "        # Save information about end questionnaire\n",
    "        if trial_type == 'survey-html-form':\n",
    "            if 'effort' in trial['trialdata']['response']:\n",
    "                effort = trial['trialdata']['response']['effort']\n",
    "                demographicsAvailable = True\n",
    "        \n",
    "        # Participant failed quiz\n",
    "        if trial_id.startswith(\"finish_fail\"):\n",
    "            demographicsAvailable = True\n",
    "        \n",
    "        # Save information about participant feedback/comments\n",
    "        if trial_type == 'survey-text':\n",
    "            feedback = trial['trialdata']['response']['Q0']\n",
    "            comments = trial['trialdata']['response']['Q2']\n",
    "        last_trial = trial\n",
    "\n",
    "    # Check whether participant dropped out by seeing whether any of the phases doesn't have an end timestamp\n",
    "    dropoutPoint = None\n",
    "    for obj, type_ in zip([mdp_timestamps, stroop_timestamps[\"1\"], stroop_timestamps[\"2\"]], [\"mdp\", \"s1\", \"s2\"]):\n",
    "        if obj[\"start\"] > 0 and obj[\"end\"] == 0:\n",
    "            dropoutPoint = type_\n",
    "            obj[\"end\"] = lastTrialStamp\n",
    "            \n",
    "    # Save participant statistics in dictionary   \n",
    "    part_stats_dict = {\n",
    "        \"workerId\": participant['workerId'],\n",
    "        \"Beginhit\": beginhit,\n",
    "        \"Endhit\": endhit,\n",
    "        \"Beginexp\": beginexp,\n",
    "        \"psiturkStatus\" : participant[\"psiturkStatus\"],\n",
    "        \"browser\" : participant[\"browser\"],\n",
    "        \"platform\" : participant[\"platform\"],\n",
    "        \"language\" : participant[\"language\"],\n",
    "        \"hitLength\": hitLength,\n",
    "        \"expLength\": expLength,\n",
    "        \"totalLengthSum\": (lastTrialStamp - firstTrialStamp) / 1000,\n",
    "        \"numQuizAttempts\": numPracTrials / 2,\n",
    "        \"trialLengths\": trialLengths,\n",
    "        \"trialScores\" : trialScores,\n",
    "        \"numTrialsCompleted\": len(trialLengths),\n",
    "        \"rewardsWithheld\": rewardsWithheld,\n",
    "        \"effort\": effort,\n",
    "        \"condition\": participant['condition'],\n",
    "        \"stroop1Length\" : (stroop_timestamps[\"1\"][\"end\"] - stroop_timestamps[\"1\"][\"start\"]) / 1000,\n",
    "        \"stroop2Length\" : (stroop_timestamps[\"2\"][\"end\"] - stroop_timestamps[\"2\"][\"start\"]) / 1000,\n",
    "        \"mouselabLength\" : (mdp_timestamps[\"end\"] - mdp_timestamps[\"start\"]) / 1000,\n",
    "        \"stroop1Completed\" : stroop1Completed,\n",
    "        \"stroop2Completed\" : stroop2Completed,\n",
    "        \"stroop1Correct\" : stroop1Correct,\n",
    "        \"stroop2Correct\" : stroop2Correct,\n",
    "        \"dropoutPoint\" : dropoutPoint,\n",
    "        \"feedback\": feedback,\n",
    "        \"comments\": comments,\n",
    "        \"expectedScores\": expectedScores,\n",
    "        \"numClicks\": numClicks,\n",
    "        \"avgClickLevel\" : avgClickLevel,\n",
    "        \"demographicsAvailable\": demographicsAvailable\n",
    "    }\n",
    "    \n",
    "    # Get final score from data file, or if not present, sum up the scores of all the rewarded trials\n",
    "    if 'questiondata' in participant and 'final_score' in participant['questiondata']:\n",
    "        part_stats_dict['finalScore'] = participant['questiondata']['final_score']\n",
    "    else:\n",
    "        part_stats_dict['finalScore'] = sum([score for (score, withheld) in zip(trialScores, rewardsWithheld) if not withheld])\n",
    "    \n",
    "    part_stats_dict[\"expLengthSum\"] = part_stats_dict[\"stroop1Length\"] + part_stats_dict[\"mouselabLength\"] + part_stats_dict[\"stroop2Length\"]\n",
    "    part_stats_dict[\"expLengthDiff\"] = part_stats_dict[\"expLengthSum\"] - part_stats_dict[\"expLength\"]\n",
    "    if len(trialLengths) > 0:\n",
    "        part_stats_dict[\"averageTrialLength\"] = sum(trialLengths) / len(trialLengths)\n",
    "        part_stats_dict[\"propWithheld\"] = sum(rewardsWithheld) / len(rewardsWithheld)\n",
    "        \n",
    "    part_stats_dicts.append(part_stats_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ba11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling all data into trial data\n",
    "\n",
    "all_trial_data = {\n",
    "    \"trialScores\": [],\n",
    "    \"expectedScores\": [],\n",
    "    \"numClicks\" : [],\n",
    "    \"avgClickLevel\" : [],\n",
    "    \"trialNumbers\": [],\n",
    "    \"scarce\": [],\n",
    "    \"numRewardedTrials\": [],\n",
    "    \"numUnrewardedTrials\": [],\n",
    "    \"workerId\": [],\n",
    "    \"rewardsWithheld\" : [],\n",
    "}\n",
    "\n",
    "count = 0;\n",
    "\n",
    "for part in part_stats_dicts:\n",
    "    if len(part['trialScores']) not in [30, 120]:\n",
    "        print(part['condition'], part['workerId'], \"MDP Data Incomplete\")\n",
    "        continue\n",
    "        \n",
    "    # Include in analysis only those participants whose data is complete\n",
    "    if not part['demographicsAvailable']:\n",
    "        print(part['condition'], part['workerId'], \"Demographics missing\")\n",
    "        continue\n",
    "    count += 1\n",
    "    all_trial_data['trialScores'] += part['trialScores']\n",
    "    all_trial_data['rewardsWithheld'] += part['rewardsWithheld']\n",
    "    all_trial_data['workerId'] += [part['workerId']] * len(part['trialScores'])\n",
    "    all_trial_data['expectedScores'] += part['expectedScores']\n",
    "    all_trial_data['numClicks'] += part['numClicks']\n",
    "    all_trial_data['avgClickLevel'] += part['avgClickLevel']\n",
    "    all_trial_data['trialNumbers'] += list(range(1,len(part['trialScores'])+1))\n",
    "    all_trial_data['scarce'] += [int(len(part['trialScores']) == 120)] * len(part['trialScores'])\n",
    "    all_trial_data['numRewardedTrials'] += [i - sum(part['rewardsWithheld'][0:i]) for i in range(len(part['trialScores']))]\n",
    "    all_trial_data['numUnrewardedTrials'] += [sum(part['rewardsWithheld'][0:i]) for i in range(len(part['trialScores']))]\n",
    "\n",
    "all_trials_df = pd.DataFrame(all_trial_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2572bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3d94ed",
   "metadata": {},
   "source": [
    "## Analyzing Inferred Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d33b86",
   "metadata": {},
   "source": [
    "The strategies used by the participants on each trial were inferred by the Computational Microscope of the mcl_toolbox repository based on the click sequences made by the participants. These are then read in this section and added to the dataframe.\n",
    "\n",
    "(The results from this analysis were not used in the paper, so this data was not made available. Skip this section.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd085c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining paths\n",
    "\n",
    "# CM inferred strategies\n",
    "results_path = '../results'\n",
    "inferred_path = results_path + \"/cm/inferred_strategies\"\n",
    "\n",
    "scarce_inferred = inferred_path + \"/scarcity_scarce/strategies.pkl\"\n",
    "control_inferred = inferred_path + \"/scarcity_control/strategies.pkl\"\n",
    "\n",
    "# Output files for strategies\n",
    "scarce_file = results_path + \"/mouselab-mdp-final-scarce.csv\"\n",
    "control_file = results_path + \"/mouselab-mdp-final-control.csv\"\n",
    "\n",
    "# Expected scores of all strategies\n",
    "strategy_scores_scarce_path = results_path + \"/cm/strategy_scores/scarcity_scarce_clickcost_0.25_strategy_scores.pkl\"\n",
    "strategy_scores_control_path = results_path + \"/cm/strategy_scores/scarcity_control_clickcost_1.0_strategy_scores.pkl\"\n",
    "\n",
    "\n",
    "scarce_df_rows = []\n",
    "control_df_rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49afba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading csv files\n",
    "\n",
    "with open(scarce_file, newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for idx, row in enumerate(reader):\n",
    "        if idx == 0: continue\n",
    "        scarce_df_rows.append([row[0], int(row[1])])\n",
    "        \n",
    "with open(control_file, newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for idx, row in enumerate(reader):\n",
    "        if idx == 0: continue\n",
    "        control_df_rows.append([row[0], int(row[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c5b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading inferred strategies\n",
    "\n",
    "with open(scarce_inferred, 'rb') as f:\n",
    "    scarce_strategies = pickle.load(f)\n",
    "    \n",
    "with open(control_inferred, 'rb') as f:\n",
    "    control_strategies = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2cb6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_df = all_trials_df[['workerId', 'trialNumbers', 'scarce']].copy()\n",
    "strategy_df['strategy'] = [None] * len(strategy_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f1ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for workerId, stratList in scarce_strategies.items():\n",
    "    try:\n",
    "        strategy_df.loc[strategy_df.workerId == workerId, 'strategy'] = stratList\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "for workerId, stratList in control_strategies.items():\n",
    "    try:\n",
    "        strategy_df.loc[strategy_df.workerId == workerId, 'strategy'] = stratList\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading strategy scores\n",
    "\n",
    "with open(strategy_scores_scarce_path, 'rb') as file:\n",
    "    strategy_scores_scarce = pickle.load(file)\n",
    "    \n",
    "with open(strategy_scores_control_path, 'rb') as file:\n",
    "    strategy_scores_control = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e7fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the strategy score dataframes\n",
    "\n",
    "strategy_scores_scarce_list = [[k,v] for k,v in strategy_scores_scarce.items()]\n",
    "strategy_scores_control_list = [[k,v] for k,v in strategy_scores_control.items()]\n",
    "\n",
    "scarce_cluster_df = pd.DataFrame(strategy_scores_scarce_list, columns=[\"strategy\", \"score\"])\n",
    "control_cluster_df = pd.DataFrame(strategy_scores_control_list, columns=[\"strategy\", \"score\"])\n",
    "\n",
    "# Scale to 0-1:\n",
    "scarce_cluster_df['scoreScaled'] = (scarce_cluster_df['score'] - scarce_cluster_df['score'].min()) / (scarce_cluster_df['score'].max() - scarce_cluster_df['score'].min())\n",
    "control_cluster_df['scoreScaled'] = (control_cluster_df['score'] - control_cluster_df['score'].min()) / (control_cluster_df['score'].max() - control_cluster_df['score'].min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2771c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the strategy scores\n",
    "\n",
    "scarce_clusters = scipy.cluster.vq.kmeans(scarce_cluster_df['scoreScaled'], k_or_guess=[0.6,0.5,0.4])\n",
    "scarce_cluster_centers = scarce_clusters[0]\n",
    "\n",
    "control_clusters = scipy.cluster.vq.kmeans(control_cluster_df['scoreScaled'], k_or_guess=[0.6,0.5,0.4])\n",
    "control_cluster_centers = control_clusters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293db829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cluster labels: 0 - adaptive, 1 - moderately adaptive, 2 - maladaptive\n",
    "\n",
    "scarce_clusters_repeated = np.repeat(np.expand_dims(scarce_cluster_centers, axis=0), len(scarce_cluster_df), axis=0)\n",
    "scarce_cluster_distances = (scarce_clusters_repeated - np.repeat(np.expand_dims(np.array(scarce_cluster_df['scoreScaled']), axis=1),repeats=len(scarce_cluster_centers), axis=1))**2\n",
    "scarce_cluster_labels = np.argmin(scarce_cluster_distances, axis=1)\n",
    "\n",
    "control_clusters_repeated = np.repeat(np.expand_dims(control_cluster_centers, axis=0), len(control_cluster_df), axis=0)\n",
    "control_cluster_distances = (control_clusters_repeated - np.repeat(np.expand_dims(np.array(control_cluster_df['scoreScaled']), axis=1),repeats=len(control_cluster_centers), axis=1))**2\n",
    "control_cluster_labels = np.argmin(control_cluster_distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22137693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of cluster labels:\n",
    "\n",
    "scarce_cluster_dict = { strat: cluster for (strat, cluster) in zip(scarce_cluster_df.strategy, scarce_cluster_labels)}\n",
    "\n",
    "control_cluster_dict = { strat: cluster for (strat, cluster) in zip(control_cluster_df.strategy, control_cluster_labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add clusters and strategy scores to strategy_df \n",
    "\n",
    "strategy_df['cluster'] = [None] * len(strategy_df)\n",
    "strategy_df['strategyScores'] = [None] * len(strategy_df)\n",
    "\n",
    "for idx, row in strategy_df.iterrows():\n",
    "    if row['scarce'] == 0:\n",
    "        strategy_df.at[idx, 'cluster'] = scarce_cluster_dict[row['strategy']-1]\n",
    "        strategy_df.at[idx, 'strategyScores'] = strategy_scores_scarce[row['strategy']-1] / scarcity_level\n",
    "    else:\n",
    "        strategy_df.at[idx, 'cluster'] = control_cluster_dict[row['strategy']-1]\n",
    "        strategy_df.at[idx, 'strategyScores'] = strategy_scores_control[row['strategy']-1]\n",
    "    \n",
    "    \n",
    "strategy_df['strategyScores'] = strategy_df['strategyScores'].astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25460c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging trial data with strategy data\n",
    "\n",
    "strategy_cols = [\"workerId\", \"trialNumbers\", \"strategy\", \"cluster\", \"strategyScores\"]\n",
    "\n",
    "all_trials_df = pd.merge(all_trials_df, strategy_df[strategy_cols], on=['workerId', \"trialNumbers\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f6155",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529f5da4",
   "metadata": {},
   "source": [
    "## Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36477ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group statistics by participant\n",
    "all_part_groups = all_trials_df.groupby('workerId')\n",
    "control_part_groups = all_trials_df.loc[all_trials_df.scarce==0].groupby(\"workerId\")\n",
    "scarce_part_groups = all_trials_df.loc[all_trials_df.scarce==1].groupby(\"workerId\")\n",
    "\n",
    "# Get the averages of each statistic by participant\n",
    "control_part_averages = control_part_groups.mean()\n",
    "scarce_part_averages = scarce_part_groups.mean()\n",
    "all_part_averages = all_part_groups.mean()\n",
    "\n",
    "# Compute the proportion of trials with no clicks\n",
    "all_part_averages['pctgSomeClick'] = (all_part_groups.apply(lambda x: x[x > 0].count()) / all_part_groups.count())['numClicks']\n",
    "scarce_part_averages['pctgSomeClick'] = (scarce_part_groups.apply(lambda x: x[x > 0].count()) / scarce_part_groups.count())['numClicks']\n",
    "control_part_averages['pctgSomeClick'] = (control_part_groups.apply(lambda x: x[x > 0].count()) / control_part_groups.count())['numClicks']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2744b244",
   "metadata": {},
   "source": [
    "### Removing Based on Selected Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ef9283",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_exclusion_col = 'pctgSomeClick'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809124e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers \n",
    "\n",
    "# Excluding those that have 100% of their trials with no clicks\n",
    "\n",
    "\n",
    "zscore_threshold = (0.0 - all_part_averages[selected_exclusion_col].mean()) / all_part_averages[selected_exclusion_col].std()\n",
    "excluded_parts_control = control_part_averages[control_part_averages[selected_exclusion_col] == 0]\n",
    "included_parts_control = control_part_averages[control_part_averages[selected_exclusion_col] > 0]\n",
    "\n",
    "zscore_threshold = (0.0 - scarce_part_averages[selected_exclusion_col].mean()) / scarce_part_averages[selected_exclusion_col].std()\n",
    "excluded_parts_scarce = scarce_part_averages[scarce_part_averages[selected_exclusion_col] == 0]\n",
    "included_parts_scarce = scarce_part_averages[scarce_part_averages[selected_exclusion_col] > 0]\n",
    "\n",
    "num_excluded_parts = len(excluded_parts_scarce) + len(excluded_parts_control)\n",
    "\n",
    "print(\"Number of participants excluded: {0} ({1:0.2f}%)\".format(\n",
    "    num_excluded_parts,\n",
    "    100 * num_excluded_parts /len(all_trials_df.groupby(\"workerId\"))\n",
    "))\n",
    "print(\"\\tControl: {0} ({1:0.2f}%)\".format(\n",
    "    len(excluded_parts_control),\n",
    "    100 * len(excluded_parts_control) / len(control_part_averages)\n",
    "))\n",
    "print(\"\\tScarce: {0} ({1:0.2f}%)\".format(\n",
    "    len(excluded_parts_scarce),\n",
    "    100 * len(excluded_parts_scarce) / len(scarce_part_averages)\n",
    "))\n",
    "included_part_IDs = list(included_parts_scarce.index) + list(included_parts_control.index)\n",
    "outliers_excluded_part = all_trials_df.loc[all_trials_df.workerId.isin(included_part_IDs)].reset_index(drop=True)\n",
    "outliers_excluded_part['trialNumbers'] = outliers_excluded_part['trialNumbers'].astype('int64')\n",
    "outliers_excluded_part['scarce'] = outliers_excluded_part['scarce'].astype('int64')\n",
    "outliers_excluded_part['numRewardedTrials'] = outliers_excluded_part['numRewardedTrials'].astype('int64')\n",
    "outliers_excluded_part['numUnrewardedTrials'] = outliers_excluded_part['numUnrewardedTrials'].astype('int64')\n",
    "\n",
    "\n",
    "print(\"Number of total trials excluded: {0} ({1:0.2f}%)\".format(len(all_trials_df)-len(outliers_excluded_part), 100-100*len(outliers_excluded_part)/(len(all_trials_df))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c4fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to use for the remainder of the analysis\n",
    "\n",
    "model_save_folder = \"../results/model_sim_data/\"\n",
    "control_model = \"2.0.3.0.0\"\n",
    "scarce_model = \"2.0.3.0.0\"\n",
    "num_simulations = 1\n",
    "model_file_name = f\"c{control_model}_s{scarce_model}_{num_simulations}.csv\"\n",
    "\n",
    "use_model_data = False\n",
    "\n",
    "if use_model_data:\n",
    "    filtered_data = pd.read_csv(model_save_folder + model_file_name)\n",
    "else:\n",
    "    filtered_data = all_trials_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6df2cd",
   "metadata": {},
   "source": [
    "# Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd66de",
   "metadata": {},
   "source": [
    "## Expected Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba1e07",
   "metadata": {},
   "source": [
    "Only first graph reported in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b16941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting mean expected score vs. trial number for each condition\n",
    "\n",
    "data_to_copy = filtered_data.copy() # all_trials_df, outliers_excluded, or outliers_excluded_part\n",
    "\n",
    "num_scarce = len(data_to_copy.loc[data_to_copy.scarce == 1].groupby('workerId'))\n",
    "num_control = len(data_to_copy.loc[data_to_copy.scarce == 0].groupby('workerId'))\n",
    "\n",
    "# Taking the mean expected score over all trials \n",
    "scarce_data_all = np.array(data_to_copy.loc[data_to_copy.scarce == 1].groupby('trialNumbers').mean()[\"expectedScores\"])\n",
    "scarce_data_all_se = np.array(data_to_copy.loc[data_to_copy.scarce == 1].groupby('trialNumbers').std()[\"expectedScores\"]) / np.sqrt(num_scarce)\n",
    "control_data = np.array(data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers').mean()[\"expectedScores\"])\n",
    "control_data_se = np.array(data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers').std()[\"expectedScores\"]) / np.sqrt(num_scarce)\n",
    "scarce_data_first_30 = scarce_data_all[0:30]\n",
    "scarce_data_first_30_se = scarce_data_all_se[0:30]\n",
    "\n",
    "# Taking only the rewarded trials in the scarce condition\n",
    "rewarded_trials_df = pd.DataFrame(columns=data_to_copy.columns)\n",
    "for worker in np.unique(data_to_copy.workerId):\n",
    "    workerRows = data_to_copy.loc[(data_to_copy.scarce == 1) & (data_to_copy.workerId == worker)].reset_index(drop=True)\n",
    "    rewarded = workerRows.loc[workerRows.rewardsWithheld == False]\n",
    "    rewarded['trialNumbers'] = list(range(1,len(rewarded)+1))\n",
    "    rewarded_trials_df = pd.concat([rewarded_trials_df,rewarded], ignore_index=True)\n",
    "\n",
    "rewarded_trials_df['trialNumbers'] = rewarded_trials_df['trialNumbers'].astype(\"int64\")\n",
    "rewarded_trials_df['scarce'] = rewarded_trials_df['scarce'].astype(\"int64\")\n",
    "rewarded_trials_df['numRewardedTrials'] = rewarded_trials_df['numRewardedTrials'].astype(\"int64\")\n",
    "rewarded_trials_df['numUnrewardedTrials'] = rewarded_trials_df['numUnrewardedTrials'].astype(\"int64\")\n",
    "rewarded_trials_df['rewardsWithheld'] = rewarded_trials_df['rewardsWithheld'].astype(\"boolean\")\n",
    "scarce_data_rewarded = np.array(rewarded_trials_df.groupby('trialNumbers').mean()[\"expectedScores\"])\n",
    "scarce_data_rewarded_sd = np.array(rewarded_trials_df.groupby('trialNumbers').std()[\"expectedScores\"])\n",
    "\n",
    "plt.figure()\n",
    "#plt.title(\"Learning Curves over All Trials\")\n",
    "plt.plot(list(range(len(scarce_data_all))), scarce_data_all, label=\"Scarcity\", color='darkturquoise')\n",
    "plt.fill_between(list(range(len(scarce_data_all))), scarce_data_all + 1.96*scarce_data_all_se, scarce_data_all - 1.96*scarce_data_all_se, alpha=0.15, color='darkturquoise')\n",
    "plt.plot(list(range(len(control_data))), control_data, label=\"Control\", color='firebrick')\n",
    "plt.fill_between(list(range(len(control_data))), control_data + 1.96*control_data_se, control_data - 1.96*control_data_se, alpha=0.15, color='firebrick')\n",
    "plt.axvline(13, linestyle='--', color='darkturquoise')\n",
    "plt.axvline(9, linestyle='--', color='firebrick')\n",
    "#plt.axhline(39.97, color='k', label='Optimal', linestyle=\"--\")\n",
    "plt.legend(loc=\"lower right\",prop={'size': 12})\n",
    "#plt.ylim([10,60])\n",
    "plt.xlim([0, 125])\n",
    "plt.xlabel(\"Trial Number\",fontsize=12)\n",
    "plt.ylabel(\"Average expected trial score\",fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curves over Rewarded Trials\")\n",
    "plt.plot(list(range(len(scarce_data_rewarded))), scarce_data_rewarded, label=\"Scarce\")\n",
    "plt.plot(list(range(len(control_data))), control_data, label=\"Control\")\n",
    "plt.legend()\n",
    "plt.ylim([10,60])\n",
    "plt.xlim([0, 40])\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Average expected score\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curves over First 30 trials\")\n",
    "plt.plot(list(range(len(scarce_data_first_30))), scarce_data_first_30, label=\"Scarce\")\n",
    "plt.plot(list(range(len(control_data))), control_data, label=\"Control\")\n",
    "plt.legend()\n",
    "plt.ylim([10,60])\n",
    "plt.xlim([0, 40])\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Average expected score\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58507ea",
   "metadata": {},
   "source": [
    "## Clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293b2c8",
   "metadata": {},
   "source": [
    "Not reported in publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a7968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting mean expected score vs. trial number for each condition\n",
    "\n",
    "data_to_copy = filtered_data.copy() # all_trials_df, outliers_excluded, or outliers_excluded_part\n",
    "\n",
    "# Taking the mean expected score over all trials \n",
    "scarce_level_all = np.array(data_to_copy.loc[data_to_copy.scarce == 1].groupby('trialNumbers').mean()[\"avgClickLevel\"])\n",
    "scarce_level_all_sd = np.array(data_to_copy.loc[data_to_copy.scarce == 1].groupby('trialNumbers').std()[\"avgClickLevel\"])\n",
    "control_level_all = np.array(data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers').mean()[\"avgClickLevel\"])\n",
    "control_level_sd = np.array(data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers').std()[\"avgClickLevel\"])\n",
    "\n",
    "# Taking the mean expected score over all trials \n",
    "scarce_num_all = np.array(data_to_copy.loc[data_to_copy.scarce == 1].groupby('trialNumbers').mean()[\"numClicks\"])\n",
    "scarce_num_all_sd = np.array(data_to_copy.loc[data_to_copy.scarce == 1].groupby('trialNumbers').std()[\"numClicks\"])\n",
    "control_num_all = np.array(data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers').mean()[\"numClicks\"])\n",
    "control_num_sd = np.array(data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers').std()[\"numClicks\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curves of Number of Clicks\")\n",
    "plt.plot(list(range(len(scarce_num_all))), scarce_num_all, label=\"Scarce\")\n",
    "plt.fill_between(list(range(len(scarce_num_all))), scarce_num_all + scarce_num_all_sd, scarce_num_all - scarce_num_all_sd, alpha=0.1)\n",
    "plt.plot(list(range(len(control_num_all))), control_num_all, label=\"Control\")\n",
    "plt.fill_between(list(range(len(control_num_all))), control_num_all + control_num_sd, control_num_all - control_num_sd, alpha=0.1)\n",
    "plt.axhline(3.56, color='k', label='Optimal', linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.xlim([0, 130])\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Average number of clicks\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curves of Average Click Level\")\n",
    "plt.plot(list(range(len(scarce_level_all))), scarce_level_all, label=\"Scarce\")\n",
    "plt.fill_between(list(range(len(scarce_level_all))), scarce_level_all + scarce_level_all_sd, scarce_level_all - scarce_level_all_sd, alpha=0.1)\n",
    "plt.plot(list(range(len(control_level_all))), control_level_all, label=\"Control\")\n",
    "plt.fill_between(list(range(len(control_level_all))), control_level_all + control_level_sd, control_level_all - control_level_sd, alpha=0.1)\n",
    "plt.axhline(2.968, color='k', label='Optimal', linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.xlim([0, 130])\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Average click level\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d989440f",
   "metadata": {},
   "source": [
    "## Strategy Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f4439",
   "metadata": {},
   "source": [
    "Not reported in publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155216dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting mean expected score vs. trial number for each condition\n",
    "\n",
    "data_to_copy = filtered_data.copy() # all_trials_df, outliers_excluded, or outliers_excluded_part\n",
    "\n",
    "scarce_trial_group = data_to_copy.loc[data_to_copy.scarce == 1].groupby('trialNumbers')\n",
    "control_trial_group = data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers')\n",
    "\n",
    "# Taking the mean expected score over all trials \n",
    "scarce_data_all = np.array(scarce_trial_group.mean()[\"strategyScores\"])\n",
    "scarce_data_all_sd = np.array(scarce_trial_group.std()[\"strategyScores\"])\n",
    "control_data = np.array(control_trial_group.mean()[\"strategyScores\"])\n",
    "control_data_sd = np.array(control_trial_group.std()[\"strategyScores\"])\n",
    "\n",
    "# Getting the proportions of clusters\n",
    "scarce_data_adaptive = np.array(scarce_trial_group[\"cluster\"].apply(lambda x: (x==0).sum()) / scarce_trial_group[\"cluster\"].count())\n",
    "control_data_adaptive = np.array(control_trial_group[\"cluster\"].apply(lambda x: (x==0).sum()) / control_trial_group[\"cluster\"].count())\n",
    "scarce_data_modadaptive = np.array(scarce_trial_group[\"cluster\"].apply(lambda x: (x==1).sum()) / scarce_trial_group[\"cluster\"].count())\n",
    "control_data_modadaptive = np.array(control_trial_group[\"cluster\"].apply(lambda x: (x==1).sum()) / control_trial_group[\"cluster\"].count())\n",
    "scarce_data_maladaptive = np.array(scarce_trial_group[\"cluster\"].apply(lambda x: (x==2).sum()) / scarce_trial_group[\"cluster\"].count())\n",
    "control_data_maladaptive = np.array(control_trial_group[\"cluster\"].apply(lambda x: (x==2).sum()) / control_trial_group[\"cluster\"].count())\n",
    "#control_data = np.array(data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers').mean()[\"strategyScores\"])\n",
    "#control_data_sd = np.array(data_to_copy.loc[data_to_copy.scarce == 0].groupby('trialNumbers').std()[\"strategyScores\"])\n",
    "\n",
    "# Taking only the rewarded trials in the scarce condition\n",
    "rewarded_trials_df = pd.DataFrame(columns=data_to_copy.columns)\n",
    "for worker in np.unique(data_to_copy.workerId):\n",
    "    workerRows = data_to_copy.loc[(data_to_copy.scarce == 1) & (data_to_copy.workerId == worker)].reset_index(drop=True)\n",
    "    rewarded = workerRows.loc[workerRows.rewardsWithheld == False]\n",
    "    rewarded['trialNumbers'] = list(range(1,len(rewarded)+1))\n",
    "    rewarded_trials_df = pd.concat([rewarded_trials_df,rewarded], ignore_index=True)\n",
    "\n",
    "rewarded_trials_df['trialNumbers'] = rewarded_trials_df['trialNumbers'].astype(\"int64\")\n",
    "rewarded_trials_df['scarce'] = rewarded_trials_df['scarce'].astype(\"int64\")\n",
    "rewarded_trials_df['numRewardedTrials'] = rewarded_trials_df['numRewardedTrials'].astype(\"int64\")\n",
    "rewarded_trials_df['numUnrewardedTrials'] = rewarded_trials_df['numUnrewardedTrials'].astype(\"int64\")\n",
    "rewarded_trials_df['rewardsWithheld'] = rewarded_trials_df['rewardsWithheld'].astype(\"boolean\")\n",
    "scarce_data_rewarded = np.array(rewarded_trials_df.groupby('trialNumbers').mean()[\"strategyScores\"])\n",
    "scarce_data_rewarded_sd = np.array(rewarded_trials_df.groupby('trialNumbers').std()[\"strategyScores\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Strategy score Learning Curves over All Trials\")\n",
    "plt.plot(list(range(len(scarce_data_all))), scarce_data_all, label=\"Scarce\")\n",
    "plt.fill_between(list(range(len(scarce_data_all))), scarce_data_all + scarce_data_all_sd, scarce_data_all - scarce_data_all_sd, alpha=0.1)\n",
    "plt.plot(list(range(len(control_data))), control_data, label=\"Control\")\n",
    "plt.fill_between(list(range(len(control_data))), control_data + control_data_sd, control_data - control_data_sd, alpha=0.1)\n",
    "plt.axhline(np.max(np.array(strategy_scores_control_list)[:,1], axis=0), color='k', label='Optimal', linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.xlim([0, 130])\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Average strategy score\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curves over Rewarded Trials\")\n",
    "plt.plot(list(range(len(scarce_data_rewarded))), scarce_data_rewarded, label=\"Scarce\")\n",
    "plt.plot(list(range(len(control_data))), control_data, label=\"Control\")\n",
    "plt.legend()\n",
    "plt.xlim([0, 40])\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Average strategy score\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Strategy Type Change\")\n",
    "alpha = 0.7\n",
    "plt.plot(list(range(len(scarce_data_all))), scarce_data_adaptive, label=\"Scarce Adaptive\", color='b',alpha=alpha)\n",
    "plt.plot(list(range(len(control_data))), control_data_adaptive, label=\"Control Adaptive\",color='orange',alpha=alpha)\n",
    "plt.plot(list(range(len(scarce_data_all))), scarce_data_modadaptive, label=\"Scarce Mod-Adaptive\", color='b',alpha=alpha, linestyle=\"--\")\n",
    "plt.plot(list(range(len(control_data))), control_data_modadaptive, label=\"Control Mod-Adaptive\",color='orange',alpha=alpha, linestyle=\"--\")\n",
    "plt.plot(list(range(len(scarce_data_all))), scarce_data_maladaptive, label=\"Scarce Maldaptive\", color='b',alpha=alpha, linestyle=\":\")\n",
    "plt.plot(list(range(len(control_data))), control_data_maladaptive, label=\"Control Maldaptive\",color='orange',alpha=alpha, linestyle=\":\")\n",
    "plt.legend()\n",
    "plt.ylim([0,1])\n",
    "plt.xlim([0, 130])\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Proportion of Trials\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255fe6f",
   "metadata": {},
   "source": [
    "# 0.1 - Transforming Response Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d95e4",
   "metadata": {},
   "source": [
    "Not used for results reported in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3753f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_to_range(arr, lb, ub):\n",
    "    return ((arr - arr.min()) / (arr.max() - arr.min())) * (ub - lb) + lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc28fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the response variable\n",
    "\n",
    "response_vars = ['expectedScores', 'avgClickLevel', 'strategyScores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9025202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the data\n",
    "\n",
    "data_to_copy = filtered_data.copy() # or outliers_excluded\n",
    "transformed_trials_df = data_to_copy.copy()\n",
    "trans_list = {res: [] for res in response_vars}\n",
    "\n",
    "for response_var in response_vars:\n",
    "    \n",
    "\n",
    "    log_transformed_trials_df = data_to_copy.copy()\n",
    "    sq_transformed_trials_df = data_to_copy.copy()\n",
    "    sq2_transformed_trials_df = data_to_copy.copy()\n",
    "    cb_transformed_trials_df = data_to_copy.copy()\n",
    "    cb2_transformed_trials_df = data_to_copy.copy()\n",
    "    cb3_transformed_trials_df = data_to_copy.copy()\n",
    "    arcsin_transformed_trials_df = data_to_copy.copy()\n",
    "    bc_transformed_trials_df = data_to_copy.copy()\n",
    "    yj_transformed_trials_df = data_to_copy.copy()\n",
    "    inverse_transformed_trials_df = data_to_copy.copy()\n",
    "\n",
    "    # Compute separate dataframes for each transformation\n",
    "    log_transformed_trials_df[response_var] = np.log(transformed_trials_df[response_var].max() + 1 - transformed_trials_df[response_var])\n",
    "    sq_transformed_trials_df[response_var] = np.sqrt(transformed_trials_df[response_var].max() + 1 - transformed_trials_df[response_var])\n",
    "    sq2_transformed_trials_df[response_var] = np.sqrt(-transformed_trials_df[response_var].min() + 1 + transformed_trials_df[response_var])\n",
    "    cb_transformed_trials_df[response_var] = np.cbrt(transformed_trials_df[response_var])\n",
    "    cb2_transformed_trials_df[response_var] = np.cbrt(transformed_trials_df[response_var].max() + 1 - transformed_trials_df[response_var])\n",
    "    cb3_transformed_trials_df[response_var] = np.cbrt(-transformed_trials_df[response_var].min() + 1 + transformed_trials_df[response_var])\n",
    "    arcsin_transformed_trials_df[response_var] = np.arcsin(np.sqrt(scale_to_range(transformed_trials_df[response_var], 0, 1)))\n",
    "    bc_transformed_trials_df[response_var],lam = scipy.stats.boxcox(transformed_trials_df[response_var].max() + 1 - transformed_trials_df[response_var])\n",
    "    #logit_transformed_trials_df[response_var] = scipy.special.logit(scale_to_range(transformed_trials_df[response_var], 0, 1))\n",
    "    yj_transformed_trials_df[response_var],lam_y = scipy.stats.yeojohnson(transformed_trials_df[response_var])\n",
    "    inverse_transformed_trials_df[response_var] = 1/(transformed_trials_df[response_var].max() + 1 - transformed_trials_df[response_var])\n",
    "\n",
    "    # All the transformed data and the corresponding transformation\n",
    "    trans_list[response_var] = [\n",
    "        (transformed_trials_df, \"Original data\"),\n",
    "        (log_transformed_trials_df, \"Log(max(Y) - Y + 1)\"),\n",
    "        (sq_transformed_trials_df, \"Sqrt(max(Y) - Y + 1)\"),\n",
    "        (sq2_transformed_trials_df, \"Sqrt(-min(Y) + Y + 1)\"),\n",
    "        (cb_transformed_trials_df, \"Cbrt(Y)\"),\n",
    "        (bc_transformed_trials_df, \"Box-Cox(max(Y) - Y + 1), l={0:0.4f}\".format(lam)),\n",
    "        (cb2_transformed_trials_df, \"Cbrt(max(Y) - Y + 1)\"),\n",
    "        (cb3_transformed_trials_df, \"Cbrt(-min(Y) + Y + 1)\"),\n",
    "        (arcsin_transformed_trials_df, \"Arcsin(sqrt(Y))\"),\n",
    "        (yj_transformed_trials_df, \"Yeo-Johnson(Y), l={0:0.4f}\".format(lam_y)),\n",
    "        (inverse_transformed_trials_df, \"1/(max(Y) - Y + 1)\")\n",
    "    ]\n",
    "    \n",
    "    # Plot all the transformed data\n",
    "    fig, ax = plt.subplots(nrows=math.ceil(len(trans_list[response_var])/2),ncols=2,figsize=(10, 3 * math.ceil(len(trans_list[response_var])/2)),squeeze=False)\n",
    "    plt.suptitle(\"Distributions of Transformed Data - {}\".format(response_var))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for idx, trans in enumerate(trans_list[response_var]):\n",
    "        data = trans[0][response_var]\n",
    "        label = trans[1]\n",
    "        skew = scipy.stats.skew(data)\n",
    "        density = scipy.stats.gaussian_kde(data)\n",
    "        kurtosis = scipy.stats.kurtosis(data)\n",
    "        n, x, _ = ax[idx].hist(data, density=True)\n",
    "        ax[idx].set_title(label)\n",
    "        ax[idx].plot(x, density(x))\n",
    "        ax[idx].text(0.05,0.8, \"S: {0:0.4f}\\nK: {1:0.4f}\".format(skew, kurtosis), transform=ax[idx].transAxes)\n",
    "\n",
    "    plt.subplots_adjust(\n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27d4fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the response data\n",
    "\n",
    "for response_var in response_vars:\n",
    "    fig, ax = plt.subplots(nrows=math.ceil(len(trans_list[response_var])/2),ncols=2,figsize=(10, 3 * math.ceil(len(trans_list[response_var])/2)),squeeze=False)\n",
    "    plt.suptitle(\"Transformed Response Data - {}\".format(response_var))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for idx, trans in enumerate(trans_list[response_var]):\n",
    "        data = trans[0]\n",
    "        scarce = data.loc[data.scarce == 1]\n",
    "        control = data.loc[data.scarce == 0]\n",
    "        label = trans[1]\n",
    "\n",
    "        scarce_mean = np.array(scarce.groupby('trialNumbers').mean()[response_var])\n",
    "        control_mean = np.array(control.groupby('trialNumbers').mean()[response_var])\n",
    "        ax[idx].plot(list(range(len(scarce_mean))), scarce_mean, label=\"Scarce\")\n",
    "        ax[idx].plot(list(range(len(control_mean))), control_mean, label=\"Control\")\n",
    "        ax[idx].set_title(label)\n",
    "\n",
    "    plt.subplots_adjust(\n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbfc691",
   "metadata": {},
   "source": [
    "## Linear Regression: Expected Score vs. Trial Number x Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4bcc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formula = \"expectedScores ~ trialNumbers + C(scarce) + trialNumbers:C(scarce)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691e807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GLM Model 1 - Expected score vs. Trial number & Condition - all possible transformations\n",
    "\n",
    "# Creating the list of data to subplot all residuals\n",
    "\n",
    "all_residuals = {res: [] for res in response_vars}\n",
    "mixed = True\n",
    "\n",
    "for response_var in response_vars:\n",
    "    formula = '{} ~ C(trialNumbers) + C(scarce)'.format(response_var);\n",
    "\n",
    "    # Perform regression on each transformed dataset\n",
    "    for idx, trans in enumerate(trans_list[response_var]):\n",
    "        if mixed:\n",
    "            glm = smf.mixedlm(formula=formula, data=trans[0], groups=trans[0]['workerId'])\n",
    "        else:\n",
    "            glm = smf.glm(formula=formula, data=trans[0])\n",
    "\n",
    "        results = glm.fit()\n",
    "\n",
    "        print(\"\\n{} - {}:\".format(response_var, trans[1]))\n",
    "        print(results.summary())\n",
    "\n",
    "        resids = results.resid_response if not mixed else results.resid\n",
    "        normaltest = scipy.stats.normaltest(resids)\n",
    "        \n",
    "        # Save the results of the regression for plotting\n",
    "        all_residuals[response_var].append((\n",
    "            resids,\n",
    "            trans[0][response_var],\n",
    "            trans[1],\n",
    "            normaltest\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a66ed63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting residuals for all GLM Models\n",
    "\n",
    "analysis_cols = {res: [] for res in response_vars}\n",
    "highest_normality_trans = {res: \"\" for res in response_vars}\n",
    "\n",
    "for response_var in response_vars:\n",
    "    fig, ax = plt.subplots(nrows=math.ceil(len(all_residuals[response_var])/2),ncols=2,figsize=(10, 3 * math.ceil(len(all_residuals[response_var])/2)),squeeze=False)\n",
    "    plt.suptitle(\"Residuals of Transformed Data - {}\".format(response_var))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    lowest_normality_stat = float(\"inf\")\n",
    "    for idx, trans in enumerate(all_residuals[response_var]):\n",
    "        resids = trans[0]\n",
    "        response_data = trans[1]\n",
    "        label = trans[2]\n",
    "        normaltest = trans[3]\n",
    "        if normaltest.statistic < lowest_normality_stat:\n",
    "            analysis_cols[response_var] = response_data\n",
    "            highest_normality_trans[response_var] = label\n",
    "            lowest_normality_stat = normaltest.statistic\n",
    "        density = scipy.stats.gaussian_kde(resids)\n",
    "        n, x, _ = ax[idx].hist(resids, density=True)\n",
    "        ax[idx].set_title(label)\n",
    "        ax[idx].plot(x, density(x))\n",
    "        ax[idx].text(0.05,0.8, \"S: {0:0.4f}\\nP: {1:0.4f}\".format(normaltest.statistic, normaltest.pvalue), transform=ax[idx].transAxes)\n",
    "\n",
    "    plt.subplots_adjust(\n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b13dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data which is to be used for the remainder of the analysis\n",
    "\n",
    "# Displaying which of the data has the most normal residuals \n",
    "analysis_data = transformed_trials_df.copy()\n",
    "for response_var in response_vars:\n",
    "    print(\"{} - {}\".format(response_var, highest_normality_trans[response_var]))\n",
    "    analysis_data[response_var] = analysis_cols[response_var]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f8b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transformed data for each response variable\n",
    "\n",
    "for response_var in response_vars:\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(10, 3),squeeze=True)\n",
    "    plt.suptitle(\"Transformed Response Data - {}\".format(response_var),y=1.1)\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for idx, trans in enumerate([transformed_trials_df, analysis_data]):\n",
    "        data = trans\n",
    "        scarce = data.loc[data.scarce == 1]\n",
    "        control = data.loc[data.scarce == 0]\n",
    "        label = \"Original\" if idx == 0 else highest_normality_trans[response_var]\n",
    "\n",
    "        scarce_mean = np.array(scarce.groupby('trialNumbers').mean()[response_var])\n",
    "        control_mean = np.array(control.groupby('trialNumbers').mean()[response_var])\n",
    "        ax[idx].plot(list(range(len(scarce_mean))), scarce_mean, label=\"Scarce\")\n",
    "        ax[idx].plot(list(range(len(control_mean))), control_mean, label=\"Control\")\n",
    "        ax[idx].set_title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fe7392",
   "metadata": {},
   "source": [
    "# 1.1.1 - Linear Regression: Finding Best Model of Learning vs. Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1587a3b",
   "metadata": {},
   "source": [
    "Used to compute partitions between fast learning phase and slow improvement phase for each condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e809b4",
   "metadata": {},
   "source": [
    "### Finding Best Model for Scarce Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c21155",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_df = analysis_data\n",
    "\n",
    "mixed = False\n",
    "    \n",
    "# For scarce trials\n",
    "relevant_trials = test_df.loc[test_df.scarce == 1].copy()\n",
    "\n",
    "all_trials_mean_scarce = relevant_trials.groupby(\"trialNumbers\").mean()[\"expectedScores\"]\n",
    "all_trial_nos_scarce = list(range(1,len(all_trials_mean_scarce)+1))\n",
    "\n",
    "sig_bic_scores_scarce = []\n",
    "scarce_plot_data = []\n",
    "scarce_glm_results = []\n",
    "\n",
    "# Check all possible data splits for both conditions\n",
    "for trialNum in range(2,60,1):\n",
    "    isPerformanceTrial = [int(trial > trialNum) for trial in relevant_trials.trialNumbers]\n",
    "    relevant_trials['isPerformanceTrial'] = isPerformanceTrial\n",
    "    \n",
    "    partition_1_df = relevant_trials.loc[relevant_trials.isPerformanceTrial == 0]\n",
    "    partition_2_df = relevant_trials.loc[relevant_trials.isPerformanceTrial == 1]\n",
    "    partition_1_mean_scores = partition_1_df.groupby(\"trialNumbers\").mean()[\"expectedScores\"]\n",
    "    partition_2_mean_scores = partition_2_df.groupby(\"trialNumbers\").mean()[\"expectedScores\"]\n",
    "    \n",
    "    two_slope_formula = \"expectedScores ~ trialNumbers + trialNumbers:C(isPerformanceTrial) + C(isPerformanceTrial)\"\n",
    "    one_slope_formula = \"expectedScores ~ trialNumbers\"\n",
    "    \n",
    "    # Fitting a single-slope model and a two-slope model to see which is better\n",
    "    if mixed:\n",
    "        glm_1 = smf.mixedlm(formula=two_slope_formula, data=relevant_trials, groups=relevant_trials['workerId'])\n",
    "        glm_2 = smf.mixedlm(formula=one_slope_formula, data=relevant_trials, groups=relevant_trials['workerId'])\n",
    "    else:\n",
    "        glm_1 = smf.glm(formula=two_slope_formula, data=relevant_trials)\n",
    "        glm_2 = smf.glm(formula=one_slope_formula, data=relevant_trials)\n",
    "\n",
    "    try:\n",
    "        results_1 = glm_1.fit()\n",
    "        results_2 = glm_2.fit()\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    bic1 = results_1.bic_llf\n",
    "    bic2 = results_2.bic_llf\n",
    "        \n",
    "    # Saving the slopes and the single intercept of both phases from the two-slope model\n",
    "    b1 = results_1.params['trialNumbers']\n",
    "    b2 = results_1.params['trialNumbers'] + results_1.params['trialNumbers:C(isPerformanceTrial)[T.1]']\n",
    "    i1 = results_1.params['Intercept']\n",
    "    i2 = results_1.params['Intercept'] + results_1.params['C(isPerformanceTrial)[T.1]']\n",
    "    \n",
    "    b3 = results_2.params['trialNumbers']\n",
    "    i3 = results_2.params['Intercept']\n",
    "    \n",
    "    if(bic1 < bic2):\n",
    "        # Save data about significant partitions, where the bic of the two-slope model is higher than one-slope\n",
    "        sig_bic_scores_scarce.append((trialNum, bic1))\n",
    "        \n",
    "    x1 = all_trial_nos_scarce[0:len(partition_1_mean_scores)]\n",
    "    x2 = all_trial_nos_scarce[len(partition_1_mean_scores):len(partition_1_mean_scores)+len(partition_2_mean_scores)]\n",
    "    scarce_plot_data.append((\n",
    "        trialNum,\n",
    "        x1,\n",
    "        x2,\n",
    "        partition_1_mean_scores,\n",
    "        partition_2_mean_scores,\n",
    "        b1,\n",
    "        b2,\n",
    "        i1,\n",
    "        i2,\n",
    "        b3,\n",
    "        i3,\n",
    "        partition_1_df,\n",
    "        partition_2_df\n",
    "    ))\n",
    "    scarce_glm_results.append((trialNum, results_1, results_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe0ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the best partition of all significant ones \n",
    "if use_model_data:\n",
    "    partition_candidates = [13] # Use partition from human dataset\n",
    "else:\n",
    "    sig_bic_scores_scarce = np.array(sig_bic_scores_scarce)\n",
    "\n",
    "    g_range = np.arange(0, 1, 0.0001)\n",
    "    best_partitions = []\n",
    "\n",
    "    for gamma in g_range:\n",
    "        # Minimum bic\n",
    "        partition_scores = sig_bic_scores_scarce[:,1]\n",
    "        best_partition = sig_bic_scores_scarce[np.argmin(partition_scores),0]\n",
    "        best_partitions.append(best_partition)\n",
    "\n",
    "    partition_candidates = np.unique(best_partitions)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Partition candidates: \")\n",
    "    print(partition_candidates)\n",
    "    plt.plot(g_range, best_partitions)\n",
    "    plt.xlabel(\"Gamma (Decay Factor)\")\n",
    "    plt.ylabel(\"Best Partition\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6564b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing GLM results - scarce\n",
    "candidate_glm_results_scarce = [data for data in scarce_glm_results if float(data[0]) in partition_candidates]\n",
    "\n",
    "for trialNum, results_1, results_2 in candidate_glm_results_scarce:\n",
    "    print(\"Trial Num: {}\".format(trialNum))\n",
    "    print(\"\\nTwo-Slope Model:\\nBIC LLF: {0:0.4f}\\n\\nBIC Deviance: {1:0.4f}\\n\\nAIC: {2:0.4f}\\n\"\n",
    "          .format(results_1.bic_llf,results_1.bic_deviance, results_1.aic))\n",
    "    print(results_1.summary())\n",
    "    \n",
    "    print(\"\\nOne-Slope Model:\\nBIC LLF: {0:0.4f}\\n\\nBIC Deviance: {1:0.4f}\\n\\nAIC: {2:0.4f}\\n\"\n",
    "          .format(results_2.bic_llf,results_2.bic_deviance, results_2.aic))\n",
    "    print(results_2.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802a651b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting candidate partitions - scarce\n",
    "candidate_plot_data_scarce = [data for data in scarce_plot_data if float(data[0]) in partition_candidates]\n",
    "#significant_plot_data_scarce = [data for data in scarce_plot_data if float(data[0]) in sig_bic_scores_scarce[:,0]]\n",
    "\n",
    "which_plot = \"candidates\" #all, significant, candidates\n",
    "\n",
    "if which_plot == \"candidates\":\n",
    "    plot_data = candidate_plot_data_scarce\n",
    "elif which_plot == \"significant\":\n",
    "    plot_data = significant_plot_data_scarce\n",
    "else:\n",
    "    plot_data = scarce_plot_data\n",
    "    \n",
    "plot_single_slope = False\n",
    "plot_two_slope = True\n",
    "scatter = False\n",
    "error = False\n",
    "\n",
    "for trialNum, x1, x2, y1, y2, b1, b2, i1, i2,b3, i3, df_1, df_2 in plot_data:\n",
    "    plt.figure()\n",
    "    # Expected Score curves\n",
    "    plt.plot(all_trial_nos_scarce, all_trials_mean_scarce, alpha=0.2, color='k')\n",
    "    plt.plot(x1, y1, alpha=0.5)\n",
    "    plt.plot(x2, y2, alpha=0.5)\n",
    "    \n",
    "    # Scatter points\n",
    "    if scatter:\n",
    "        plt.scatter(df_1['trialNumbers'], df_1['expectedScores'], color='b', marker='o', alpha=0.1,s=0.8)\n",
    "        plt.scatter(df_2['trialNumbers'], df_2['expectedScores'], color='orange', marker='o', alpha=0.1,s=0.8)\n",
    "\n",
    "    # Partition line\n",
    "    plt.axvline(trialNum, linestyle=\"--\", color='k',alpha=0.5)\n",
    "\n",
    "    # Slope lines of one-slope model\n",
    "    if plot_single_slope:\n",
    "        x3 = np.concatenate([x1,x2],axis=0)\n",
    "        plt.plot(x3, np.array(x3) * b3 + i3, color='g', alpha=0.5)\n",
    "    \n",
    "    if plot_two_slope:\n",
    "        # Slope lines of two-slope model\n",
    "        plt.plot(x1, np.array(x1) * b1 + i1, color='b')\n",
    "        plt.plot(x2, np.array(x2) * b2 + i2, color='orange')\n",
    "        \n",
    "    if error:\n",
    "        plt.fill_between(x1, scarce_data_all + scarce_data_all_sd, scarce_data_all - scarce_data_all_sd, alpha=0.1)\n",
    "    \n",
    "    plt.title(\"Trial: {}\".format(trialNum))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12357479",
   "metadata": {},
   "outputs": [],
   "source": [
    "scarce_partition = partition_candidates[0] # Somehow choose based on possible candidates\n",
    "\n",
    "# Separating into learning and performance phase\n",
    "scarce_learning_data = analysis_data.loc[(analysis_data.scarce == 1) & (analysis_data.trialNumbers <= scarce_partition)]\n",
    "scarce_performance_data = analysis_data.loc[(analysis_data.scarce ==1) & (analysis_data.trialNumbers > scarce_partition)]\n",
    "\n",
    "# Without transformation\n",
    "scarce_learning_data_orig = filtered_data.loc[(filtered_data.scarce == 1) & (filtered_data.trialNumbers <= scarce_partition)]\n",
    "scarce_performance_data_orig = filtered_data.loc[(filtered_data.scarce ==1) & (filtered_data.trialNumbers > scarce_partition)]\n",
    "\n",
    "# Store the slope, intercept, etc. of the best partition\n",
    "scarce_partition_plot_data = [data for data in candidate_plot_data_scarce if data[0] == scarce_partition][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446adf6",
   "metadata": {},
   "source": [
    "### Finding Best Model for Control Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3aff6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GLM 3 - finding cut off point of learning phase\n",
    "\n",
    "test_df = analysis_data\n",
    "\n",
    "mixed = False\n",
    "    \n",
    "# For control trials\n",
    "relevant_trials = test_df.loc[test_df.scarce == 0].copy()\n",
    "\n",
    "all_trials_mean_control = relevant_trials.groupby(\"trialNumbers\").mean()[\"expectedScores\"]\n",
    "all_trial_nos_control = list(range(1,len(all_trials_mean_control)+1))\n",
    "\n",
    "sig_bic_scores_control = []\n",
    "control_plot_data = []\n",
    "control_glm_results = []\n",
    "for trialNum in range(2,30,1):\n",
    "    isPerformanceTrial = [int(trial > trialNum) for trial in relevant_trials.trialNumbers]\n",
    "    relevant_trials['isPerformanceTrial'] = isPerformanceTrial\n",
    "    \n",
    "    partition_1_df = relevant_trials.loc[relevant_trials.isPerformanceTrial == 0]\n",
    "    partition_2_df = relevant_trials.loc[relevant_trials.isPerformanceTrial == 1]\n",
    "    partition_1_mean_scores = partition_1_df.groupby(\"trialNumbers\").mean()[\"expectedScores\"]\n",
    "    partition_2_mean_scores = partition_2_df.groupby(\"trialNumbers\").mean()[\"expectedScores\"]\n",
    "    \n",
    "    two_slope_formula = \"expectedScores ~ trialNumbers + C(scarce) + C(isPerformanceTrial) + trialNumbers:C(isPerformanceTrial)\"\n",
    "    one_slope_formula = \"expectedScores ~ trialNumbers + C(scarce)\"\n",
    "    \n",
    "    # Fitting a single-slope model and a two-slope model to see which is better\n",
    "    if mixed:\n",
    "        glm_1 = smf.mixedlm(formula=two_slope_formula, data=relevant_trials, groups=relevant_trials['workerId'])\n",
    "        glm_2 = smf.mixedlm(formula=one_slope_formula, data=relevant_trials, groups=relevant_trials['workerId'])\n",
    "    else:\n",
    "        glm_1 = smf.glm(formula=two_slope_formula, data=relevant_trials)\n",
    "        glm_2 = smf.glm(formula=one_slope_formula, data=relevant_trials)\n",
    "\n",
    "    try:\n",
    "        results_1 = glm_1.fit()\n",
    "        results_2 = glm_2.fit()\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    bic1 = results_1.bic_llf\n",
    "    bic2 = results_2.bic_llf\n",
    "    \n",
    "    # print(results_1.summary())\n",
    "    \n",
    "    # Saving the slopes and the single intercept of both phases from the two-slope model\n",
    "    b1 = results_1.params['trialNumbers']\n",
    "    b2 = results_1.params['trialNumbers'] + results_1.params['trialNumbers:C(isPerformanceTrial)[T.1]']\n",
    "    i1 = results_1.params['Intercept']\n",
    "    i2 = results_1.params['Intercept'] + results_1.params['C(isPerformanceTrial)[T.1]']\n",
    "    \n",
    "    b3 = results_2.params['trialNumbers']\n",
    "    i3 = results_2.params['Intercept']\n",
    "    if(bic1 < bic2):\n",
    "        # Save data about significant partitions, where the bic of the two-slope model is higher than one-slope\n",
    "        sig_bic_scores_control.append((trialNum, bic1))\n",
    "        \n",
    "    x1 = all_trial_nos_control[0:len(partition_1_mean_scores)]\n",
    "    x2 = all_trial_nos_control[len(partition_1_mean_scores):len(partition_1_mean_scores)+len(partition_2_mean_scores)]\n",
    "    control_plot_data.append((\n",
    "        trialNum,\n",
    "        x1,\n",
    "        x2,\n",
    "        partition_1_mean_scores,\n",
    "        partition_2_mean_scores,\n",
    "        b1,\n",
    "        b2,\n",
    "        i1,\n",
    "        i2,\n",
    "        b3, \n",
    "        i3,\n",
    "        partition_1_df,\n",
    "        partition_2_df\n",
    "    ))\n",
    "    control_glm_results.append((trialNum, results_1, results_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdefd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_bic_scores_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad78a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the best partition of all significant ones \n",
    "\n",
    "if use_model_data:\n",
    "    partition_candidates = [9] # Use partition from human data\n",
    "else:\n",
    "    sig_bic_scores_control = np.array(sig_bic_scores_control)\n",
    "\n",
    "    g_range = np.arange(0, 1, 0.0001)\n",
    "    best_partitions = []\n",
    "\n",
    "    for gamma in g_range:\n",
    "        # Minimum bic\n",
    "        partition_scores = sig_bic_scores_control[:,1]\n",
    "        best_partition = sig_bic_scores_control[np.argmin(partition_scores),0]\n",
    "        best_partitions.append(best_partition)\n",
    "\n",
    "    partition_candidates = np.unique(best_partitions)\n",
    "    print(\"Partition candidates: \")\n",
    "    print(partition_candidates)\n",
    "    plt.plot(g_range, best_partitions)\n",
    "    plt.xlabel(\"Gamma (Decay Factor)\")\n",
    "    plt.ylabel(\"Best Partition\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing GLM results - control\n",
    "candidate_glm_results_control = [data for data in control_glm_results if float(data[0]) in partition_candidates]\n",
    "\n",
    "for trialNum, results_1, results_2 in candidate_glm_results_control:\n",
    "    print(\"Trial Num: {}\".format(trialNum))\n",
    "    print(\"\\nTwo-Slope Model:\\nBIC LLF: {0:0.4f}\\n\\nBIC Deviance: {1:0.4f}\\n\\nAIC: {2:0.4f}\\n\"\n",
    "          .format(results_1.bic_llf,results_1.bic_deviance, results_1.aic))\n",
    "    print(results_1.summary())\n",
    "    \n",
    "    print(\"\\nOne-Slope Model:\\nBIC LLF: {0:0.4f}\\n\\nBIC Deviance: {1:0.4f}\\n\\nAIC: {2:0.4f}\\n\"\n",
    "          .format(results_2.bic_llf,results_2.bic_deviance, results_2.aic))\n",
    "    print(results_2.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d2b578",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting candidate partitions - control\n",
    "candidate_plot_data_control = [data for data in control_plot_data if float(data[0]) in partition_candidates]\n",
    "#significant_plot_data_control = [data for data in control_plot_data if float(data[0]) in sig_bic_scores_control[:,0]]\n",
    "\n",
    "which_plot = \"candidates\" #all, significant, candidates\n",
    "\n",
    "if which_plot == \"candidates\":\n",
    "    plot_data = candidate_plot_data_control\n",
    "elif which_plot == \"significant\":\n",
    "    plot_data = significant_plot_data_control\n",
    "else:\n",
    "    plot_data = control_plot_data\n",
    "    \n",
    "plot_single_slope = False\n",
    "plot_two_slope = True\n",
    "scatter = False\n",
    "\n",
    "for trialNum, x1, x2, y1, y2, b1, b2, i1, i2,b3, i3, df_1, df_2 in plot_data:\n",
    "    plt.figure()\n",
    "    # Expected Score curves\n",
    "    plt.plot(all_trial_nos_control, all_trials_mean_control, alpha=0.2, color='k')\n",
    "    plt.plot(x1, y1, alpha=0.5)\n",
    "    plt.plot(x2, y2, alpha=0.5)\n",
    "\n",
    "    # Partition line\n",
    "    plt.axvline(trialNum, linestyle=\"--\", color='k',alpha=0.5)\n",
    "    \n",
    "    # Scatter points\n",
    "    if scatter:\n",
    "        plt.scatter(df_1['trialNumbers'], df_1['expectedScores'], color='b', marker='o', alpha=0.1,s=0.8)\n",
    "        plt.scatter(df_2['trialNumbers'], df_2['expectedScores'], color='orange', marker='o', alpha=0.1,s=0.8)\n",
    "\n",
    "    # Slope lines of one-slope model\n",
    "    if plot_single_slope:\n",
    "        x3 = np.concatenate([x1,x2],axis=0)\n",
    "        plt.plot(x3, np.array(x3) * b3 + i3, color='g', alpha=0.5)\n",
    "    \n",
    "    if plot_two_slope:\n",
    "        # Slope lines of two-slope model\n",
    "        plt.plot(x1, np.array(x1) * b1 + i1, color='b')\n",
    "        plt.plot(x2, np.array(x2) * b2 + i2, color='orange')\n",
    "    \n",
    "    plt.title(\"Trial: {}\".format(trialNum))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b81b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_partition = partition_candidates[0] # Somehow choose based on possible candidates\n",
    "\n",
    "# Separating into learning and performance phase\n",
    "control_learning_data = analysis_data.loc[(analysis_data.scarce == 0) & (analysis_data.trialNumbers <= control_partition)]\n",
    "control_performance_data = analysis_data.loc[(analysis_data.scarce == 0) & (analysis_data.trialNumbers > control_partition)]\n",
    "\n",
    "# Without transformation\n",
    "control_learning_data_orig = filtered_data.loc[(filtered_data.scarce == 0) & (filtered_data.trialNumbers <= control_partition)]\n",
    "control_performance_data_orig = filtered_data.loc[(filtered_data.scarce == 0) & (filtered_data.trialNumbers > control_partition)]\n",
    "\n",
    "# Store the slope, intercept, etc. of the best partition\n",
    "control_partition_plot_data = [data for data in candidate_plot_data_control if data[0] == control_partition][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254b9c37",
   "metadata": {},
   "source": [
    "# 1.2 - Linear Regression: Response Var vs. Trial Number x Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fb5066",
   "metadata": {},
   "source": [
    "Results for response variable \"expectedScores\" reported in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a54e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the response variable for this analysis\n",
    "\n",
    "response_var = \"expectedScores\"\n",
    "assert (response_var in response_vars), \"{} is not a valid response variable\".format(response_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag to compare entire performance data or only upto 30 trials, where both conditions are comparable\n",
    "\n",
    "compare_all = True\n",
    "num_max_trials = min(\n",
    "    analysis_data[analysis_data.scarce==1].trialNumbers.max(),\n",
    "    analysis_data[analysis_data.scarce==0].trialNumbers.max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a310a56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Learning Phase\n",
    "\n",
    "use_orig = True\n",
    "\n",
    "if use_orig:\n",
    "    learning_df = pd.concat([scarce_learning_data_orig, control_learning_data_orig]).reset_index(drop=True)\n",
    "else: \n",
    "    learning_df = pd.concat([scarce_learning_data, control_learning_data]).reset_index(drop=True)\n",
    "\n",
    "# Compare only the first 30 trials\n",
    "if not compare_all:\n",
    "    learning_df = learning_df.loc[learning_df.trialNumbers <= num_max_trials]\n",
    "\n",
    "formula = '{} ~ trialNumbers + C(scarce) + trialNumbers:C(scarce)'.format(response_var)\n",
    "\n",
    "mixed = True\n",
    "\n",
    "if mixed:\n",
    "    glm = smf.mixedlm(formula=formula, data=learning_df, groups=learning_df['workerId'])\n",
    "else:\n",
    "    glm = smf.glm(formula=formula, data=learning_df)\n",
    "\n",
    "results = glm.fit()\n",
    "\n",
    "print(results.summary())\n",
    "resids = results.resid_response if not mixed else results.resid\n",
    "normaltest = scipy.stats.normaltest(resids)\n",
    "plt.hist(resids, density=True)\n",
    "plt.title(\"Normal Test: {0:0.4f}, p: {1:0.4f}\".format(normaltest[0], normaltest[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data of both learning phases\n",
    "\n",
    "x1 = control_partition_plot_data[1]\n",
    "x2 = scarce_partition_plot_data[1]\n",
    "y1 = learning_df[learning_df.scarce==0].groupby(\"trialNumbers\").mean()[response_var]\n",
    "y2 = learning_df[learning_df.scarce==1].groupby(\"trialNumbers\").mean()[response_var]\n",
    "b1 = results.params['trialNumbers']\n",
    "b2 = results.params['trialNumbers'] + results.params['trialNumbers:C(scarce)[T.1]']\n",
    "i1 = results.params['Intercept']\n",
    "i2 = results.params['Intercept'] + results.params['C(scarce)[T.1]']\n",
    "\n",
    "plt.plot(x2, y2, color='b', alpha=0.5, label=\"Scarce\")\n",
    "plt.plot(x1, y1, color='orange', alpha=0.5, label=\"Control\")\n",
    "\n",
    "\n",
    "# Slope lines\n",
    "plt.plot(x1, np.array(x1) * b1 + i1, color='orange')\n",
    "plt.plot(x2, np.array(x2) * b2 + i2, color='b')\n",
    "plt.title(\"Learning Phases - Learning Curves - {}\".format(response_var))\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"{} - {}\".format(response_var, \"(Not Transformed)\" if use_orig else \"(Transformed)\"))\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d437f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Phase\n",
    "\n",
    "use_orig = True\n",
    "\n",
    "if use_orig:\n",
    "    performance_df = pd.concat([scarce_performance_data_orig, control_performance_data_orig]).reset_index(drop=True)\n",
    "else: \n",
    "    performance_df = pd.concat([scarce_performance_data, control_performance_data]).reset_index(drop=True)\n",
    "\n",
    "if not compare_all:\n",
    "    performance_df = performance_df.loc[performance_df.trialNumbers <= num_max_trials]\n",
    "    \n",
    "formula = '{} ~ trialNumbers + C(scarce) + trialNumbers:C(scarce)'.format(response_var)\n",
    "\n",
    "mixed = True\n",
    "\n",
    "if mixed:\n",
    "    glm = smf.mixedlm(formula=formula, data=performance_df, groups=performance_df['workerId'])\n",
    "else:\n",
    "    glm = smf.glm(formula=formula, data=performance_df)\n",
    "\n",
    "results = glm.fit()\n",
    "\n",
    "print(results.summary())\n",
    "resids = results.resid_response if not mixed else results.resid\n",
    "normaltest = scipy.stats.normaltest(resids)\n",
    "plt.hist(resids, density=True)\n",
    "plt.title(\"Normal Test: {0:0.4f}, p: {1:0.4f}\".format(normaltest[0], normaltest[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d2d98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data of both performance phases together\n",
    "\n",
    "x1 = control_partition_plot_data[2]\n",
    "x2 = scarce_partition_plot_data[2]\n",
    "y1 = performance_df[performance_df.scarce==0].groupby(\"trialNumbers\").mean()[response_var]\n",
    "y2 = performance_df[performance_df.scarce==1].groupby(\"trialNumbers\").mean()[response_var]\n",
    "b1 = results.params['trialNumbers']\n",
    "b2 = results.params['trialNumbers'] + results.params['trialNumbers:C(scarce)[T.1]']\n",
    "i1 = results.params['Intercept']\n",
    "i2 = results.params['Intercept'] + results.params['C(scarce)[T.1]']\n",
    "\n",
    "if not compare_all:\n",
    "    end_index = np.where(x1 == num_max_trials)[0]\n",
    "    if len(end_index) > 0:\n",
    "        end_index = end_index[0]\n",
    "        x1 = x1[0:end_index]\n",
    "        y1 = y1[0:end_index]\n",
    "    \n",
    "    end_index = np.where(x2 == num_max_trials)[0]\n",
    "    if len(end_index) > 0:\n",
    "        end_index = end_index[0]\n",
    "        x2 = x2[0:end_index]\n",
    "        y2 = y2[0:end_index]\n",
    "        \n",
    "\n",
    "plt.plot(x2, y2, color='b', alpha=0.5, label=\"Scarce\")\n",
    "plt.plot(x1, y1, color='orange', alpha=0.5, label=\"Control\")\n",
    "\n",
    "\n",
    "# Slope lines\n",
    "plt.plot(x1, np.array(x1) * b1 + i1, color='orange')\n",
    "plt.plot(x2, np.array(x2) * b2 + i2, color='b')\n",
    "plt.title(\"Performance Phases - Learning Curves - {}\".format(response_var))\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"{} - {}\".format(response_var, \"(Not Transformed)\" if use_orig else \"(Transformed)\"))\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4623c4de",
   "metadata": {},
   "source": [
    "# 1.3 - Linear Regression: Response Var vs. Num Rewarded Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f08f0c",
   "metadata": {},
   "source": [
    "GLM Analysis for the effect of number of previously rewarded trials and previously unrewarded trials on improvement in response variable in each condition\n",
    "\n",
    "(response_var) ~ numRewardedTrials + numUnrewardedTrials + condition + numRewardedTrials x condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033cd9af",
   "metadata": {},
   "source": [
    "Results for response variable \"expectedScores\" reported in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc4b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the response variable for this analysis\n",
    "\n",
    "response_var = \"expectedScores\"\n",
    "assert (response_var in response_vars), \"{} is not a valid response variable\".format(response_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9086ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag to compare entire performance data or only upto 30 trials, where both conditions are comparable\n",
    "\n",
    "compare_all = True\n",
    "num_max_trials = min(\n",
    "    analysis_data[analysis_data.scarce==1].trialNumbers.max(),\n",
    "    analysis_data[analysis_data.scarce==0].trialNumbers.max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90383387",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Learning Phase \n",
    "\n",
    "use_orig = True\n",
    "\n",
    "if use_orig:\n",
    "    learning_df = pd.concat([scarce_learning_data_orig, control_learning_data_orig]).reset_index(drop=True)\n",
    "else: \n",
    "    learning_df = pd.concat([scarce_learning_data, control_learning_data]).reset_index(drop=True)\n",
    "\n",
    "if not compare_all:\n",
    "    learning_df = learning_df.loc[learning_df.trialNumbers <= num_max_trials]\n",
    "    \n",
    "formula = '{} ~ numRewardedTrials + C(scarce) + numUnrewardedTrials + numRewardedTrials:C(scarce)'.format(response_var)\n",
    "\n",
    "mixed = True\n",
    "\n",
    "if mixed:\n",
    "    glm = smf.mixedlm(formula=formula, data=learning_df, groups=learning_df['workerId'])\n",
    "else:\n",
    "    glm = smf.glm(formula=formula, data=learning_df)\n",
    "\n",
    "results_learning = glm.fit()\n",
    "\n",
    "print(results_learning.summary())\n",
    "resids = results_learning.resid_response if not mixed else results_learning.resid\n",
    "normaltest = scipy.stats.normaltest(resids)\n",
    "plt.hist(resids, density=True)\n",
    "plt.title(\"Normal Test: {0:0.4f}, p: {1:0.4f}\".format(normaltest[0], normaltest[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308ecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing coefficients using contrasts\n",
    "\n",
    "# between conditions\n",
    "# H_a = numRewardedTrials - numUnrewardedTrials > 0\n",
    "print(\"H_a = numRewardedTrials - numUnrewardedTrials > 0\")\n",
    "r = np.zeros_like(results_learning.params)\n",
    "r[list(results_learning.params.index).index(\"numRewardedTrials\")] = 1\n",
    "r[list(results_learning.params.index).index(\"numUnrewardedTrials\")] = -1\n",
    "\n",
    "print(results_learning.t_test(np.expand_dims(r[:-1], axis=0)))\n",
    "print(results_learning.f_test(r))\n",
    "print(results_learning.wald_test(r))\n",
    "\n",
    "# within scarce condition\n",
    "# H_a = numRewardedTrials + numRewardedTrial:C(scarce)[T.1] - numUnrewardedTrials > 0\n",
    "print(\"\\nH_a = numRewardedTrials + numRewardedTrial:C(scarce)[T.1] - numUnrewardedTrials > 0\")\n",
    "r = np.zeros_like(results_learning.params)\n",
    "r[list(results_learning.params.index).index(\"numRewardedTrials\")] = 1\n",
    "r[list(results_learning.params.index).index(\"numRewardedTrials:C(scarce)[T.1]\")] = 1\n",
    "r[list(results_learning.params.index).index(\"numUnrewardedTrials\")] = -1\n",
    "print(results_learning.t_test(np.expand_dims(r[:-1], axis=0)))\n",
    "print(results_learning.f_test(r))\n",
    "print(results_learning.wald_test(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd974bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance phase\n",
    "\n",
    "use_orig = True\n",
    "\n",
    "if use_orig:\n",
    "    performance_df = pd.concat([scarce_performance_data_orig, control_performance_data_orig]).reset_index(drop=True)\n",
    "else: \n",
    "    performance_df = pd.concat([scarce_performance_data, control_performance_data]).reset_index(drop=True)\n",
    "\n",
    "if not compare_all:\n",
    "    performance_df = performance_df.loc[performance_df.trialNumbers <= num_max_trials]\n",
    "    \n",
    "formula = '{} ~ numRewardedTrials + C(scarce) + numUnrewardedTrials + numRewardedTrials:C(scarce)'.format(response_var)\n",
    "\n",
    "mixed = True\n",
    "\n",
    "if mixed:\n",
    "    glm = smf.mixedlm(formula=formula, data=performance_df, groups=performance_df['workerId'])\n",
    "else:\n",
    "    glm = smf.glm(formula=formula, data=performance_df)\n",
    "\n",
    "results_performance = glm.fit()\n",
    "\n",
    "print(results_performance.summary())\n",
    "resids = results_performance.resid_response if not mixed else results_performance.resid\n",
    "normaltest = scipy.stats.normaltest(resids)\n",
    "plt.hist(resids, density=True)\n",
    "plt.title(\"Normal Test: {0:0.4f}, p: {1:0.4f}\".format(normaltest[0], normaltest[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a286c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing coefficients using contrasts\n",
    "\n",
    "# between conditions\n",
    "# H_a = numRewardedTrials - numUnrewardedTrials > 0\n",
    "print(\"H_a = numRewardedTrials - numUnrewardedTrials > 0\")\n",
    "r = np.zeros_like(results_performance.params)\n",
    "r[list(results_performance.params.index).index(\"numRewardedTrials\")] = 1\n",
    "r[list(results_performance.params.index).index(\"numUnrewardedTrials\")] = -1\n",
    "\n",
    "print(results_performance.t_test(np.expand_dims(r[:-1], axis=0)))\n",
    "print(results_performance.f_test(r))\n",
    "print(results_performance.wald_test(r))\n",
    "\n",
    "# within scarce condition\n",
    "# H_a = numRewardedTrials + numRewardedTrial:C(scarce)[T.1] - numUnrewardedTrials > 0\n",
    "print(\"\\nH_a = numRewardedTrials + numRewardedTrial:C(scarce)[T.1] - numUnrewardedTrials > 0\")\n",
    "r = np.zeros_like(results_performance.params)\n",
    "r[list(results_performance.params.index).index(\"numRewardedTrials\")] = 1\n",
    "r[list(results_performance.params.index).index(\"numRewardedTrials:C(scarce)[T.1]\")] = 1\n",
    "r[list(results_performance.params.index).index(\"numUnrewardedTrials\")] = -1\n",
    "print(results_performance.t_test(np.expand_dims(r[:-1], axis=0)))\n",
    "print(results_performance.f_test(r))\n",
    "print(results_performance.wald_test(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5219c0a",
   "metadata": {},
   "source": [
    "# 1.4 - Independent Samples T-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f52e6",
   "metadata": {},
   "source": [
    "Results for response variable \"expectedScores\" reported in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2507b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the response variable for this analysis\n",
    "\n",
    "response_var = \"expectedScores\"\n",
    "assert (response_var in response_vars), \"{} is not a valid response variable\".format(response_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ba481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag to compare entire performance data or only upto 30 trials, where both conditions are comparable\n",
    "\n",
    "compare_all = False\n",
    "num_max_trials = min(\n",
    "    analysis_data[analysis_data.scarce==1].trialNumbers.max(),\n",
    "    analysis_data[analysis_data.scarce==0].trialNumbers.max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb4d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_transformed = False\n",
    "\n",
    "if not compare_all:\n",
    "    control_performance_data = control_performance_data.loc[control_performance_data.trialNumbers <= num_max_trials]\n",
    "    scarce_performance_data = scarce_performance_data.loc[scarce_performance_data.trialNumbers <= num_max_trials]\n",
    "\n",
    "if use_transformed:\n",
    "    # Using the transformed data divided into phases\n",
    "    control_averages = control_performance_data.groupby('workerId').mean()\n",
    "    scarce_averages = scarce_performance_data.groupby('workerId').mean()\n",
    "else:\n",
    "    # Using the original data divided into phases\n",
    "    control_averages = control_performance_data_orig.groupby('workerId').mean()\n",
    "    scarce_averages = scarce_performance_data_orig.groupby('workerId').mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.sort(np.unique(control_performance_data.trialNumbers)), control_performance_data.groupby('trialNumbers').mean()[response_var], color='orange')\n",
    "plt.plot(np.sort(np.unique(scarce_performance_data.trialNumbers)), scarce_performance_data.groupby('trialNumbers').mean()[response_var], color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cb9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting normality for both groups\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(10, 5),squeeze=False)\n",
    "plt.suptitle(\"Distributions of Averages - {}\".format(response_var))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for idx, group in enumerate([(control_averages, \"Control\"), (scarce_averages, \"Scarce\")]):\n",
    "    data = group[0][response_var]\n",
    "    label = group[1]\n",
    "    print(data.mean(), data.std())\n",
    "    normaltest = scipy.stats.normaltest(data)\n",
    "    density = scipy.stats.gaussian_kde(data)\n",
    "    n, x, _ = ax[idx].hist(data, density=True)\n",
    "    ax[idx].set_title(label)\n",
    "    ax[idx].plot(x, density(x))\n",
    "    ax[idx].text(0.75,0.9, \"S: {0:0.4f}\\nP: {1:0.4f}\".format(normaltest.statistic, normaltest.pvalue), transform=ax[idx].transAxes)\n",
    "\n",
    "plt.subplots_adjust(\n",
    "                wspace=0.4, \n",
    "                hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ac5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform independent T-test if data is normal\n",
    "\n",
    "\n",
    "two = scipy.stats.ttest_ind(scarce_averages[response_var], control_averages[response_var], equal_var=True, alternative='two-sided')\n",
    "less = scipy.stats.ttest_ind(scarce_averages[response_var], control_averages[response_var], equal_var=True, alternative='less')\n",
    "greater = scipy.stats.ttest_ind(scarce_averages[response_var], control_averages[response_var], equal_var=True, alternative='greater')\n",
    "\n",
    "print(\"Response Variable: {}\".format(response_var))\n",
    "print(\"Two-Sided:\")\n",
    "print(\"\\tStatistic: {0:0.3f}\\n\\tP-value: {1}\".format(two.statistic, two.pvalue))\n",
    "print(\"Scarce < Control:\")\n",
    "print(\"\\tStatistic: {0:0.3f}\\n\\tP-value: {1}\".format(less.statistic, less.pvalue))\n",
    "print(\"Scarce > Control:\")\n",
    "print(\"\\tStatistic: {0:0.3f}\\n\\tP-value: {1}\".format(greater.statistic, greater.pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193519cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform Mann-Whitney U test if data is not normal\n",
    "\n",
    "two = scipy.stats.mannwhitneyu(scarce_averages[response_var], control_averages[response_var], alternative='two-sided')\n",
    "less = scipy.stats.mannwhitneyu(scarce_averages[response_var], control_averages[response_var], alternative='less')\n",
    "greater = scipy.stats.mannwhitneyu(scarce_averages[response_var], control_averages[response_var], alternative='greater')\n",
    "\n",
    "print(\"Two-Sided:\")\n",
    "print(\"\\tStatistic: {0:0.3f}\\n\\tP-value: {1}\".format(two.statistic, two.pvalue))\n",
    "print(\"Scarce < Control:\")\n",
    "print(\"\\tStatistic: {0:0.3f}\\n\\tP-value: {1}\".format(less.statistic, less.pvalue))\n",
    "print(\"Scarce > Control:\")\n",
    "print(\"\\tStatistic: {0:0.3f}\\n\\tP-value: {1}\".format(greater.statistic, greater.pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47dc7b8",
   "metadata": {},
   "source": [
    "# 1.6 - Stable Strategy Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ef5620",
   "metadata": {},
   "source": [
    "Analysis to see which of the strategies participants converged on and whether they are the same in both conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e02d5",
   "metadata": {},
   "source": [
    "Not reported in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9772d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['strategy'] = filtered_data['strategy'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c06038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategies taken are from the latter 50% of the trials of each participant\n",
    "\n",
    "# Take median of participant strategies or mode (most frequent)\n",
    "measure = 'median' #median, mode\n",
    "\n",
    "if measure == 'median':\n",
    "    scarce_end_strat = filtered_data.loc[(filtered_data.scarce == 1) & (filtered_data.trialNumbers > 60)].groupby('workerId').median()['strategy'].astype('int64')\n",
    "    control_end_strat = filtered_data.loc[(filtered_data.scarce == 0) & (filtered_data.trialNumbers > 15)].groupby('workerId').median()['strategy'].astype('int64')\n",
    "elif measure == 'mode':\n",
    "    scarce_end_strat = filtered_data.loc[(filtered_data.scarce == 1) & (filtered_data.trialNumbers > 60)].groupby('workerId')['strategy'].agg(pd.Series.mode)#.astype('int64')\n",
    "    control_end_strat = filtered_data.loc[(filtered_data.scarce == 0) & (filtered_data.trialNumbers > 15)].groupby('workerId')['strategy'].agg(pd.Series.mode)#.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d34e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contingency table\n",
    "\n",
    "table = np.zeros((89,2))\n",
    "\n",
    "for strat in scarce_end_strat:\n",
    "    #print(type(strat))\n",
    "    if type(strat) != int and type(strat) != np.int64:\n",
    "        strat = strat[0]\n",
    "    table[strat][1] += 1\n",
    "    \n",
    "for strat in control_end_strat:\n",
    "    if type(strat) != int and type(strat) != np.int64:\n",
    "        strat = strat[0]\n",
    "    table[strat][0] += 1\n",
    "    \n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform chi square test of independence\n",
    "\n",
    "summed_tab = table.sum(axis=1)\n",
    "nz_rows = np.where(summed_tab != 0)[0]\n",
    "new_table = table[nz_rows]\n",
    "\n",
    "\n",
    "\n",
    "crosstab = pd.crosstab(new_table[:,0], new_table[:,1])\n",
    "scipy.stats.chi2_contingency(new_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
